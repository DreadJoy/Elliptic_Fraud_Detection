{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRUzBFsfJlCb"
   },
   "source": [
    "# Elliptic Data Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R58zQ_38KOC_"
   },
   "source": [
    "## Objective\n",
    "\n",
    "Find fraud detection.\n",
    "\n",
    "### This Project\n",
    "\n",
    "This project applies graph-theoretic and topological methods to Bitcoin transaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLdfaBmW3O4t"
   },
   "source": [
    "## Content\n",
    "\n",
    "Here is the three data sets using Weber, Domeniconi, Chen, Weidele, Bellei, Robinson, and Leiserson, *Anti-money laundering in Bitcoin: Experimenting with graph convolutional networks for financial forensics* [9] and it worth repeating again.\n",
    "\n",
    "### `classes`\n",
    "\n",
    "This anonymized data set is a transaction graph collected from the Bitcoin blockchain. A node in the graph represents a transaction, an edge can be viewed as a flow of Bitcoins between one transaction and the other. Each node has 166 features and has been labeled as being created by a \"licit\", \"illicit\" or \"unknown\" entity.\n",
    "\n",
    "## Nodes and edges\n",
    "\n",
    "### `edgelist`\n",
    "\n",
    "The graph is made of 203,769 nodes and 234,355 edges. Two percent (4,545) of the nodes are labelled class1 (illicit). Twenty-one percent (42,019) are labelled class2 (licit). The remaining transactions are not labelled with regard to licit versus illicit.\n",
    "\n",
    "## Features\n",
    "\n",
    "### `features`\n",
    "\n",
    "There are 166 features associated with each node. Due to intellectual property issues, we cannot provide an exact description of all the features in the dataset. There is a time step associated to each node, representing a measure of the time when a transaction was broadcasted to the Bitcoin network. The time steps, running from 1 to 49, are evenly spaced with an interval of about two weeks. Each time step contains a single connected component of transactions that appeared on the blockchain within less than three hours between each other; there are no edges connecting the different time steps.\n",
    "\n",
    "The first 94 features represent local information about the transaction – including the time step described above, number of inputs/outputs, transaction fee, output volume and aggregated figures such as average BTC received (spent) by the inputs/outputs and average number of incoming (outgoing) transactions associated with the inputs/outputs. The remaining 72 features are aggregated features, obtained using transaction information one-hop backward/forward from the center node - giving the maximum, minimum, standard deviation and correlation coefficients of the neighbour transactions for the same information data (number of inputs/outputs, transaction fee, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dmUTJnMNsieR",
    "outputId": "2202ddee-62d2-471a-bc0e-8a167d8d1b0e"
   },
   "outputs": [],
   "source": [
    "# Mount the libraries:\n",
    "# igraph\n",
    "!pip install igraph\n",
    "\n",
    "# Shortest path\n",
    "!pip install cairocffi\n",
    "\n",
    "# Role Embeddings\n",
    "!pip install node2vec\n",
    "\n",
    "# topological features\n",
    "!pip install gudhi\n",
    "\n",
    "# Ripser\n",
    "!pip -q install ripser persim\n",
    "\n",
    "# Standard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# more matplotlib\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Google Colab and files\n",
    "from google.colab import files\n",
    "\n",
    "# Runtime\n",
    "import psutil, time\n",
    "\n",
    "# Animate, batches, HTML and base\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Graph\n",
    "import networkx as nx\n",
    "\n",
    "# Ticker\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Eigenvalue and eigenvectors4\n",
    "# Uses ARPACK under the hood in non-symmetric mode\n",
    "from scipy.sparse.linalg import eigs\n",
    "# Uses ARPACK in symmetric mode or Hermitian.\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "# K Means and Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter, deque\n",
    "\n",
    "# Betweenness Centrality\n",
    "from igraph import Graph, plot\n",
    "from networkx.algorithms.centrality import betweenness_centrality_subset\n",
    "\n",
    "# hubs\n",
    "from networkx.algorithms import cycle_basis\n",
    "\n",
    "# Louvain\n",
    "import community.community_louvain as community_louvain\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Shortest path\n",
    "import igraph as ig\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from math import log\n",
    "\n",
    "# Role Embeddings\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "# Simplicial Complex, Clique Complex, Polygon, Patch\n",
    "import gudhi\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "import math\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "# Ripser\n",
    "from ripser import ripser\n",
    "\n",
    "# operating system\n",
    "import os, json\n",
    "\n",
    "# garbage collector\n",
    "import gc\n",
    "\n",
    "# combinations of elements\n",
    "from itertools import combinations, count\n",
    "\n",
    "# Saving\n",
    "import pickle\n",
    "\n",
    "# Requires pairwise distances (e.g. shortest path)\n",
    "from scipy.spatial.distance import squareform\n",
    "from gudhi import RipsComplex\n",
    "\n",
    "# Principal Component analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Nearest Neighbors\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Standard Scaler or Robust\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Breadth First Search\n",
    "from networkx.algorithms.traversal.breadth_first_search import bfs_edges\n",
    "\n",
    "# Scaling\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Log Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Metrics, Confustion Matrix\n",
    "from sklearn.metrics import (classification_report,\n",
    "confusion_matrix,\n",
    "ConfusionMatrixDisplay,\n",
    "f1_score, precision_score, recall_score, log_loss, precision_recall_curve,\n",
    "roc_auc_score, roc_curve, brier_score_loss)\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Neural Network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Tools\n",
    "from itertools import product\n",
    "\n",
    "# Model and Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Function Tools reduce\n",
    "from functools import reduce\n",
    "\n",
    "# Path\n",
    "from pathlib import Path\n",
    "\n",
    "# Calibrated Classifier of OOF\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DlwDeMxsuWS",
    "outputId": "3eae1979-0115-4286-c699-228e40f5d06f"
   },
   "outputs": [],
   "source": [
    "# Mount the data sets\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_2cPqWqu8tz"
   },
   "outputs": [],
   "source": [
    "# 3 data sets: Content, nodes and edges, and features.\n",
    "df_classes = pd.read_csv('/content/drive/MyDrive/elliptic_txs_classes.csv')\n",
    "df_edgelist = pd.read_csv('/content/drive/MyDrive/elliptic_txs_edgelist.csv')\n",
    "# need a header\n",
    "df_features = pd.read_csv('/content/drive/MyDrive/elliptic_txs_features.csv', header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp7ijKiPjnvY"
   },
   "source": [
    "# Act 1 - Graph Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjubdLAjfzBj"
   },
   "source": [
    "## Data for Classes\n",
    "\n",
    "This is nodes and then either unknown illicit or licit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "e1IeCk8sv51Q",
    "outputId": "97b3122b-a424-4bd6-b84f-4ab7b4214e04"
   },
   "outputs": [],
   "source": [
    "df_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7dGayYizGp_",
    "outputId": "3a3c967b-6f1d-4284-ab33-9a257c2a56cd"
   },
   "outputs": [],
   "source": [
    "# How many types?\n",
    "df_classes['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "upD5H-so43EU",
    "outputId": "019e0d05-e1c4-4997-b571-64b64f0defcf"
   },
   "outputs": [],
   "source": [
    "# How many types and count?\n",
    "df_classes['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "eufkKsnk5bu6",
    "outputId": "d617a2ec-f90d-49b2-9b26-60e21c95d3c8"
   },
   "outputs": [],
   "source": [
    "# How many classes and want percent?\n",
    "df_classes['class'].value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQKH8l_b3_kq"
   },
   "source": [
    "The ID is all unique and there is a class is 'unknown', '2' and '1' and (4,545) of the nodes are labelled class1 (illicit) and (42,019) are labelled class2 (licit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "id": "IVvTBAQfNz4m",
    "outputId": "ead96227-994d-4143-cc1f-329d1990fa21"
   },
   "outputs": [],
   "source": [
    "df_classes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3K436KbY7AQ"
   },
   "outputs": [],
   "source": [
    "# '1' and '2' are objects so let's convert it with numerical pd.to_numeric\n",
    "df_classes['class'] = pd.to_numeric(df_classes['class'], errors='coerce')\n",
    "\n",
    "# using .map for 1-illict and 2-licit\n",
    "# Also just leave the 'unknown' using .fillna\n",
    "df_classes['class'] = df_classes['class'].map({1: \"illicit\", 2: \"licit\"}).fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "iOJGQsYYai6S",
    "outputId": "ed31879a-b98b-4955-dec2-c6bd2b652410"
   },
   "outputs": [],
   "source": [
    "df_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW1e4W7hkCnt"
   },
   "source": [
    "We filter and then print only illicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbjoM8s7bLP_",
    "outputId": "2096020c-489f-4238-e946-4d7e8b1f2e8a"
   },
   "outputs": [],
   "source": [
    "# Make sure 'class' column is a string\n",
    "df_classes['class'] = df_classes['class'].astype(str)\n",
    "\n",
    "# Filter to get only the illicit nodes (class = 'illicit')\n",
    "illicit_nodes = df_classes[df_classes['class'] == 'illicit']\n",
    "\n",
    "# Optionally view the top 5\n",
    "print(illicit_nodes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monTSWhikiqe"
   },
   "source": [
    "Plot a visual using unknown, illict, lict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "rRD-zAQ_gcZ5",
    "outputId": "9d81477f-63d3-4036-d9eb-4cfd5cced4ed"
   },
   "outputs": [],
   "source": [
    "# visual for df_classes using values counts().plot, and kind='bar'\n",
    "# illicit = red, and licit = green, unknow = gray\n",
    "\n",
    "df_classes['class'].value_counts().plot(kind='bar',\n",
    "                                        title='Class Distribution',\n",
    "                                        color = ['gray', 'green', 'red'])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1_2RqmxkqU8"
   },
   "source": [
    "Again very small with `illicit 2.2%`, `licit 20.6%` and `unknown 77.1%`. `Unknown` Is shy of 160,000 or (157,205 exactly) `licit` Is a little bit more 40,000 or (42,019 exactly), and `illicit` is mabye 5,000 or (4545 exactly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZwXhSiXf9qv"
   },
   "source": [
    "## Data for Edge List\n",
    "\n",
    "Node directed edge to another node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "fOkU4oDwwAp3",
    "outputId": "8040f293-778f-4f56-b60c-bf31e34ed16b"
   },
   "outputs": [],
   "source": [
    "df_edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4_1mqoWpY7C",
    "outputId": "2edd56b5-a48a-4a11-baa1-8b62c2081945"
   },
   "outputs": [],
   "source": [
    "# How many columns?\n",
    "df_edgelist.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOBN05Gtpk_6"
   },
   "outputs": [],
   "source": [
    "# Let's change the features from txId1 is the source and txId2 is the target\n",
    "df_edgelist = df_edgelist.rename(columns = {'txId1':'source', 'txId2':'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH7TEhrVrE-x",
    "outputId": "99d5607a-2348-4658-f2d2-b85e8f536353"
   },
   "outputs": [],
   "source": [
    "# How many columns and new type\n",
    "df_edgelist.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcHIcWfEpG0l"
   },
   "source": [
    "So just want it a little bit cleaner source to `source` versus `txId1` and `target` versus `txId2`. So that's the node is a source and the pointing to a target which is a direct graph of an edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mwsp_2BNPpe"
   },
   "source": [
    "## Data for Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzYVb-xRp2sh"
   },
   "source": [
    "Here is data `df_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "VK4EdMha-9QG",
    "outputId": "e8d42570-bd1b-49ca-bcaa-23ccf6608d28"
   },
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VWWl89MAB-l"
   },
   "source": [
    "Let to a `txId` and a `time_step` is the data set, So that we can `df_classes` and `df_edgelist` Have the same nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhSOwfPDJjlO"
   },
   "outputs": [],
   "source": [
    "# add header re-names columns\n",
    "new_column_names = {\n",
    "    0: 'txId',\n",
    "    1: 'time_step',\n",
    "    **{i: f'local_info_{i-1}' for i in range(2, 95)},\n",
    "    **{i: f'agg_info_{i-94}' for i in range(95, 167)}\n",
    "}\n",
    "\n",
    "df_features = df_features.rename(columns=new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "ewJPVkMD_feH",
    "outputId": "3cf7ee9b-75d9-4dd8-f855-74f1ae01382e"
   },
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWJa1cb6E_bh"
   },
   "source": [
    "OK let's do the time series from 1 to 49 and Put a picture so we can see every step.\n",
    "\n",
    "See [9, p 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "9J0ND6e-TZpb",
    "outputId": "f08210a0-6067-4abc-f272-47a7b9088fe0"
   },
   "outputs": [],
   "source": [
    "# Make sure txId is aligned (same dtype)\n",
    "df_features['txId'] = df_features['txId'].astype(str)\n",
    "df_classes['txId'] = df_classes['txId'].astype(str)\n",
    "\n",
    "# Merge and keep labels\n",
    "df_merged = df_features.merge(df_classes[['txId', 'class']], on='txId', how='left')\n",
    "\n",
    "# Define the cleaning function\n",
    "def clean_scores(df_features, df_classes):\n",
    "    # Ensure consistent txId types\n",
    "    df_features['txId'] = df_features['txId'].astype(str)\n",
    "    df_classes['txId'] = df_classes['txId'].astype(str)\n",
    "\n",
    "    # Merge labels into features\n",
    "    df_scores = df_features.merge(df_classes[['txId', 'class']], on='txId', how='left')\n",
    "\n",
    "    # Rename 'class' to 'label'\n",
    "    df_scores.rename(columns={'class': 'label'}, inplace=True)\n",
    "\n",
    "    # Drop rows with any missing features (not in df_features)\n",
    "    feature_cols = df_features.columns.drop('txId')\n",
    "    df_scores = df_scores.dropna(subset=feature_cols)\n",
    "\n",
    "    return df_scores.reset_index(drop=True)\n",
    "\n",
    "# Plot counts\n",
    "counts = df_merged.groupby(['time_step', 'class']).size().unstack(fill_value=0)\n",
    "counts = counts.reindex(columns=['unknown', 'licit', 'illicit'], fill_value=0)\n",
    "\n",
    "# Stacked bar chart and color\n",
    "custom_colors = ['#999999', '#2ca02c', '#d62728']\n",
    "ax = counts.plot(kind='bar', stacked=True, figsize=(14, 6), color=custom_colors, width=0.9)\n",
    "\n",
    "# plot title, xlable ect\n",
    "plt.title('Transaction Count per Time Step by Class')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xticks(ticks=range(len(counts.index)), labels=counts.index, rotation=45)\n",
    "plt.legend(title='Class')\n",
    "plt.tight_layout()\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cL-nSDaVInzL",
    "outputId": "25cecb28-bd44-4d0a-d54d-db9ed3f91a8f"
   },
   "outputs": [],
   "source": [
    "# Visualize correlation within local features\n",
    "# Select local info columns (features 2-94 based on description, index 1-93 in 0-indexed DataFrame)\n",
    "local_info_features = df_features.iloc[:, 2:95]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(local_info_features.corr(), cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Heatmap — Local Transaction Features\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize correlation within aggregated features\n",
    "# Select aggregated features (features 95-166 based on description, index 94-165 in 0-indexed DataFrame)\n",
    "agg_features = df_features.iloc[:, 95:167]\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(agg_features.corr(), cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Heatmap — 1-Hop Aggregated Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx-Dak0ENSQL"
   },
   "source": [
    "Not much correlation, especially when the `local_info` may have the same information, just a different way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ2X5lDfIup9"
   },
   "source": [
    "## Let's do it Animated time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvvArsIm3Ysp"
   },
   "source": [
    "So have to do Make sure that the string is consistency for all `df_features`, `df_edgelist`, and `df_classes`. This is a directed graph and using the `licit`, `illicit` and `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51SVsRCvb_nv"
   },
   "outputs": [],
   "source": [
    "# Convert txId to string for consistency\n",
    "df_features['txId'] = df_features['txId'].astype(str)\n",
    "df_edgelist = df_edgelist.astype(str)\n",
    "df_classes['txId'] = df_classes['txId'].astype(str)\n",
    "\n",
    "# Build the full directed graph\n",
    "G_directed = nx.DiGraph()\n",
    "G_directed.add_edges_from(df_edgelist.values)\n",
    "\n",
    "# Create a mapping of txId → class\n",
    "class_map = dict(zip(df_classes['txId'], df_classes['class']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQ3uxq_g4C81"
   },
   "source": [
    "Using subgraph for all from 1 to 49 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "giJvOFUZcBCg"
   },
   "outputs": [],
   "source": [
    "# df_features groupby time_step\n",
    "dfs_by_time = {t: df for t, df in df_features.groupby(\"time_step\")}\n",
    "\n",
    "# tolist and subgraph\n",
    "graphs_by_time = {}\n",
    "for t, df_t in dfs_by_time.items():\n",
    "    tx_ids = df_t['txId'].tolist()\n",
    "    subG = G_directed.subgraph(tx_ids)\n",
    "    graphs_by_time[t] = subG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InzE7L_k45wi"
   },
   "source": [
    "Layout and color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciDxZueicEKm"
   },
   "outputs": [],
   "source": [
    "# sprint layout\n",
    "pos = nx.spring_layout(graphs_by_time[49], seed=42)\n",
    "\n",
    "# color\n",
    "def get_color(node):\n",
    "    label = class_map.get(node, 'unknown')\n",
    "    if label == 'illicit':\n",
    "        return '#d62728'  # red\n",
    "    elif label == 'licit':\n",
    "        return '#2ca02c'  # green\n",
    "    else:              # unknown\n",
    "        return '#999999'  # gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNKkO1BjcHVx"
   },
   "outputs": [],
   "source": [
    "# Update 1 to 49\n",
    "def update(t):\n",
    "    ax.clear()\n",
    "    G_t = graphs_by_time[t]\n",
    "\n",
    "    pos = nx.spring_layout(G_t, seed=42)\n",
    "\n",
    "    colors = [get_color(n) for n in G_t.nodes()]\n",
    "\n",
    "    # Add this to the bottom of your `update(t)` function:\n",
    "    red_patch = mpatches.Patch(color='#d62728', label='Illicit')\n",
    "    green_patch = mpatches.Patch(color='#2ca02c', label='Licit')\n",
    "    gray_patch = mpatches.Patch(color='#999999', label='Unknown')\n",
    "    ax.legend(handles=[red_patch, green_patch, gray_patch], loc='upper right', fontsize=10)\n",
    "\n",
    "    nx.draw(\n",
    "        G_t,\n",
    "        pos=pos,\n",
    "        ax=ax,\n",
    "        node_size=15,\n",
    "        node_color=colors,\n",
    "        with_labels=False,\n",
    "        edge_color=\"#cccccc\",\n",
    "        alpha=0.9\n",
    "    )\n",
    "    ax.set_title(f\"Elliptic Transaction Graph — Time Step {t}\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "pPC_JdwcLxOq",
    "outputId": "89f7eded-7e85-46b1-bf67-0fbe27b56960"
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ani = animation.FuncAnimation(fig, update, frames=range(1, 50), interval=600)\n",
    "ani.save(\"elliptic_graph_colored.mp4\", writer=\"ffmpeg\", dpi=150)\n",
    "\n",
    "\n",
    "# 1 hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "UIt_h-e8Ifmp",
    "outputId": "821a0f3f-9668-4f6a-8341-8bdd7bd9ad32"
   },
   "outputs": [],
   "source": [
    "# video\n",
    "mp4 = open(\"elliptic_graph_colored.mp4\", \"rb\").read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=600 controls>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")\n",
    "\n",
    "# 2 hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhYxAUGVF7BQ"
   },
   "source": [
    "### $k$-Hop Neighborhoods and Illicit Subgraphs\n",
    "\n",
    "What is a $k$-hop neighborhood?\n",
    "\n",
    "Let $n_0$ be a node of interest — for example, a node suspected of illicit activity. The $k$-hop neighborhood of $n_0$, denoted $N_k(n_0)$, is the set of all nodes that can be reached from $n_0$ by traversing at most $k$ edges.\n",
    "\n",
    "When $k = 0$, the neighborhood includes only $n_0$ itself.\n",
    "\n",
    "When $k=1$, we include all immediate neighbors connected by one edge.\n",
    "\n",
    "When $k=2$, we include neighbors of neighbors — and so on.\n",
    "\n",
    "You can think of this visually as concentric rings expanding out from the center node $n_0$. Each ring represents a further layer in the network, moving outward edge by edge.\n",
    "\n",
    "So let’s focus only on illicit nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7oi1HJQQMa-",
    "outputId": "34bbf77b-28a7-4880-f508-f3f4353e0084"
   },
   "outputs": [],
   "source": [
    "# Make sure 'class' column is a string\n",
    "df_classes['class'] = df_classes['class'].astype(str)\n",
    "\n",
    "# Filter to get only the illicit nodes (class = 'illicit')\n",
    "illicit_nodes = df_classes[df_classes['class'] == 'illicit']\n",
    "\n",
    "# view the .head()\n",
    "print(illicit_nodes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDw2AT6fuoYm"
   },
   "source": [
    "We are using unweighted shortest path algorithms, which rely on **Breadth-First Search** or **(BFS)**. If edge weights were available or relevant, we would instead use **Dijkstra’s** algorithm to account for edge costs.\n",
    "\n",
    "We focus on 5-hop neighborhoods meaning that $N_5(n_0)$ reachable within five steps or fewer. This captures the local structure and potential influence zone of a node without needing to traverse the entire network.\n",
    "\n",
    "The algorithm effectively builds a **shortest-path spanning tree** rooted at the **center**, capturing the **minimal paths** to all nodes within the hop limit.\n",
    "\n",
    "See [10, p. 99], [5, p. 315], [11, p. 3] and [13, p. 35]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "Z-y_nDWy4yM4",
    "outputId": "de48e8f3-d365-4ed0-d02a-6836e100d5cb"
   },
   "outputs": [],
   "source": [
    "# Make sure that Consistently across both df_edgelist and df_classes:\n",
    "df_edgelist['source'] = df_edgelist['source'].astype(str)\n",
    "df_edgelist['target'] = df_edgelist['target'].astype(str)\n",
    "df_classes['txId'] = df_classes['txId'].astype(str)\n",
    "\n",
    "# Build the directed graph from edgelist\n",
    "G_directed = nx.from_pandas_edgelist(df_edgelist, source='source', target='target', create_using=nx.DiGraph())\n",
    "\n",
    "# Get a list of illicit nodes\n",
    "illicit_nodes = df_classes[df_classes['class'] == 'illicit']['txId'].tolist()\n",
    "\n",
    "# This loop ensures the selected start_node is: Illicit Actually present in the graph\n",
    "# Pick an Illicit Node That Exists in the Graph\n",
    "start_node = None\n",
    "for node in illicit_nodes:\n",
    "    if node in G_directed:\n",
    "        start_node = node\n",
    "        break\n",
    "\n",
    "if start_node is None:\n",
    "    print(\"No illicit nodes found in the graph.\")\n",
    "else:\n",
    "    # Perform 5-hop neighborhood expansion\n",
    "    # This function finds all nodes that are reachable from the start node within depth hops (in both directions).\n",
    "    # `frontier` is the current boundary of the search.\n",
    "    # `successors` = nodes that receive an edge from the current node (->)\n",
    "    # `predecessors` = nodes that send an edge to the current node (<-)\n",
    "    def get_n_hop_neighbors(graph, start, depth=5):\n",
    "        visited = set([start])\n",
    "        frontier = set([start])\n",
    "        for _ in range(depth):\n",
    "            next_frontier = set()\n",
    "            for node in frontier:\n",
    "                next_frontier.update(set(graph.successors(node)))\n",
    "                next_frontier.update(set(graph.predecessors(node)))\n",
    "            frontier = next_frontier - visited\n",
    "            visited.update(frontier)\n",
    "        return visited\n",
    "\n",
    "    # Get subgraph\n",
    "    # five_hop_nodes is a set of nodes found within 5 hops.\n",
    "    # H is a new subgraph built from those nodes plus the original start_node.\n",
    "    five_hop_nodes = get_n_hop_neighbors(G_directed, start_node, depth=5)\n",
    "    H = G_directed.subgraph(five_hop_nodes | {start_node})  # include start node explicitly\n",
    "\n",
    "    # Build color mapping\n",
    "    node_colors = []\n",
    "    class_map = df_classes.set_index('txId')['class'].to_dict()\n",
    "\n",
    "    # This loop assigns:\n",
    "    # Red for 'illicit'\n",
    "    # Green for 'licit'\n",
    "    # Gray if the class is unknown\n",
    "    for node in H.nodes():\n",
    "        cls = class_map.get(node, 'unknown')\n",
    "        if cls == 'illicit':\n",
    "            node_colors.append('red')\n",
    "        elif cls == 'licit':\n",
    "            node_colors.append('green')\n",
    "        else:\n",
    "            node_colors.append('gray')\n",
    "\n",
    "    # Plot the colored subgraph\n",
    "    # nx.kamada_kawai_layout() — like spring layout but more geometric\n",
    "    # nx.circular_layout() — nodes in a circle\n",
    "    # nx.shell_layout()\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    pos = nx.shell_layout(H)  # layout for consistent spacing\n",
    "    nx.draw(H, pos, node_color=node_colors, with_labels=False, node_size=30, alpha=0.8, arrows=True)\n",
    "    plt.title(f\"5-Hop Neighborhood from Illicit Node {start_node}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iyuk5K6CAdm6"
   },
   "source": [
    "So you can see two illicit nodes are connected to the same licit node — almost like they're targeting it. But is that really two separate events, or could it still be coming from one main actor?\n",
    "\n",
    "Let’s run a 3-hop expansion on a few more illicit nodes — maybe 6 in total. We’ll check whether each one connects to just a single target or multiple targets, and whether the patterns repeat or vary. This might help us understand if these illicit nodes are working together or acting independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5YmnWDOHAYCc",
    "outputId": "5023d4f5-dbe3-4efe-b5e3-ab6218666258"
   },
   "outputs": [],
   "source": [
    "# Function: 3-hop neighborhood expansion\n",
    "def get_n_hop_neighbors(graph, start, depth=3):\n",
    "    visited = set([start])\n",
    "    frontier = set([start])\n",
    "    for _ in range(depth):\n",
    "        next_frontier = set()\n",
    "        for node in frontier:\n",
    "            next_frontier.update(set(graph.successors(node)))\n",
    "            next_frontier.update(set(graph.predecessors(node)))\n",
    "        frontier = next_frontier - visited\n",
    "        visited.update(frontier)\n",
    "    return visited\n",
    "\n",
    "# Color mapping helper\n",
    "class_map = df_classes.set_index('txId')['class'].to_dict()\n",
    "\n",
    "# Loop through first 6 illicit nodes that exist in G\n",
    "count = 0\n",
    "# Get the list of illicit transaction IDs\n",
    "illicit_tx_ids = df_classes[df_classes['class'] == 'illicit']['txId'].tolist()\n",
    "\n",
    "# Iterate through the illicit transaction IDs\n",
    "for node in illicit_tx_ids:\n",
    "    if node not in G_directed:\n",
    "        continue\n",
    "\n",
    "    start_node = node\n",
    "    count += 1\n",
    "    print(f\"\\n Visualizing 3-hop neighborhood for illicit node {start_node}\")\n",
    "\n",
    "    # Subgraph creation\n",
    "    three_hop_nodes = get_n_hop_neighbors(G_directed, start_node, depth=3)\n",
    "    H = G_directed.subgraph(three_hop_nodes | {start_node})\n",
    "\n",
    "    # Node color mapping\n",
    "    node_colors = []\n",
    "    for n in H.nodes():\n",
    "        cls = class_map.get(n, 'unknown')\n",
    "        if cls == 'illicit':\n",
    "            node_colors.append('red')\n",
    "        elif cls == 'licit':\n",
    "            node_colors.append('green')\n",
    "        else:\n",
    "            node_colors.append('gray')\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    pos = nx.shell_layout(H)\n",
    "    nx.draw(H, pos, node_color=node_colors, with_labels=False, node_size=30, alpha=0.8, arrows=True)\n",
    "    plt.title(f\"3-Hop Neighborhood from Illicit Node {start_node}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Stop after 6\n",
    "    if count >= 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOlaw7PXHnwJ"
   },
   "source": [
    "`232629023`: Same structure at 3 hops and 5 hops — two illicit nodes converging on one licit. Could suggest repeated coordination or one main actor.\n",
    "\n",
    "`230389796`: Three illicit nodes connect to distinct licit nodes nearby, forming a cluster — this resembles a potential fraud ring.\n",
    "\n",
    "`17387772`: Almost identical to `230389796` — the structural overlap suggests it may be part of the same coordinated group.\n",
    "\n",
    "`232947878`: More chaotic, scattered connections — less like targeted fraud, more like background laundering.\n",
    "\n",
    "`16754007`: One illicit node in a dense cluster of licit/unknown — could be a “hiding” strategy.\n",
    "\n",
    "`231990430`: Same repeat pattern as `232629023` — two illicit nodes targeting one licit. Likely not coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1anWm_UwQKb"
   },
   "source": [
    "## Degree Distribution in an Undirected Graph\n",
    "\n",
    "A degree distribution represents how the degrees (number of edges) are **spread across all nodes** in a graph.\n",
    "\n",
    "For an undirected graph with $N$ total nodes, we count how many nodes have degree $(0, 1, \\dots, N-1)$ and gives us a **frequency distribution** of degrees.\n",
    "\n",
    "To convert this into a **probability distribution**, we divide each frequency by $N$, so the total area under the distribution **sums to 1**. This gives us insight into the overall structure of the network\n",
    "\n",
    "See [10, p. 243] and [4 , p. 6]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJrVsxcfURvY"
   },
   "source": [
    "Also, for the code, I will using a `dataframe` called `scores`. That is useful for engineer features, And that will be throughout this Jupiter notebook especially when we start modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "hkGZns_viLFm",
    "outputId": "ff11cc77-ede8-437d-b133-529179426399"
   },
   "outputs": [],
   "source": [
    "# create and undirected\n",
    "G_undirected = nx.from_pandas_edgelist(df_edgelist, source='source', target='target', create_using=nx.Graph())\n",
    "\n",
    "# Add degrees to your dataframe\n",
    "# Add a scores DataFrame, Which we are going to use that eventually for modeling and predicting\n",
    "\n",
    "df_scores = pd.DataFrame({\n",
    "    'txId': list(G_undirected.nodes),\n",
    "    'label': [class_map.get(n, 'unknown') for n in G_undirected.nodes]\n",
    "})\n",
    "\n",
    "# Add a scores to:\n",
    "df_scores['degree'] = df_scores['txId'].map(lambda n: G_undirected.degree(n))\n",
    "\n",
    "# split know classes\n",
    "licit_deg = df_scores[df_scores['label'] == 'licit']['degree']\n",
    "illicit_deg = df_scores[df_scores['label'] == 'illicit']['degree']\n",
    "unknown_deg = df_scores[df_scores['label'] == 'unknown']['degree']\n",
    "\n",
    "# Plot KDE (Kernel Density Estimation) for smooth curves\n",
    "# using log1p Because it's very steep and then almost zero when degree gets larger\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.kdeplot(np.log1p(licit_deg), label='Licit', color='green', fill=True)\n",
    "sns.kdeplot(np.log1p(illicit_deg), label='Illicit', color='red', fill=True)\n",
    "sns.kdeplot(np.log1p(unknown_deg), label='Unknown', color='gray', fill=True)\n",
    "\n",
    "# Set log-transformed x-axis ticks back to real degree values\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{np.expm1(x):.0f}\"))\n",
    "\n",
    "# Set labels and\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Node Degree: Licit vs Illicit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BV6KYFq5LlG"
   },
   "source": [
    "To better understand the structure of the graph, I applied the `log1p` transformation to the node degrees. This helps visualize the degree density, especially since the distribution is highly skewed.\n",
    "\n",
    "Most nodes have a degree of 1, 2, or 3, and then the frequency drops off sharply. However, some nodes exhibit degrees as high as 400 or more.\n",
    "\n",
    "When comparing node classes:\n",
    "\n",
    "`Illicit` and `licit` nodes both tend to have a peak at degree 1, followed by a **rapid decline**.\n",
    "\n",
    "Interestingly, unknown nodes have a higher concentration at degree 2, which stands out and may suggest a different interaction pattern.\n",
    "\n",
    "Note that this analysis is based on the total degree $(\\text{in} + \\text{out})$ in the undirected version of the graph — not separating in-degree from out-degree.\n",
    "\n",
    "For further analysis, I stored the degree values as a new feature column in the `df_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIWEHXleXeAE",
    "outputId": "216293fa-97c8-4135-eaa6-5d7290a0f942"
   },
   "outputs": [],
   "source": [
    "df_scores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiPy1eln6HFw"
   },
   "source": [
    "## Cycles\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "Let a $G = (V,E)$ where $V$ is a **vertices** or a **node**, and $E$ are **edges**. A **simple graph** is a pair $G = (V,E)$ with $V \\neq \\emptyset$ and\n",
    "$$ E \\subseteq \\{ \\{u,v\\} : u, v \\in V, u \\neq v\\} $$\n",
    "\n",
    "so edges are unordered pairs of distinct vertices (no loops, no parallel edges). $G$ is connected iff $\\forall v_i \\neq v_j \\in V$ there exists a path $ v_i \\rightsquigarrow v_j$; equivalently, $G$ has exactly one connected component.\n",
    "\n",
    "A **path** is a listing of vertices what the starting point $v_1$ is set $ v_1, v_2, \\dots , v_k $\n",
    "\n",
    "A **cycle** in a graph is a path that starts and ends at the same node, with no repeated nodes $ v_1 \\rightarrow v_2 \\rightarrow \\dots \\rightarrow v_k \\rightarrow v_1$ That each the nodes are placed around the circle, and the **order** does matter.\n",
    "\n",
    "In an **undirected** graph, it's the same idea but edge direction doesn’t matter.\n",
    "\n",
    "Many money laundering schemes rely on cycles to disguise the origin of funds. These circular transaction patterns can help money flow through multiple nodes and eventually re-enter the system appearing legitimate — making them harder to detect.\n",
    "\n",
    "Let’s explore whether these cycles might reveal potential entry points for illicit activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3iZlQwHk_vF"
   },
   "source": [
    "Code:\n",
    "\n",
    "We examine cycles in a directed graph and keep only those that contain enough `illicit` nodes. We’ll inspect up to six candidate start nodes. Let `min_illicit` be the minimum number of illicit nodes a cycle must contain to be considered interesting. `class_map` maps each node ID to its label ('illicit', 'licit', or 'unknown'). For any cycle, `red_count` is the number of nodes labeled 'illicit'. We filter cycles by `red_count >= min_illicit`. Then we ensure we’re evaluating cycles in the directed graph and log the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-rbiCcdwJ-o",
    "outputId": "55375d62-1b64-48fd-cf29-9b6028c43e0b"
   },
   "outputs": [],
   "source": [
    "# Runs a loops analysis on those same 6 nodes: `232629023` ect.\n",
    "\n",
    "# Number of Illicit starting points to check and runtime control\n",
    "max_nodes = 6\n",
    "\n",
    "# list to store results\n",
    "cycle_results = []\n",
    "\n",
    "# Set threshold for what counts as \"suspicious\"\n",
    "min_illicit = 1\n",
    "\n",
    "# This function checks if a cycle contains enough illicit nodes (at least min_illicit).\n",
    "# class_map.get(node) looks up the label ('illicit', 'licit', and unknowns).\n",
    "# Cycles with enough red_count are considered interesting.\n",
    "def is_interesting_cycle(cycle, class_map, min_illicit=1):\n",
    "    red_count = sum(1 for node in cycle if class_map.get(node) == 'illicit')\n",
    "    return red_count >= min_illicit\n",
    "\n",
    "# Looping through list of known illicit node IDs (illicit_tx_ids)\n",
    "# Skip nodes not in the graph (some might be missing from edge list)\n",
    "count = 0\n",
    "for node in illicit_tx_ids:\n",
    "    if node not in G_directed:\n",
    "        continue\n",
    "\n",
    "    # Log the progress\n",
    "    print(f\"\\n Analyzing 3-hop neighborhood for illicit node {node}\")\n",
    "    count += 1\n",
    "\n",
    "    # 3-hop subgraph\n",
    "    # get_n_hop_neighbors() returns all nodes reachable from node within 3 hops\n",
    "    # H is a small subgraph centered on this node’s local neighborhood\n",
    "    neighbors = get_n_hop_neighbors(G_directed, node, depth=3)\n",
    "    H = G_directed.subgraph(neighbors | {node})\n",
    "\n",
    "    # Finds simple directed cycles in the 3-hop neighborhood using NetworkX.\n",
    "    # These are cycles with no repeated nodes except the start/end.\n",
    "    cycles = list(nx.simple_cycles(H))\n",
    "    interesting_cycles = [c for c in cycles if is_interesting_cycle(c, class_map, min_illicit=1)]\n",
    "\n",
    "    # log the cycle_results\n",
    "    if interesting_cycles:\n",
    "        print(f\"Found {len(interesting_cycles)} interesting cycle(s) in neighborhood of node {node}\")\n",
    "        for c in interesting_cycles:\n",
    "            print(\" -\", c)\n",
    "        cycle_results.append((node, interesting_cycles))\n",
    "    else:\n",
    "        print(\"No interesting cycles found.\")\n",
    "    # Stopping after 6 nodes\n",
    "    if count >= max_nodes:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45ipZDdPh10v"
   },
   "source": [
    "So nothing yet And that's fine because some of the fraud are rare. The next is using Only illicit nodes using a directed subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJWsNRPoxSuH",
    "outputId": "25091ef5-aa9b-4790-d2eb-36af7afc0aef"
   },
   "outputs": [],
   "source": [
    "# Get all illicit node IDs using list and set()\n",
    "illicit_nodes = df_classes[df_classes['class'] == 'illicit']['txId'].tolist()\n",
    "illicit_nodes = set(illicit_nodes)\n",
    "\n",
    "# Create subgraph of illicit-only nodes (including only edges between them)\n",
    "G_illicit = G_directed.subgraph(illicit_nodes).copy()\n",
    "\n",
    "#  Find all simple cycles in this subgraph\n",
    "cycles = list(nx.simple_cycles(G_illicit))\n",
    "\n",
    "# Print summary\n",
    "print(f\"Found {len(cycles)} cycles among illicit nodes only.\")\n",
    "\n",
    "# Arbitrate numbers for 4545\n",
    "for c in cycles[:4546]:\n",
    "    print(\"Cycle:\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZ_CMunwiUxE"
   },
   "source": [
    "Nothing there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5UbpRzO-eaq",
    "outputId": "30e20c8f-0a6e-43d2-e785-d022c40a7b67"
   },
   "outputs": [],
   "source": [
    "# Start with illicit node IDs\n",
    "illicit_nodes = set(df_classes[df_classes['class'] == 'illicit']['txId'].tolist())\n",
    "\n",
    "# Find all neighbors (in or out) of illicit nodes\n",
    "neighbors = set()\n",
    "for node in illicit_nodes:\n",
    "    if node in G_directed:\n",
    "        neighbors.update(G_directed.successors(node))\n",
    "        neighbors.update(G_directed.predecessors(node))\n",
    "\n",
    "# Union to get an expanded subgraph\n",
    "expanded_nodes = illicit_nodes.union(neighbors)\n",
    "G_expanded = G_directed.subgraph(expanded_nodes).copy()\n",
    "\n",
    "# Step 4: Find all simple cycles\n",
    "cycles = list(nx.simple_cycles(G_expanded))\n",
    "\n",
    "# Step 5: Filter cycles that contain 2+ illicit nodes\n",
    "def is_interesting_cycle(cycle, class_map):\n",
    "    return sum(1 for node in cycle if class_map.get(node) == 'illicit') >= 2 # illicit 1 ?\n",
    "\n",
    "class_map = df_classes.set_index('txId')['class'].to_dict()\n",
    "interesting_cycles = [c for c in cycles if is_interesting_cycle(c, class_map)]\n",
    "\n",
    "# Step 6: Print results\n",
    "print(f\"Found {len(interesting_cycles)} interesting cycles with 2+ illicit nodes.\")\n",
    "\n",
    "# Arbitrate numbers for 4545\n",
    "for c in interesting_cycles[:4545]:\n",
    "    print(\"Cycle:\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7e3GEZ5kogX"
   },
   "source": [
    "Again nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YiBDlDqJ_ltR",
    "outputId": "4cb377e7-f339-4899-a9fe-363b2b5030b0"
   },
   "outputs": [],
   "source": [
    "print(f'Number of cycles: {len(cycles)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUdvPaKglc4c"
   },
   "source": [
    "So, Even two elicited node of a corrected graph is still zero. So let's using undirected graph and see if that'll help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liLqiAbzu_UR"
   },
   "source": [
    "### Change to directed to undirected.\n",
    "\n",
    "We switch to an undirected graph $G = (V,E)$ and perform a **depth‑first search (DFS)** to build a search tree $T$.\n",
    "\n",
    "Start from a root $u$ and grow $T$ by exploring unseen vertices.\n",
    "\n",
    "When we encounter an edge $\\{ {v,w} \\}$:\n",
    "\n",
    "If $w \\notin V(T)$, add $\\{ {v,w} \\}$ to $T$ (a tree edge).\n",
    "\n",
    "If $w \\in V(T)$ and $w$ is an ancestor of $v$ in $T$, then $ \\{ {v,w} \\}$ is a back edge. The unique path in $T$ from $w$ to $v$, together with $\\{ {v,w} \\}$, forms a simple cycle.\n",
    "\n",
    "After DFS (including **backtracking**), the set of cycles discovered from all back edges forms a cycle basis of $G$ (a minimal set of cycles from which every cycle in $G$ can be formed via symmetric difference of edges). See West [10, pg 156].\n",
    "\n",
    "`networkx.cycle_basis(G)` implements this idea for undirected graphs, and the algirithm is Still today. See [17] K. Paton, *An algorithm for finding a fundamental set of cycles of a graph*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "na-UhIW-AUyK",
    "outputId": "44baf559-60c9-446b-85a6-fc464a11b599"
   },
   "outputs": [],
   "source": [
    "# undierected cycles\n",
    "undirected_cycles = nx.cycle_basis(G_undirected)\n",
    "print(f\"Found {len(undirected_cycles)} undirected cycles in the full graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6FrWJOMm_Hp"
   },
   "source": [
    "OK now getting somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90NI_DtzCpLx",
    "outputId": "d08a9499-1e16-4072-ebd8-d06214964374"
   },
   "outputs": [],
   "source": [
    "suspicious_cycles = [c for c in undirected_cycles if any(class_map.get(n) == 'illicit' for n in c)]\n",
    "print(f\"Found {len(suspicious_cycles)} undirected cycles with at least one illicit node.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDs3lrl4-jgd"
   },
   "source": [
    "So this is one node for `illicit` per a cycle, and the cycle with count and Produce the top score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlTpOqdOEEki",
    "outputId": "315c59a0-3261-4891-960d-cca9384dbda0"
   },
   "outputs": [],
   "source": [
    "illicit_counts_per_loop = [\n",
    "    sum(1 for node in c if class_map.get(node) == 'illicit')\n",
    "    for c in suspicious_cycles\n",
    "]\n",
    "\n",
    "illicit_cycle_count = Counter(illicit_counts_per_loop)\n",
    "print(\"Illicit node count per cycle:\")\n",
    "for num_illicit, count in sorted(illicit_cycle_count.items()):\n",
    "    print(f\"{num_illicit} illicit nodes: {count} loops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFIIKry5W8Ne"
   },
   "source": [
    "### Most suspicious unkowns nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUUoGJHa5gPr"
   },
   "source": [
    "Let a undirected cycle in the graph, and let $V = \\text{nodes}$ Have at least 5 illicit nodes. We define (tau) **threshold** is bigger than 5. In other words: $\\tau \\geq 5$. We define all the cycles bigger than or equal to are $$ \\mathcal C_{\\tau}(G) = \\{ C \\in C(G) \\mid \\mathcal I(C) \\geq 5 \\} $$\n",
    "\n",
    "where $C(G)$ meaning cycles are in $G$ and $\\mathcal I (C)$ meaning that only looks through the threshold from cycle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "reLaapQaFm7I",
    "outputId": "4d6eca01-f71c-4ed7-e896-40593540ada3"
   },
   "outputs": [],
   "source": [
    "suspicious_unknowns = Counter()\n",
    "\n",
    "for cycle in suspicious_cycles:\n",
    "    illicit_count = sum(1 for node in cycle if class_map.get(node) == 'illicit')\n",
    "    if illicit_count >= 5:  # threshold\n",
    "        for node in cycle:\n",
    "            if class_map.get(node) == 'unknown':\n",
    "                suspicious_unknowns[node] += 1\n",
    "\n",
    "print(\"Most suspicious unknown nodes:\")\n",
    "print(suspicious_unknowns.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ3Fx1GM4_k0"
   },
   "source": [
    "This ones will do those five illicit and then loop through those cycles and then put a plot so you can see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TyYiltKTMFT7",
    "outputId": "a68d0220-047f-49c6-f3e5-28b2ee917159"
   },
   "outputs": [],
   "source": [
    "# Top 5 suspicious unknowns\n",
    "top_unknowns = [node for node, _ in suspicious_unknowns.most_common(5)]\n",
    "\n",
    "for i, unk_node in enumerate(top_unknowns):\n",
    "    # Find cycles that include this unknown node\n",
    "    relevant_cycles = [c for c in suspicious_cycles\n",
    "                       if unk_node in c and sum(1 for n in c if class_map.get(n) == 'illicit') >= 5]\n",
    "\n",
    "    # Flatten nodes and edges\n",
    "    cycle_nodes = set(n for c in relevant_cycles for n in c)\n",
    "    cycle_edges = [(c[j], c[(j+1)%len(c)]) for c in relevant_cycles for j in range(len(c))]\n",
    "\n",
    "    # Build subgraph\n",
    "    H = nx.Graph()\n",
    "    H.add_edges_from(cycle_edges)\n",
    "\n",
    "    # Node coloring\n",
    "    node_colors = []\n",
    "    for node in H.nodes():\n",
    "        if node == unk_node:\n",
    "            node_colors.append('blue')  # highlight target\n",
    "        elif class_map.get(node) == 'illicit':\n",
    "            node_colors.append('red')\n",
    "        elif class_map.get(node) == 'licit':\n",
    "            node_colors.append('green')\n",
    "        else:\n",
    "            node_colors.append('gray')\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    pos = nx.spring_layout(H, seed=42)\n",
    "    nx.draw(H, pos, node_color=node_colors, with_labels=False, node_size=120, edge_color='black')\n",
    "    plt.title(f\"Suspicious Unknown Node {unk_node} in {len(relevant_cycles)} cycles\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSvPROJuwXNZ"
   },
   "source": [
    "So this is confirmed and you can see on the visual the cycles are the same cycles just different nodes with suspicious illicit So that is a good insight for these cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX2DluDLhqp9"
   },
   "source": [
    "### Sub Graph Top 5 suspicious unknowns using eigenvalues, eigenvector and Spectral Clustering.\n",
    "\n",
    "So for this we are going to use `nx.ego_graph` And what that means is  returns induced subgraph of neighbors centered at node $n$ within a given $\\text{radius} = 3$ in the `def suspicious_none_report`.\n",
    "\n",
    "This line creates a dictionary called class_map where:\n",
    "Each key is a transaction ID (txId) and Each value is the corresponding class label ('illicit', 'licit', or 'unknown').\n",
    "\n",
    "The code of `cycles = list(nx.cycle_basis(H))` is using **Johnson algorithm** using `networkx` in the hood.\n",
    "\n",
    "See D. B. Johnson, *Finding all the elementary circuits of a directed graph* [12].\n",
    "\n",
    "Next **Spectral Clustering** and **eigenvectors**\n",
    "\n",
    "Under the hood we are using **ARPACK** or **ARnoldi PACKage**\n",
    "\n",
    "The Arnoldi iteration for general (possibly non-symmetric) matrices\n",
    "\n",
    "The Lanczos algorithm for symmetric (or Hermitian) matrices\n",
    "\n",
    "These are both Krylov subspace methods that avoid computing the full matrix spectrum.\n",
    "\n",
    "It’s all about finding a low-dimensional approximation of a giant matrix.\n",
    "\n",
    "See Trefethen and Bau [7, Lecture 34]\n",
    "\n",
    "Essentially what they're doing is how Arnoldi builds an orthonormal basis of the Krylov subspace, constructs a smaller matrix $H_k$ (Hessenberg form), and uses its eigenvalues as approximations to the original matrix’s eigenvalues.\n",
    "\n",
    "That smaller matrix $H_k$ plays the same role as $T_k$ in the Lanczos method for symmetric matrices.\n",
    "\n",
    "We use `from scipy.sparse.linalg import eigs`\n",
    "and calling `ARPACK` (under the hood), which:\n",
    "\n",
    "- Uses Arnoldi if the matrix is non-symmetric\n",
    "\n",
    "- Uses Lanczos if it detects symmetry\n",
    "\n",
    "ARPACK Is great because efficient, sparse-friendly, and backed by decades of numerical research.\n",
    "\n",
    "Tiny little history:\n",
    "\n",
    "- ISPACK – First eigenvalue solver library (1970s), written in FORTRAN.\n",
    "\n",
    "- LINPACK – More general linear systems.\n",
    "\n",
    "- LAPACK – Successor to both, with better performance and modern routines.\n",
    "\n",
    "- ARPACK – Specializes in sparse eigenproblems using Arnoldi and Lanczos.\n",
    "\n",
    "- SciPy / NumPy – Wraps all of these (through compiled C/Fortran bindings).\n",
    "\n",
    "Python did not inveted the wheel, using `from scipy.sparse.linalg import eigs` calls ARPACK (written in FORTRAN) under the hood.\n",
    "\n",
    "See Trefethen and Bau [7, Lecture 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9OQHDOYhyQu",
    "outputId": "bb1591c1-ed75-486a-b586-24c9950ba3c2"
   },
   "outputs": [],
   "source": [
    "top_unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJSCCQyHO_Zj"
   },
   "outputs": [],
   "source": [
    "def suspicious_node_report(G_undirected, class_map, node_id, radius=3, show_plot=True, run_spectrum=True, n_clusters=2):\n",
    "    node_id = str(node_id)\n",
    "\n",
    "    # Build ego subgraph and H subgraph\n",
    "    sub_nodes = nx.ego_graph(G_undirected, node_id, radius=radius)\n",
    "    H = G_undirected.subgraph(sub_nodes)\n",
    "\n",
    "    print(f\"\\n Suspicious Node: {node_id}\")\n",
    "    print(f\"→ Subgraph size: {H.number_of_nodes()} nodes, {H.number_of_edges()} edges\")\n",
    "\n",
    "    # labels class and counts\n",
    "    labels = [class_map.get(n, 'unknown') for n in H.nodes()]\n",
    "    label_counts = Counter(labels)\n",
    "    print(\"→ Label counts:\", label_counts)\n",
    "\n",
    "    # undirected or directed\n",
    "    if not G_undirected.is_directed():\n",
    "        cycles = list(nx.cycle_basis(H))\n",
    "        print(f\"→ Cycles found: {len(cycles)}\")\n",
    "    else:\n",
    "        print(\"→ Skipping cycle detection (graph is directed)\")\n",
    "\n",
    "    # Visualize subgraph\n",
    "    if show_plot:\n",
    "        node_colors = []\n",
    "        node_sizes = []\n",
    "        for n in H.nodes():\n",
    "            label = class_map.get(n, 'unknown')\n",
    "            if n == node_id:\n",
    "                node_colors.append('blue')\n",
    "                node_sizes.append(150)\n",
    "            elif label == 'illicit':\n",
    "                node_colors.append('red')\n",
    "                node_sizes.append(60)\n",
    "            elif label == 'licit':\n",
    "                node_colors.append('green')\n",
    "                node_sizes.append(60)\n",
    "            else:\n",
    "                node_colors.append('gray')\n",
    "                node_sizes.append(60)\n",
    "\n",
    "        pos = nx.spring_layout(H, seed=42)\n",
    "        nx.draw(H, pos, node_color=node_colors, node_size=node_sizes, with_labels=False, alpha=0.8)\n",
    "        plt.title(f\"{radius}-Hop Neighborhood of Suspicious Node {node_id}\")\n",
    "        plt.show()\n",
    "\n",
    "    # Spectral Clustering and 50 eigen vaules and vectors\n",
    "    if run_spectrum and H.number_of_nodes() > 5:\n",
    "        try:\n",
    "            A = nx.adjacency_matrix(H).astype(float)\n",
    "            eigvals, eigvecs = eigs(A, k=min(50, H.number_of_nodes()-2))\n",
    "            eigvals = np.real(eigvals)\n",
    "            eigvecs = np.real(eigvecs)\n",
    "\n",
    "            # Plot spectrum\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(eigvals, 'bo-', label='Eigenvalues')\n",
    "            plt.title(f\"Eigenvalue Spectrum for Node {node_id}\")\n",
    "            plt.xlabel(\"Index\")\n",
    "            plt.ylabel(\"Eigenvalue\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # KMeans on top 50 eigenvectors\n",
    "            coords = eigvecs[:, :2]\n",
    "            kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "            cluster_labels = kmeans.fit_predict(coords)\n",
    "\n",
    "            # Plot embedded 2D points\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            plt.scatter(coords[:, 0], coords[:, 1], c=cluster_labels, cmap='viridis', s=60)\n",
    "            plt.title(f\"Spectral Clustering of Node {node_id}'s Neighborhood\")\n",
    "            plt.xlabel(\"1st Eigenvector\")\n",
    "            plt.ylabel(\"2nd Eigenvector\")\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\" Spectral analysis failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iiz4z2F4O_TU",
    "outputId": "8458f84a-00c2-4bc9-9fa6-1c93ffa3cd87"
   },
   "outputs": [],
   "source": [
    "top_unknowns = ['71462499', '73224674', '72629376', '71461473', '29949646']\n",
    "\n",
    "for uid in top_unknowns:\n",
    "    suspicious_node_report(G_undirected, class_map, uid, radius=3, show_plot=True, run_spectrum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGdV6HP_7Ake"
   },
   "source": [
    "So, What is going on? I'm going to pick node $1 \\rightarrow$ `71462499` and node $4 \\rightarrow$ `71461473`.\n",
    "\n",
    "1. Graph Structure\n",
    "\n",
    "Node $1$ have smaller ego network (fewer nodes and edges). Has a clear cycle with 5 illicit + 2 unknown nodes. Structure has loops + hubs, but the loops are relatively tight and localized. Many branches terminate quickly — fewer long-range interconnections. This compactness often means fewer large eigenvalues in the adjacency spectrum.\n",
    "\n",
    "Node $4$\n",
    "\n",
    "Much larger ego network (771 nodes, 970 edges). Multiple hubs and dense interconnections — network looks more meshed. The illicit nodes (red) form several large, separate clusters with strong connectivity to licit and unknown nodes. Far more edges per node on average, meaning higher degree variance and more diverse local structures.\n",
    "\n",
    "2. Eigenvalue Spectrum\n",
    "\n",
    "Node $1$\n",
    "\n",
    "Eigenvalues between -4 and +4, tapering toward -1 to 0. This narrower range suggests fewer dominant structural patterns. The largest eigenvalues correspond to the cycle and its few main hubs.\n",
    "\n",
    "Node $4$\n",
    "\n",
    "Eigenvalues span ~ -10 to +10. Bigger magnitude eigenvalues indicate more strongly connected substructures and higher degree nodes. The large range + multiple spikes reflects several dense hubs and more global structure in the ego network.\n",
    "\n",
    "3. Spectral Clustering Shape\n",
    "\n",
    "Node $1$\n",
    "\n",
    "First two eigenvectors produce a clear V-shape, indicating 2–3 well-separated communities in embedding space. Suggests a simpler community split — likely one illicit-heavy cycle and one licit-heavy hub.\n",
    "\n",
    "Node $4$\n",
    "\n",
    "Clustering plot shows tighter, overlapping points but with a few outliers far from the core. Means most of the network is strongly interconnected, but some smaller communities (outliers) exist. This is consistent with the “big hubs + small satellites” structure you see in the ego-graph.\n",
    "\n",
    "4. Fraud-Detection Implications\n",
    "\n",
    "Node $1$: Smaller, more defined illicit cycle — potentially a local fraud ring.\n",
    "\n",
    "Node $4$: Large, hub-heavy network with mixed activity — potentially a central broker node connecting multiple illicit and licit subgroups, higher complexity, and potentially higher reach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyYlEcaYEinV"
   },
   "source": [
    "## Degree Centrality\n",
    "\n",
    "According to Newman **degree centrality** measures how many direct connections a node has compared to the maximum possible.\n",
    "\n",
    "For an undirected graph with $n$ nodes and $m$ edges the mean degree is\n",
    "\n",
    "$$ c = \\frac{1}{n} \\sum_{i=1}^{n} k_i = \\frac{2m}{n} $$\n",
    "\t​\n",
    "\n",
    "\n",
    "where $k_i$ is the degree of node $i$. Normalizing by the maximum possible degree $(n−1)$ gives\n",
    "\n",
    "$$ \\rho = \\frac{c}{n-1} , \\rho \\in [0,1] $$\n",
    "\n",
    "This $\\rho$ is also known as **graph density**—the fraction of realized edges out of all possible edges.\n",
    "\n",
    "See Newman, *Networks: An Introduction* [5, p. 133].\n",
    "\n",
    "Degree centrality is a simple but powerful indicator of how **connected** a node is in the network. In fraud analysis, unusually high degree nodes may act as hubs in illicit flows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bneex6-ILXKo"
   },
   "outputs": [],
   "source": [
    "# degree centrality\n",
    "deg_centrality = nx.degree_centrality(G_undirected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC9ipu8hjd-W"
   },
   "outputs": [],
   "source": [
    "df_scores = df_scores.merge(pd.DataFrame(deg_centrality.items(), columns=['txId', 'deg_centrality']), on='txId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E787rTzq16e_",
    "outputId": "15d4be05-0dc7-4276-8862-d509652733a0"
   },
   "outputs": [],
   "source": [
    "# mean of degree centrality\n",
    "mean_deg_centrality = sum(deg_centrality.values()) / len(deg_centrality)\n",
    "print(f\"Mean Degree Centrality: {mean_deg_centrality:.4f}\")\n",
    "\n",
    "# density \\rho\n",
    "density = nx.density(G_undirected)\n",
    "print(f\"Network Density: {density:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b6Z3YxgAzFf",
    "outputId": "18320f48-4e4a-4bbc-ae8d-964c77c758c6"
   },
   "outputs": [],
   "source": [
    "print(\"Nodes:\", G_undirected.number_of_nodes())\n",
    "print(\"Edges:\", G_undirected.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7MJ4salDWzA"
   },
   "source": [
    "The last two code these are very **sparse**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHaeNiwjHrmI"
   },
   "source": [
    "## Degree Distribution and Power Laws in Directed Graphs\n",
    "\n",
    "### Background\n",
    "\n",
    "We follow Newman's treatment (Section 8.4) of power-law degree distributions in real-world networks. Many systems, such as the World Wide Web, exhibit degree distributions of the form:\n",
    "\n",
    "\n",
    "$$p_k = C k^{-\\alpha} $$\n",
    "\n",
    "This implies that most nodes have low degree, but a few nodes (hubs) have very high degree. Such distributions are called **scale-free** and appear linear on a log-log plot.\n",
    "\n",
    "### In-Degree and Out-Degree Distributions\n",
    "\n",
    "Since our transaction graph is **directed**, each node has:\n",
    "\n",
    "- In-degree: number of incoming edges\n",
    "- Out-degree: number of outgoing edges\n",
    "\n",
    "We compute both distributions using NetworkX:\n",
    "\n",
    "`in_degrees = dict(G.in_degree())`\n",
    "and\n",
    "`out_degrees = dict(G.out_degree())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "gJyWP_hKCYnL",
    "outputId": "b3921055-a636-4079-94e5-2f2a6daa3f43"
   },
   "outputs": [],
   "source": [
    "# Calculate degrees\n",
    "degrees = [degree for node, degree in G_undirected.degree()]\n",
    "\n",
    "degree_counts = np.bincount(degrees)\n",
    "degree_values = np.arange(len(degree_counts))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.loglog(degree_values[degree_counts > 0], degree_counts[degree_counts > 0], marker='o', linestyle='None')\n",
    "plt.title(\"Log-Log Degree Distribution\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxauUkJDQBP0"
   },
   "source": [
    "**Overall Degree Distribution** (log–log):\n",
    "\n",
    "The Elliptic graph exhibits a heavy-tailed distribution, consistent with scale-free behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "sd88psDVHQ4l",
    "outputId": "43b3bfa4-2198-4622-877c-63b22e81d72c"
   },
   "outputs": [],
   "source": [
    "# Calculate in-degree and out-degree for each node\n",
    "in_degrees = dict(G_directed.in_degree())\n",
    "out_degrees = dict(G_directed.out_degree())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.kdeplot(np.log1p(list(in_degrees.values())), label='In-degree', color='blue', fill=True)\n",
    "sns.kdeplot(np.log1p(list(out_degrees.values())), label='Out-degree', color='orange', fill=True)\n",
    "\n",
    "# Set log transformed x-axis ticks back to real on and out values\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{np.expm1(x):.0f}\"))\n",
    "\n",
    "plt.title(\"Distribution of In-Degree and Out-Degree\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"In/Out Degree Distributions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLdnLgSVLF1r"
   },
   "source": [
    "Here are the log log which power laws using of a straight line in the visual plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "id": "MmbhnK2IH2md",
    "outputId": "7216e76e-123b-42c0-806a-2cd73d61cff6"
   },
   "outputs": [],
   "source": [
    "def plot_log_log(degree_dict, title):\n",
    "    count = Counter(degree_dict.values())\n",
    "    x, y = zip(*sorted(count.items()))\n",
    "    x = np.array(x)\n",
    "    y = np.array(y) / sum(y)  # normalize to get p_k\n",
    "    plt.loglog(x, y, marker='o', linestyle='None')\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"p(k)\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_log_log(in_degrees, \"In-Degree Distribution (log-log)\")\n",
    "plot_log_log(out_degrees, \"Out-Degree Distribution (log-log)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSzHl7OCNnxx"
   },
   "source": [
    "**In-Degree Distribution** (log–log):\n",
    "\n",
    "Most transactions receive funds from only a few sources (low in-degree), while a small number of nodes receive funds from hundreds of sources. This **heavy-tailed** pattern suggests the presence of **hub** accounts that aggregate flows—potential candidates for fraud rings or laundering activity.\n",
    "\n",
    "**Out-Degree Distribution** (log–log):\n",
    "\n",
    "Similarly, most transactions send funds to only one or two neighbors, but a few nodes disperse funds to many addresses. These high out-degree **fan-out** nodes may represent distribution points in illicit activity, pushing money across many paths at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwVEuHlsV7PQ",
    "outputId": "f28a13de-265f-4857-a45f-721b4c5b747d"
   },
   "outputs": [],
   "source": [
    "# Build degree distribution (using undirected for simplicity here)\n",
    "degrees = [d for n, d in G_undirected.degree()]\n",
    "count = Counter(degrees)\n",
    "\n",
    "# Extract x = degree values, y = frequencies (normalized as probabilities)\n",
    "x, y = zip(*sorted(count.items()))\n",
    "x = np.array(x)\n",
    "y = np.array(y) / sum(y)   # probability distribution p(k)\n",
    "\n",
    "# Fit linear regression in log space for rough slope estimate\n",
    "mask = (x > 0) & (y > 0)\n",
    "coeffs = np.polyfit(np.log10(x[mask]), np.log10(y[mask]), 1)\n",
    "slope = coeffs[0]\n",
    "print(f\"Estimated slope (gamma) ~ {-slope:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t0SoKdWYNzh"
   },
   "source": [
    "Exponent estimate. Fitting a line to the log–log degree distribution tail gives $ \\alpha \\approx 2.14 $ This sits in the typical range $ 2 < \\alpha < 3 $ reported by Newman for many real-world networks, indicating a heavy tail with hubs but not so extreme that a few nodes absorb almost all connectivity.\n",
    "\n",
    "See [5, p. 248]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klBKKSUA89Gm"
   },
   "source": [
    "## Betweenness Centrality\n",
    "\n",
    "We compute node betweenness centrality on an undirected, unweighted graph using `igraph`’s implementation of the **Brandes** algorithm. In the unweighted case, shortest paths are obtained via **breadth-first search** (BFS); if edge weights are supplied, `igraph` switches to **Dijkstra’s** algorithm. Brandes then performs a dependency **back-propagation** step to accumulate, for each node, how often it lies on all-pairs shortest paths. The runtime is $O(nm)$ for unweighted graphs.\n",
    "\n",
    "Nodes with high betweenness often act as bridges between communities. In fraud analysis, such nodes can represent **choke-points** or brokers that connect otherwise separated clusters of transactions.\n",
    "\n",
    "See Brandes, *A Faster Algorithm for Betweenness Centrality* [13]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2AAZFbtmXvB"
   },
   "outputs": [],
   "source": [
    "# Brandes algorithm\n",
    "\n",
    "# Build igraph graph and no weights\n",
    "edges = list(zip(df_edgelist['source'], df_edgelist['target']))\n",
    "g = Graph.TupleList(edges, directed=False)\n",
    "\n",
    "# Compute betweenness\n",
    "btw_scores = g.betweenness()\n",
    "vertex_names = g.vs['name']\n",
    "\n",
    "# Save to DataFrame\n",
    "btw_df = pd.DataFrame({'txId': vertex_names, 'betweenness': btw_scores})\n",
    "\n",
    "# 2. Merge betweenness\n",
    "btw_df['txId'] = btw_df['txId'].astype(str)\n",
    "df_scores = df_scores.merge(btw_df, on='txId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiO8WM-Ddsyt"
   },
   "source": [
    "Histogram of betweenness centrality and shows the skew of most nodes have very low betweenness, a few have very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "7tGymhU5dAIa",
    "outputId": "ff120fcf-144e-4b33-ba61-28cc97881927"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(btw_scores, bins=100, color=\"steelblue\", alpha=0.8)\n",
    "plt.xlabel(\"Betweenness Centrality\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.title(\"Distribution of Betweenness Centrality\")\n",
    "plt.yscale(\"log\")  # often highly skewed, log scale helps\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H6mYb0jdRky"
   },
   "source": [
    "Most nodes rarely appear on shortest paths, but a small fraction have very high betweenness. These high-betweenness **bridges** may represent brokers or choke points in laundering flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 974
    },
    "id": "J5tWsdWjk1Rs",
    "outputId": "5c7d86a4-74cb-48d2-c34e-a93c68009e56"
   },
   "outputs": [],
   "source": [
    "# Pick a node of interest (e.g. highest betweenness in full graph)\n",
    "center_node = vertex_names[np.argmax(btw_scores)]\n",
    "\n",
    "# Ego network: all nodes within 2 steps\n",
    "ego_nodes = g.neighborhood(center_node, order=2)\n",
    "subG = g.subgraph(ego_nodes)\n",
    "\n",
    "# --- Prep sizes (betweenness) ---\n",
    "btw_sub = np.array(subG.betweenness())\n",
    "# robust scaling to avoid \"all purple\" effect\n",
    "p5, p95 = np.percentile(btw_sub, [5, 95])\n",
    "btw_scaled = np.clip((btw_sub - p5) / (p95 - p5 + 1e-9), 0, 1)\n",
    "sizes = 6 + 20 * btw_scaled  # small base + emphasis on higher btw\n",
    "\n",
    "# --- Color by class (objective-aligned) ---\n",
    "# Build txId -> class map (licit/illicit/unknown) once earlier in your notebook\n",
    "# class_map = dict(zip(df_classes['txId'].astype(str), df_classes['class']))\n",
    "\n",
    "# igraph vertex names are strings\n",
    "labels = [class_map.get(v['name'], 'unknown') for v in subG.vs]\n",
    "color_map = {'illicit': '#d62728', 'licit': '#2ca02c', 'unknown': '#999999'}\n",
    "colors = [color_map.get(lbl, '#999999') for lbl in labels]\n",
    "\n",
    "layout = subG.layout(\"fr\")\n",
    "plot(\n",
    "    subG,\n",
    "    layout=layout,\n",
    "    vertex_size=sizes,        # size encodes betweenness\n",
    "    vertex_color=colors,      # color encodes class\n",
    "    edge_color=\"lightgray\",\n",
    "    bbox=(700,700)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbt6qVqyiRIx"
   },
   "source": [
    "Betweenness x Class (ego network):\n",
    "\n",
    "Node size encodes betweenness centrality; color encodes class (green=licit, red=illicit, gray=unknown). Large nodes act as structural bridges. Large gray or red nodes are especially interesting: they may represent chokepoints in laundering flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "50Nw6bQPeX_e",
    "outputId": "d927cdec-05f4-4d8c-8751-0bfedfb2173a"
   },
   "outputs": [],
   "source": [
    "top_btw = btw_df.sort_values(\"betweenness\", ascending=False).head(10)\n",
    "display(top_btw.merge(df_classes, on=\"txId\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLhn2cg3nBbC"
   },
   "source": [
    "Top betweenness nodes. Most of the **highest-betweenness** nodes are labeled licit (likely exchanges/aggregators), but a few unknown nodes rank near the top. Unknown chokepoints are high-priority review candidates because they sit on many transaction paths and can serve as covert **brokers** in **laundering flows**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmhRXqwj3Wrl"
   },
   "source": [
    "## Eigenvector Centrality\n",
    "\n",
    "Scores each node in proportion to the sum of its neighbors’ scores. In matrix form it’s the dominant eigenvector $x$ of the adjacency $A$ solving $ A x = \\lambda x$. `NetworkX` uses **power iteration**:\n",
    "\n",
    "$$ \\mathbf x_{k+1} = \\frac{\\mathbf A \\mathbf x_k}{\\left\\| \\mathbf A \\mathbf x_k \\right\\|} $$\n",
    "\n",
    "until convergence.\n",
    "\n",
    "\n",
    "Flags nodes embedded in well-connected cores (connected to other **central nodes**). In fraud graphs, these can be **ring leaders** or exchange-like **hubs—complements** degree (local) and betweenness (bridges).\n",
    "\n",
    "See [7, p. 205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffrAuZpAps38"
   },
   "outputs": [],
   "source": [
    "# compute eigenvector centrality\n",
    "eig_centrality = nx.eigenvector_centrality(\n",
    "    G_undirected,\n",
    "    max_iter=5000,\n",
    "    tol=1e-4\n",
    ")\n",
    "\n",
    "# convert to DataFrame\n",
    "eig_centrality_df = pd.DataFrame({\n",
    "    'txId': list(eig_centrality.keys()),\n",
    "    'eigenvector': list(eig_centrality.values())\n",
    "})\n",
    "\n",
    "# merge into scores\n",
    "eig_centrality_df['txId'] = eig_centrality_df['txId'].astype(str)\n",
    "df_scores = df_scores.merge(eig_centrality_df, on='txId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "hFadPytH-xGU",
    "outputId": "faed0dcb-ec48-4d90-8127-284d91c2d5e9"
   },
   "outputs": [],
   "source": [
    "# Subgraph to visualize: 2-hop ego of a top eigenvector node\n",
    "top_node = max(eig_centrality, key=eig_centrality.get)\n",
    "G_giant = G_undirected\n",
    "ego = nx.ego_graph(G_giant, top_node, radius=2)\n",
    "\n",
    "# Robust size scaling\n",
    "vals = np.array([eig_centrality.get(n, 0.0) for n in ego.nodes()])\n",
    "p5, p95 = np.percentile(vals, [5, 95])\n",
    "sz = 50 + 400 * np.clip((vals - p5) / (p95 - p5 + 1e-12), 0, 1)\n",
    "\n",
    "# Color by class\n",
    "color_map = {'illicit': '#d62728', 'licit': '#2ca02c', 'unknown': '#999999'}\n",
    "colors = [color_map.get(class_map.get(str(n), 'unknown'), '#999999') for n in ego.nodes()]\n",
    "\n",
    "pos = nx.spring_layout(ego, seed=42)\n",
    "plt.figure(figsize=(8,8))\n",
    "nx.draw_networkx_edges(ego, pos, edge_color='lightgray', alpha=0.5, width=0.5)\n",
    "nx.draw_networkx_nodes(ego, pos, node_color=colors, node_size=sz)\n",
    "plt.title(\"Eigenvector Centrality × Class (ego network)\")\n",
    "plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2icGT-CDvI-"
   },
   "source": [
    "Eigenvector × Class (ego network). Node size ∝ eigenvector centrality (importance in a well-connected core); color encodes class (green=licit, red=illicit, gray=unknown). Large gray/red nodes near other large nodes are high-value review targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "KbCYM4OtnWf_",
    "outputId": "409479bb-c2af-41c5-c348-69b36b9d7774"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df_scores['eigenvector'], bins=100, log_scale=True)\n",
    "plt.title(\"Distribution of Eigenvector Centrality\")\n",
    "plt.xlabel(\"Eigenvector Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsJdbs6l-UZP"
   },
   "source": [
    "Distribution is **heavy-tailed**: most nodes have tiny scores; a minority sit in highly central cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxiofCMfD5_1"
   },
   "source": [
    "## Homophily and Assortative Mixing\n",
    "\n",
    "We will use a 1 hop and compute `illicit` nodes on the top divided by 3 classes of a 1 hop nodes:\n",
    "\n",
    "$$ r_i = \\frac{\\text{illicit neighbors}(n_i)}{\\text{all neighbors}(n_i)} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $n_i$ is ego node, and only have illicit neighbors\n",
    "- $\\text{all neighbors}(n_i)$ is $\\text{deg}(n_i)$ for all `'classes'` in the neighbors.\n",
    "- $r_i$ is a ratio from $\\text{illicit neighbors} / \\text{all neighbors}$\n",
    "\n",
    "This gives a local homophily signal you can feed into models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8NYKIcwDzEw"
   },
   "outputs": [],
   "source": [
    "# % of direct neighbors that are illicit\n",
    "def neighbor_illicit_ratio(node):\n",
    "    neighbors = list(G_undirected.neighbors(node))\n",
    "    illicit = sum(1 for n in neighbors if class_map.get(n) == 'illicit')\n",
    "    return illicit / len(neighbors) if neighbors else 0\n",
    "\n",
    "df_scores['neighbors_illicit_ratio'] = df_scores['txId'].map(neighbor_illicit_ratio).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXUrD78GZirs"
   },
   "source": [
    "Here is the top to bottom for the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "nU6yMKlyBOBp",
    "outputId": "da18d9bd-57af-45af-ea17-930f02490b28"
   },
   "outputs": [],
   "source": [
    "df_scores[['label', 'neighbors_illicit_ratio']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBZ2l1-VZsuS"
   },
   "source": [
    "Here is the sort values top to the bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "H61ALSJXHhxW",
    "outputId": "ca5ae414-70f3-4c21-c57d-9b99f81e879f"
   },
   "outputs": [],
   "source": [
    "df_scores[['label', 'neighbors_illicit_ratio']].sort_values(by='neighbors_illicit_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWKlndljZ2xC"
   },
   "source": [
    "Here it is a plot using a licit which a high probability of illicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EX9ZG_TlR4U8",
    "outputId": "15edfe57-e781-4038-b84c-cf3f25af85e7"
   },
   "outputs": [],
   "source": [
    "def plot_one_hop(G, center, class_map):\n",
    "    H = nx.ego_graph(G, center, radius=1, undirected=True)\n",
    "    labels = {n: class_map.get(n, 'unknown') for n in H.nodes()}\n",
    "\n",
    "    color_map = {'illicit': 'red', 'licit': 'green', 'unknown': 'gray'}\n",
    "    node_colors = [color_map.get(labels[n], 'gray') for n in H.nodes()]\n",
    "    node_sizes  = [300 if n == center else 80 for n in H.nodes()]\n",
    "\n",
    "    pos = nx.spring_layout(H, seed=0)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    nx.draw_networkx_edges(H, pos, alpha=0.3)\n",
    "    nx.draw_networkx_nodes(H, pos, node_color=node_colors, node_size=node_sizes)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # stats for the title\n",
    "    nbrs = [n for n in H.nodes() if n != center]\n",
    "    cnt = Counter(labels[n] for n in nbrs)\n",
    "    denom = len(nbrs)\n",
    "    ratio = 0.0 if denom == 0 else cnt['illicit'] / denom\n",
    "    plt.title(f\"1-hop ego graph for {center}: illicit {cnt['illicit']}/{denom} = {ratio:.2f}\")\n",
    "\n",
    "    # legend\n",
    "    legend_elems = [\n",
    "        Line2D([0],[0], marker='o', color='w', label='illicit', markerfacecolor='red', markersize=8),\n",
    "        Line2D([0],[0], marker='o', color='w', label='licit', markerfacecolor='green', markersize=8),\n",
    "        Line2D([0],[0], marker='o', color='w', label='unknown', markerfacecolor='gray', markersize=8),\n",
    "    ]\n",
    "    plt.legend(handles=legend_elems, loc='best')\n",
    "    plt.show()\n",
    "\n",
    "# Find licit nodes with high illicit neighbor ratio,\n",
    "# Ratio =< 0.85, 5 =< Neighborhood\n",
    "cand_high = df_scores[\n",
    "    (df_scores['label'] == 'licit') &\n",
    "    (df_scores['neighbors_illicit_ratio'] >= 0.85) &\n",
    "    (df_scores['degree'] >= 5)\n",
    "].copy()\n",
    "\n",
    "# grab some nodes to inspect\n",
    "sample_nodes = cand_high['txId'].head(6).tolist()\n",
    "print(\"Sample licit nodes with high illicit ratio:\", sample_nodes)\n",
    "\n",
    "\n",
    "# plot\n",
    "for node_id in sample_nodes:\n",
    "    plot_one_hop(G_undirected, node_id, class_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx_4kpyyZU7O"
   },
   "source": [
    "You can see that the node is the big ego and you can see node `179112629` has only one of $2$ `unknowns` and $27$ `illicit`.\n",
    "\n",
    "Node `179113037` has $2$ `licit`, $3$ `unknowns` and $32$ `illicit`.\n",
    "\n",
    "Definitely suspicious, And a lot of the same actors or multiple hits. These local homophily ratios highlight structural red flags: licit-labeled nodes embedded in predominantly illicit neighborhoods. Such nodes may indicate “front” accounts or compromised intermediaries, making them valuable for fraud detection models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgnrNejEjB5b"
   },
   "source": [
    "## PageRank\n",
    "\n",
    "We computed PageRank on the **directed** transaction graph $G$, using the standard **damping factor** $\\alpha = 0.85$, which corresponds to the **random surfer model**. Convergence was determined by setting the tolerance proportional to the number of nodes `(tolerance = |G| x tol)`.\n",
    "\n",
    "PageRank was originally designed to evaluate the importance of web pages through their hyperlink structure, forming the basis of early search engines. In this context, we adapt the method to transaction networks, where high PageRank values may highlight influential or suspicious transactions.\n",
    "\n",
    "See Page, Brin, Motwani, and Winograd, *The PageRank Citation Ranking: Bringing Order to the Web* [15] and Hagberg, Schult, and Swart, *Exploring network structure, dynamics, and function using NetworkX* [16]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "lZr4v4RWjKwG",
    "outputId": "b1cccc45-1194-4197-ba64-2ddc35f68479"
   },
   "outputs": [],
   "source": [
    "# Check G directed is the big graph for pagerank\n",
    "try:\n",
    "    G_directed\n",
    "except NameError:\n",
    "    raise ValueError(\"Please load your real Elliptic graph object `G` before continuing.\")\n",
    "\n",
    "# Compute PageRank on full graph\n",
    "pagerank_scores = nx.pagerank(G_directed, alpha=0.85)\n",
    "\n",
    "# Convert to DataFrame\n",
    "pagerank_df = pd.DataFrame({\n",
    "    'txId': list(pagerank_scores.keys()),\n",
    "    'pagerank': list(pagerank_scores.values())\n",
    "})\n",
    "\n",
    "# Display top 10 PageRank nodes\n",
    "top_pagerank = pagerank_df.sort_values(by='pagerank', ascending=False).head(10)\n",
    "\n",
    "# Plot top 10\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(top_pagerank['txId'].astype(str), top_pagerank['pagerank'], color='darkorange')\n",
    "plt.title(\"Top 10 Nodes by PageRank (Elliptic Graph)\")\n",
    "plt.xlabel(\"Transaction ID\")\n",
    "plt.ylabel(\"PageRank Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REbaeOyjwAW_"
   },
   "outputs": [],
   "source": [
    "# add scores.pd.DataFrame([])\n",
    "df_scores = df_scores.merge(pagerank_df, on='txId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZW6FUH5sKta"
   },
   "source": [
    "Here is the top node; `163832295` and extracted its 3-hop neighborhood (both in- and out-edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "fxlC_CsPu4WC",
    "outputId": "f10a4b63-421b-47ec-d570-14bc0733da81"
   },
   "outputs": [],
   "source": [
    "# Ensure consistent types\n",
    "df_classes['txId'] = df_classes['txId'].astype(str)\n",
    "class_map = dict(zip(df_classes['txId'], df_classes['class']))\n",
    "target_node = str(top_pagerank['txId'].iloc[0])\n",
    "\n",
    "# in and out:\n",
    "def k_hop_nodes_bidirectional(G, node, k=3):\n",
    "    UG = G.to_undirected()\n",
    "    return set(nx.single_source_shortest_path_length(UG, node, cutoff=k).keys())\n",
    "\n",
    "# Collect nodes within k hops (both directions), then induce directed subgraph\n",
    "k = 3\n",
    "nodes_k = k_hop_nodes_bidirectional(G_directed, target_node, k=k)\n",
    "subG = G_directed.subgraph(nodes_k).copy()\n",
    "\n",
    "# Build colors and sizes\n",
    "def node_color(n):\n",
    "    lab = class_map.get(str(n), 'unknown')\n",
    "    return 'red' if lab == 'illicit' else ('green' if lab == 'licit' else 'gray')\n",
    "\n",
    "colors = [node_color(n) for n in subG.nodes()]\n",
    "sizes  = [220 if str(n) == target_node else 40 for n in subG.nodes()]\n",
    "\n",
    "# Layout + draw\n",
    "plt.figure(figsize=(9,9))\n",
    "pos = nx.spring_layout(subG, k=0.15, seed=42)\n",
    "\n",
    "nx.draw_networkx_nodes(subG, pos, node_color=colors, node_size=sizes)\n",
    "nx.draw_networkx_edges(subG, pos, arrows=True, edge_color='lightgray', width=0.6, alpha=0.7, arrowsize=8)\n",
    "# Optionally label just the ego\n",
    "nx.draw_networkx_labels(subG, pos, labels={target_node: target_node}, font_size=8)\n",
    "\n",
    "plt.title(f\"{k}-Hop Neighborhood of Top PageRank Node {target_node} (directed edges)\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Simple legend\n",
    "\n",
    "leg_items = [\n",
    "    mlines.Line2D([], [], color='red', marker='o', linestyle='None', label='illicit'),\n",
    "    mlines.Line2D([], [], color='green', marker='o', linestyle='None', label='licit'),\n",
    "    mlines.Line2D([], [], color='gray', marker='o', linestyle='None', label='unknown'),\n",
    "\n",
    "]\n",
    "plt.legend(handles=leg_items, frameon=False, loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7S5R74HzVp_"
   },
   "source": [
    "We observe that the node acts as a hub, connecting to clusters of both licit and illicit transactions. This mixture suggests that while PageRank emphasizes global influence, k-hop neighborhood analysis provides local signals that can be used as features in downstream models (e.g., fraction illicit within 3 hops)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTnuwshjqPW5"
   },
   "source": [
    "## Suspicious Cycle Score\n",
    "\n",
    "We compute a Suspicious Cycle Score using the `nx.cycle_basis(G_undirected)` function from `NetworkX`, which implements a classic algorithm for extracting a fundamental set of cycles from a spanning tree. Again, this method was first described by K. Paton in 1969, originally implemented in Fortran IV on IBM systems, and remains widely used today for efficient cycle detection [17].\n",
    "\n",
    "We compute suspicious cycle scores using `nx.cycle_basis` and a cycle basis is not the set of all cycles but a minimal generating set. Different DFS roots may produce different—but equally valid—bases. Since our focus is on cycles enriched in illicit activity, we adopt Paton’s method for efficiency, while noting that alternative bases (e.g., the minimum cycle basis) exist.\n",
    "\n",
    "In the context of fraud detection, cycles in transaction networks can indicate money laundering or circular flows of illicit funds. To adapt Paton’s approach, we identify cycles containing a high concentration of illicit transactions. Unknown transactions that appear within these cycles are flagged with higher scores, since their proximity to repeated illicit activity makes them more suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_gxyW0TqRkF"
   },
   "outputs": [],
   "source": [
    "# Initialize counter\n",
    "suspicious_unknowns = Counter()\n",
    "\n",
    "# Get all simple cycles (undirected)\n",
    "undirected_cycles = nx.cycle_basis(G_undirected)\n",
    "\n",
    "# Scan each cycle\n",
    "for cycle in undirected_cycles:\n",
    "    illicit_count = sum(1 for node in cycle if class_map.get(node) == 'illicit')\n",
    "    if illicit_count >= 5:\n",
    "        for node in cycle:\n",
    "            if class_map.get(node) == 'unknown':\n",
    "                suspicious_unknowns[node] += 1\n",
    "\n",
    "# Add to scores\n",
    "df_scores['suspicious_cycle_score'] = df_scores['txId'].map(suspicious_unknowns).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "id": "ENIFJnw7tyWE",
    "outputId": "640fca0c-5344-4c0a-adea-2c919c88d25e"
   },
   "outputs": [],
   "source": [
    "# Filter for suspicious unknown nodes with high degree\n",
    "suspicious_high_deg_unknowns = df_scores[\n",
    "    (df_scores['label'] == 'unknown') &\n",
    "    (df_scores['degree'] >= 100) &\n",
    "    (df_scores['suspicious_cycle_score'] > 0)\n",
    "]\n",
    "\n",
    "# Show top suspicious unknowns\n",
    "suspicious_high_deg_unknowns.sort_values(by='suspicious_cycle_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 888
    },
    "id": "WnK3Z4vFve4K",
    "outputId": "d9b23914-8187-4bbd-ce41-b63a465dab95"
   },
   "outputs": [],
   "source": [
    "target = str(355174807)\n",
    "\n",
    "# Get 3-hop neighbors (BFS depth=3)\n",
    "neighbors_3hop = nx.single_source_shortest_path_length(G_undirected, target, cutoff=3)\n",
    "sub_nodes = list(neighbors_3hop.keys())\n",
    "subgraph = G_undirected.subgraph(sub_nodes)\n",
    "\n",
    "# Color by label\n",
    "color_map = []\n",
    "for node in subgraph:\n",
    "    label = class_map.get(node, 'unknown')\n",
    "    if label == 'illicit':\n",
    "        color_map.append('red')\n",
    "    elif label == 'licit':\n",
    "        color_map.append('green')\n",
    "    else:\n",
    "        color_map.append('gray')\n",
    "\n",
    "# Draw\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw_spring(subgraph, node_color=color_map, with_labels=False, node_size=30)\n",
    "plt.title(\"3-Hop Neighborhood of Node 355174807\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSR-ofTByQGc"
   },
   "source": [
    "That is definitely a hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_S_NXqZ1MvE",
    "outputId": "127e1ccc-41ab-4526-8b6b-df169c2dd454"
   },
   "outputs": [],
   "source": [
    "# This will give you all simple cycles in the **undirected** graph\n",
    "all_cycles = cycle_basis(G_undirected)\n",
    "\n",
    "target = str(355174807)\n",
    "\n",
    "cycles_with_target = [\n",
    "    c for c in all_cycles\n",
    "    if target in c and sum(1 for n in c if class_map.get(n) == 'illicit') >= 5\n",
    "]\n",
    "\n",
    "print(f\"Found {len(cycles_with_target)} illicit-heavy cycles involving node {target}\")\n",
    "for i, c in enumerate(cycles_with_target[:3]):  # Show just a few\n",
    "    print(f\"Cycle {i+1}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "id": "QiE6gKb21loD",
    "outputId": "9ec67721-2bfb-45e4-e9b8-e727b3f763a9"
   },
   "outputs": [],
   "source": [
    "cycle_subgraph = G_undirected.subgraph(cycles_with_target[0])\n",
    "\n",
    "color_map = [\n",
    "    'red' if class_map.get(n) == 'illicit'\n",
    "    else 'green' if class_map.get(n) == 'licit'\n",
    "    else 'gray'\n",
    "    for n in cycle_subgraph\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "nx.draw_circular(cycle_subgraph, node_color=color_map, with_labels=True, node_size=300)\n",
    "plt.title(\"Example Illicit-Heavy Cycle Involving Target Node\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTjp5Ex7e1Ue"
   },
   "source": [
    "## Louvain Community Detection\n",
    "\n",
    "We applied Louvain clustering to partition the transaction graph into communities. The Louvain method is a greedy optimization algorithm that maximizes modularity, a measure of the density of edges inside a community compared to edges between communities. Originally introduced by Blondel et al. in 2008, it remains one of the most widely used methods for large-scale community detection due to its efficiency and scalability [18].\n",
    "\n",
    "In fraud detection, community detection is valuable because illicit transactions often cluster together in hidden groups. By identifying communities with a high concentration of illicit activity, we can flag unknown transactions in the same cluster as potentially suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 888
    },
    "id": "zt7kMJEXx4Nk",
    "outputId": "410f83aa-1dd9-4937-d78c-b1dd0612e452"
   },
   "outputs": [],
   "source": [
    "# Run Louvain clustering on undirected graph\n",
    "partition = community_louvain.best_partition(G_undirected)\n",
    "\n",
    "# Get cluster of the suspicious node\n",
    "target_cluster_id = partition.get(target)\n",
    "target_cluster_nodes = [n for n, cid in partition.items() if cid == target_cluster_id]\n",
    "\n",
    "# add as a feature\n",
    "df_scores['louvain_cluster'] = df_scores['txId'].map(partition).fillna(-1).astype(int)\n",
    "\n",
    "# Visualize that community\n",
    "subgraph_cluster = G_undirected.subgraph(target_cluster_nodes)\n",
    "\n",
    "# color\n",
    "color_map = [\n",
    "    'red' if class_map.get(n) == 'illicit'\n",
    "    else 'green' if class_map.get(n) == 'licit'\n",
    "    else 'gray'\n",
    "    for n in subgraph_cluster\n",
    "]\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw_spring(subgraph_cluster, node_color=color_map, with_labels=False, node_size=30)\n",
    "plt.title(f\"Louvain Cluster of Node {target}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "UEyRJXcsdwON",
    "outputId": "7add04d5-4b93-4ee9-966a-a0c41680e9ec"
   },
   "outputs": [],
   "source": [
    "# 1) Louvain partition (works on undirected)\n",
    "partition = community_louvain.best_partition(G_undirected)\n",
    "\n",
    "# 2) Pick the cluster for your target node\n",
    "#    (Ensure target exists and types match your graph's node types.)\n",
    "assert target in G_undirected, f\"{target} not in graph\"\n",
    "target_cluster_id = partition.get(target)\n",
    "target_cluster_nodes = [n for n, cid in partition.items() if cid == target_cluster_id]\n",
    "\n",
    "# 3) Add cluster id as a feature (optional)\n",
    "df_scores['louvain_cluster'] = df_scores['txId'].map(partition).fillna(-1).astype(int)\n",
    "\n",
    "# 4) Build the subgraph\n",
    "subgraph_cluster = G_undirected.subgraph(target_cluster_nodes).copy()\n",
    "\n",
    "# 5) Make colors IN THE SAME ORDER as the nodes you’ll draw\n",
    "nodes_in_order = list(subgraph_cluster.nodes())\n",
    "def node_color(n):\n",
    "    lbl = class_map.get(n, 'unknown')\n",
    "    if lbl == 'illicit': return 'red'\n",
    "    if lbl == 'licit':   return 'green'\n",
    "    return 'gray'\n",
    "\n",
    "color_map = [node_color(n) for n in nodes_in_order]\n",
    "\n",
    "# 6) Layout + draw\n",
    "plt.figure(figsize=(10, 10))\n",
    "pos = nx.spring_layout(subgraph_cluster, seed=42)  # deterministic layout\n",
    "nx.draw_networkx_nodes(subgraph_cluster, pos, nodelist=nodes_in_order,\n",
    "                       node_color=color_map, node_size=18, alpha=0.9)\n",
    "nx.draw_networkx_edges(subgraph_cluster, pos, width=0.3, alpha=0.4)\n",
    "plt.title(f\"Louvain Cluster of Node {target} (size={len(nodes_in_order)})\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLIi9KBk0LyZ"
   },
   "source": [
    "Louvain Cluster of Node 355174807\n",
    "\n",
    "This cluster represents a large community of over 4,000 nodes detected using the Louvain algorithm. The structure is densely interconnected, with noticeable sub-clusters forming around illicit nodes (red). The mix of licit (green), illicit (red), and unknown (gray) nodes illustrates heavy traffic between all three classes, but the concentration of illicit nodes in localized “hotspots” suggests areas of heightened fraud activity within the broader community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj05Inxt2X88"
   },
   "source": [
    "## Clustering Coefficient\n",
    "\n",
    "The clustering coefficient measures the extent to which a node’s neighbors are connected to each other, forming local triangles in the network. Originally popularized by Watts and Strogatz in their small-world network model [19], the formal definition of local clustering and redundancy is detailed by Newman [5].\n",
    "\n",
    "In transaction graphs, a high clustering coefficient may suggest collusion or tightly organized fraud rings. Conversely, a low coefficient may indicate more random or opportunistic transactions. Incorporating this metric provides a local structural signal for distinguishing between licit and illicit behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CThOmGS2U8z"
   },
   "outputs": [],
   "source": [
    "clustering = nx.clustering(G_undirected)\n",
    "df_scores['clustering_coef'] = df_scores['txId'].map(clustering).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QotBohXegzFk"
   },
   "outputs": [],
   "source": [
    "# small clustering coefficient\n",
    "def find_and_plot_cliques(\n",
    "    G, class_map=None, min_k=3, max_k=7, target=None, max_plots=4, seed=42\n",
    "):\n",
    "    # 1) Enumerate cliques up to size max_k\n",
    "    clqs = []\n",
    "    for c in nx.enumerate_all_cliques(G_undirected):\n",
    "        if len(c) > max_k:\n",
    "            break  # enumerate_all_cliques yields in nondecreasing size\n",
    "        if min_k <= len(c) <= max_k:\n",
    "            clqs.append(c)\n",
    "\n",
    "    # 2) (Optional) prioritize cliques that contain the target\n",
    "    if target is not None and target in G_undirected:\n",
    "        clqs_target = [c for c in clqs if target in c]\n",
    "        clqs_other  = [c for c in clqs if target not in c]\n",
    "        clqs = clqs_target + clqs_other  # target-first\n",
    "\n",
    "    # 3) Sort by size (bigger first) and keep top few\n",
    "    clqs.sort(key=len, reverse=True)\n",
    "    pick = clqs[:max_plots]\n",
    "\n",
    "    # 4) Quick summary of how many cliques by size\n",
    "    size_counts = Counter(map(len, clqs))\n",
    "    print(\"Cliques by size (within range):\", dict(sorted(size_counts.items())))\n",
    "\n",
    "    # 5) Plot each chosen clique as its own tiny subgraph\n",
    "    for i, c in enumerate(pick, 1):\n",
    "        H = G_undirected.subgraph(c).copy()\n",
    "\n",
    "        # Node colors by label (if provided)\n",
    "        if class_map:\n",
    "            def col(n):\n",
    "                lab = class_map.get(n)\n",
    "                return {'illicit':'#d62728','licit':'#2ca02c'}.get(lab, '#888888')\n",
    "            node_colors = [col(n) for n in H.nodes()]\n",
    "        else:\n",
    "            node_colors = '#1f77b4'\n",
    "\n",
    "        plt.figure(figsize=(3.2, 3.2))\n",
    "        pos = nx.circular_layout(H) if len(H) >= 5 else nx.spring_layout(H, seed=seed)\n",
    "        nx.draw(\n",
    "            H, pos,\n",
    "            with_labels=True, font_size=8,\n",
    "            node_color=node_colors, node_size=250,\n",
    "            edge_color=\"#999999\"\n",
    "        )\n",
    "        plt.title(f\"Clique #{i}: K_{len(H)}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# find_and_plot_cliques(G_undirected, class_map=class_map, min_k=3, max_k=6, target=target, max_plots=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EoeJy_fghVYG",
    "outputId": "e5b19d5c-c156-4cf1-ff4b-6792dd5b2a3a"
   },
   "outputs": [],
   "source": [
    "find_and_plot_cliques(G_undirected, class_map=class_map, min_k=3, max_k=7, target=target, max_plots=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZTxgIhOuj5n"
   },
   "source": [
    "This is a complete 6-node subgraph (clique), where every node connects to every other. Interestingly, the nodes here are not exclusively illicit or licit — it’s a fully connected structure without a clear fraud signature. Such cliques can still be useful signals, since they represent unusually dense transaction patterns that may not arise randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1i8qFc7jbBM"
   },
   "outputs": [],
   "source": [
    "def clique_stats_illicit(\n",
    "    G, class_map, min_k=3, max_k=8, min_illicit=1, show_examples=3, seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Count cliques by size and the subset that include >= min_illicit illicit nodes.\n",
    "    Optionally plot a few example illicit cliques.\n",
    "    \"\"\"\n",
    "    # Enumerate cliques up to max_k\n",
    "    all_cliques = []\n",
    "    illicit_cliques = []\n",
    "    for c in nx.enumerate_all_cliques(G_undirected):\n",
    "        k = len(c)\n",
    "        if k > max_k:\n",
    "            break\n",
    "        if k >= min_k:\n",
    "            all_cliques.append(c)\n",
    "            num_illicit = sum(1 for n in c if class_map.get(n) == 'illicit')\n",
    "            if num_illicit >= min_illicit:\n",
    "                illicit_cliques.append(c)\n",
    "\n",
    "    all_counts = Counter(map(len, all_cliques))\n",
    "    illicit_counts = Counter(map(len, illicit_cliques))\n",
    "\n",
    "    print(\"All cliques by size:\", dict(sorted(all_counts.items())))\n",
    "    print(f\"Cliques with ≥{min_illicit} illicit nodes:\", dict(sorted(illicit_counts.items())))\n",
    "\n",
    "    # Show a few example illicit cliques, largest first\n",
    "    illicit_cliques.sort(key=len, reverse=True)\n",
    "    for i, c in enumerate(illicit_cliques[:show_examples], 1):\n",
    "        H = G_undirected.subgraph(c).copy()\n",
    "        node_colors = [\n",
    "            {'illicit':'#d62728', 'licit':'#2ca02c'}.get(class_map.get(n), '#888888')\n",
    "            for n in H.nodes()\n",
    "        ]\n",
    "        plt.figure(figsize=(3.2, 3.2))\n",
    "        pos = nx.circular_layout(H) if len(H) >= 5 else nx.spring_layout(H, seed=seed)\n",
    "        nx.draw(H, pos, with_labels=True, font_size=8,\n",
    "                node_color=node_colors, node_size=260, edge_color=\"#999999\")\n",
    "        plt.title(f\"Illicit clique example: K_{len(H)}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return all_counts, illicit_counts\n",
    "\n",
    "# Example usage (undirected graph):\n",
    "# all_counts, illicit_counts = clique_stats_illicit(\n",
    "#     G_undirected, class_map, min_k=3, max_k=8, min_illicit=1, show_examples=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HBKbpPjbjuCm",
    "outputId": "08ed665f-7e0f-4094-e613-de429604d974"
   },
   "outputs": [],
   "source": [
    "clique_stats_illicit(G_undirected, class_map, min_k=3, max_k=8, min_illicit=1, show_examples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsUr4wdGvQCx"
   },
   "source": [
    "This smaller 3-node clique consists only of illicit transactions. The tight triangular pattern is highly suspicious: it may represent repeated circular transfers, coordinated activity, or laundering of funds among a small group of actors. These tightly knit structures highlight how clique complexes can reveal hidden fraud “cells” within the broader network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5DTS1RU2cRi"
   },
   "source": [
    "## Shortest Path to Illicit\n",
    "\n",
    "We compute the shortest path length from each node to the nearest known illicit transaction. This is a fundamental concept in graph theory, where the shortest path is the minimum number of edges required to travel between two nodes Newman p. 139-140 [5] and West p. 97, [10]. We will use `igraph` Which is faster than `networksx`.\n",
    "\n",
    "For fraud detection, proximity to known illicit nodes is a powerful signal: the closer a transaction is to confirmed fraud, the higher its likelihood of also being suspicious. This measure leverages the intuition that fraudulent activity tends to propagate locally within the network, forming dense clusters of illicit transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qy4jU5UHKFTJ",
    "outputId": "8db619f8-faae-4bd7-d558-79cead17be64"
   },
   "outputs": [],
   "source": [
    "# Example using your edgelist DataFrame\n",
    "# nx.Graph or DiGraph\n",
    "G_nx = nx.from_pandas_edgelist(df_edgelist, source='source', target='target', create_using=nx.Graph())\n",
    "\n",
    "# --- Convert NetworkX to iGraph ---\n",
    "# Ensure all node IDs are strings (igraph requires this)\n",
    "G_nx = nx.relabel_nodes(G_nx, lambda x: str(x))\n",
    "\n",
    "# Create igraph from edge list\n",
    "edges = list(G_nx.edges())\n",
    "G_ig = ig.Graph.TupleList(edges, directed=G_nx.is_directed())\n",
    "\n",
    "# Identify illicit nodes\n",
    "illicit_nodes = [str(n) for n, label in class_map.items() if label == 'illicit']\n",
    "\n",
    "# Prepare batch loop\n",
    "all_nodes = G_ig.vs['name']\n",
    "batch_size = 100\n",
    "\n",
    "for batch_num, i in enumerate(range(0, len(all_nodes), batch_size)):\n",
    "    batch = all_nodes[i:i + batch_size]\n",
    "\n",
    "    try:\n",
    "        dists_matrix = G_ig.shortest_paths(source=batch, target=illicit_nodes)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i}-{i+batch_size}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    for j, node in enumerate(batch):\n",
    "        dists = dists_matrix[j]\n",
    "        min_dist = min([d for d in dists if d < float('inf')], default=None)\n",
    "        results.append({'txId': node, 'shortest_path_to_illicit': min_dist})\n",
    "\n",
    "    # Save current batch\n",
    "    batch_df = pd.DataFrame(results)\n",
    "    batch_df.to_csv(f\"shortest_path_batch_{batch_num}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2mYrLBRKGeG"
   },
   "outputs": [],
   "source": [
    "files = glob.glob(\"shortest_path_batch_*.csv\")\n",
    "df_all = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPugf00sqQ3K",
    "outputId": "75f3c561-24c0-4263-870d-9f5bf1f0dbb0"
   },
   "outputs": [],
   "source": [
    "# Optional: check the result\n",
    "print(df_all.head())\n",
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wdF7OzArhDW"
   },
   "outputs": [],
   "source": [
    "# Convert both to string before merging add this before the merge:\n",
    "df_scores['txId'] = df_scores['txId'].astype(str)\n",
    "df_all['txId'] = df_all['txId'].astype(str)\n",
    "\n",
    "# Then safely merge:\n",
    "df_scores = df_scores.merge(df_all, on='txId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RkRlXX59JbW"
   },
   "outputs": [],
   "source": [
    "def _cycle_pos(cycle, scale=(1.0, 1.0), rotation=0.0):\n",
    "    \"\"\"Positions nodes around an ellipse with optional rotation (radians).\"\"\"\n",
    "    L = len(cycle)\n",
    "    angles = np.linspace(0, 2*np.pi, L, endpoint=False) + rotation\n",
    "    ax, ay = scale\n",
    "    return {node: (ax*np.cos(a), ay*np.sin(a)) for node, a in zip(cycle, angles)}\n",
    "\n",
    "def _node_color(node, class_map):\n",
    "    cls = class_map.get(node, \"unknown\")\n",
    "    return \"red\" if cls==\"illicit\" else (\"green\" if cls==\"licit\" else \"gray\")\n",
    "\n",
    "def plot_one_cycle(cycle_nodes, class_map, ax, title=\"\", scale=(1.0,1.0), rotation=0.0, node_size=60):\n",
    "    cyc = list(cycle_nodes)\n",
    "    H = nx.Graph()\n",
    "    H.add_nodes_from(cyc)\n",
    "    H.add_edges_from(zip(cyc, cyc[1:] + [cyc[0]]))  # ring\n",
    "    pos = _cycle_pos(cyc, scale=scale, rotation=rotation)\n",
    "    colors = [_node_color(n, class_map) for n in cyc]\n",
    "    nx.draw(H, pos, ax=ax, with_labels=False, node_color=colors, edge_color=\"#999999\", node_size=node_size)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "def plot_top_k_cycles_panel(sus_cycles, class_map, k=4):\n",
    "    rows = 2 if k > 2 else 1\n",
    "    cols = 2 if k > 1 else 1\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 10) if rows==2 else (10,5))\n",
    "    axes = np.array(axes).reshape(-1)  # flatten\n",
    "\n",
    "    # choose a few shapes/rotations to “spread out” the look\n",
    "    shapes = [((1.3,0.8), 0.3), ((1.3,0.8), 0.3), ((1.3,0.8), 0.3), ((1.3,0.8), 0.3)]\n",
    "\n",
    "    for i, info in enumerate(sus_cycles[:k]):\n",
    "        scale, rot = shapes[i % len(shapes)]\n",
    "        title = f\"Cycle #{i+1} (len={info['len']}, ill={info['illicit']}, r={info['ratio']:.2f})\"\n",
    "        plot_one_cycle(info[\"nodes\"], class_map, axes[i], title=title, scale=scale, rotation=rot, node_size=50)\n",
    "\n",
    "    # hide any unused axes\n",
    "    for j in range(len(sus_cycles[:k]), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "id": "uHZx8J119LTK",
    "outputId": "8192046b-53dc-4d89-8b05-a00bc7ef4b20"
   },
   "outputs": [],
   "source": [
    "# Four most suspicious cycles as a 2x2 panel\n",
    "plot_top_k_cycles_panel(sus_cycles, class_map, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TW3LB6tUA1wb"
   },
   "source": [
    "Top Suspicious Cycles (uniform ellipse layout).\n",
    "Each ring shows a single simple cycle from the Paton (DFS) cycle basis. We rank cycles by an illicit-presence score and plot the top four. Notice the long contiguous red segments and high illicit ratios (0.42–0.72), consistent with coordinated circular flows—classic laundering patterns. Unknown nodes embedded in these rings are strong follow-ups for investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vuIUm0z2poL"
   },
   "source": [
    "## Role Embeddings\n",
    "\n",
    "We used role-based embeddings to capture the structural function of each node in the transaction graph. Unlike neighborhood-based embeddings (e.g., Node2Vec), which encode proximity, role embeddings map nodes with similar structural patterns (e.g., hubs, bridges, periphery) into similar vector representation. See Grover and Leskovec, *node2vec: Scalable Feature Learning for Networks* [20]\n",
    "\n",
    "In fraud detection, this allows us to identify transactions that “behave” like known illicit hubs, even if they are far apart in the network. Role embeddings provide a generalized, position-aware feature space that complements community and centrality measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "caacad1eff3043a1b9dae41b668befb8",
      "1583fcec3b7747c8abb09eb641eb2d5a",
      "9fa7d42897db4157b0045dcc515f9ea5",
      "b7ab63a4e05f4245a1943cb10e604eac",
      "80dd7b080f6c47a8b3121364650fea4f",
      "cd83654a080741969a019b2fd273c49d",
      "08aa22a522394870bc578b15810a5135",
      "06751bf1c836433183ed1bd6436acf68",
      "9f09844a294f469fb937f8b652e8965a",
      "903f9da753f041449d5236db15adac79",
      "1bc844eaa24f4d2c902f3084ed9daf18"
     ]
    },
    "id": "jqNSxH-e2ukt",
    "outputId": "432cf72b-9584-410d-f796-fd2a1ee5b2fa"
   },
   "outputs": [],
   "source": [
    "\n",
    "node2vec = Node2Vec(G_undirected, dimensions=32, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Convert to DataFrame\n",
    "role_df = pd.DataFrame([model.wv[str(n)] for n in G_undirected.nodes()], columns=[f'role_{i}' for i in range(32)])\n",
    "role_df['txId'] = list(G_undirected.nodes())\n",
    "df_scores = df_scores.merge(role_df, on='txId', how='left')\n",
    "\n",
    "# 7 hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "yN-xj4MZGRqJ",
    "outputId": "c8aa0df5-e74f-4d37-92c5-d294de2eef6a"
   },
   "outputs": [],
   "source": [
    "# Reload your features\n",
    "df_scores = pd.read_parquet('/content/drive/MyDrive/df_scores_v1.parquet')\n",
    "\n",
    "# Extract role embedding columns\n",
    "role_cols = [c for c in df_scores.columns if c.startswith(\"role_\")]\n",
    "emb = df_scores[['txId'] + role_cols].copy()\n",
    "\n",
    "# Add class labels\n",
    "emb['class'] = df_scores['label']\n",
    "\n",
    "# Sample if huge\n",
    "MAXN = 20000\n",
    "if len(emb) > MAXN:\n",
    "    emb = emb.sample(MAXN, random_state=42)\n",
    "\n",
    "X = emb[role_cols].to_numpy()\n",
    "\n",
    "# PCA projection\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "Z = pca.fit_transform(X)\n",
    "emb['z1'], emb['z2'] = Z[:,0], Z[:,1]\n",
    "\n",
    "# Plot\n",
    "color_map = {'illicit':'red','licit':'green','unknown':'gray'}\n",
    "colors = emb['class'].map(color_map)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(emb['z1'], emb['z2'], c=colors, s=8, alpha=0.7, linewidths=0)\n",
    "plt.title('Role Embeddings (PCA 2D)')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)')\n",
    "for k,v in color_map.items():\n",
    "    plt.scatter([],[],c=v,s=30,label=k)\n",
    "plt.legend(frameon=False, markerscale=1.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2DDnop7Hek7"
   },
   "outputs": [],
   "source": [
    "# Rebalance: keep all illicit, sample licit/unknown to match\n",
    "emb2 = emb.copy()\n",
    "n_illicit = (emb2['class']=='illicit').sum()\n",
    "sampled = [\n",
    "    emb2[emb2['class']=='illicit'],\n",
    "    emb2[emb2['class']=='licit'  ].sample(min(n_illicit, (emb2['class']=='licit').sum()),  random_state=42),\n",
    "    emb2[emb2['class']=='unknown'].sample(min(n_illicit, (emb2['class']=='unknown').sum()),random_state=42),\n",
    "]\n",
    "emb_bal = pd.concat(sampled).sample(frac=1, random_state=42)  # shuffle\n",
    "\n",
    "Xb = emb_bal[role_cols].to_numpy().astype(np.float32)\n",
    "Xb = StandardScaler().fit_transform(Xb)  # z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "VC_-BW7gHimF",
    "outputId": "118e22bd-64b8-4ba3-fa1f-1939022420ba"
   },
   "outputs": [],
   "source": [
    "# Try UMAP; fallback to t-SNE if not installed\n",
    "use_umap = False\n",
    "try:\n",
    "    import umap\n",
    "    use_umap = True\n",
    "except Exception:\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "colors = emb_bal['class'].map({'illicit':'red','licit':'green','unknown':'gray'})\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "Zp = pca.fit_transform(Xb)\n",
    "\n",
    "# UMAP / t-SNE\n",
    "if use_umap:\n",
    "    reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.1, random_state=42)\n",
    "    Zu = reducer.fit_transform(Xb)\n",
    "    right_title = \"UMAP 2D\"\n",
    "else:\n",
    "    Zu = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=42).fit_transform(Xb)\n",
    "    right_title = \"t-SNE 2D\"\n",
    "\n",
    "# Plot panel\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "axes[0].scatter(Zp[:,0], Zp[:,1], c=colors, s=8, alpha=0.7, linewidths=0)\n",
    "axes[0].set_title(f\"PCA 2D (PC1 {pca.explained_variance_ratio_[0]*100:.1f}%, PC2 {pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "axes[0].set_xlabel(\"PC1\"); axes[0].set_ylabel(\"PC2\")\n",
    "\n",
    "axes[1].scatter(Zu[:,0], Zu[:,1], c=colors, s=8, alpha=0.7, linewidths=0)\n",
    "axes[1].set_title(f\"Node2Vec Embedding ({right_title})\")\n",
    "axes[1].set_xlabel(\"dim1\"); axes[1].set_ylabel(\"dim2\")\n",
    "\n",
    "# tiny legend (once)\n",
    "for k,v in {'illicit':'red','licit':'green','unknown':'gray'}.items():\n",
    "    axes[0].scatter([],[],c=v,s=30,label=k)\n",
    "axes[0].legend(frameon=False, markerscale=1.5, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unsU9i4hJXa5"
   },
   "source": [
    "Left plot (PCA 2D)\n",
    "\n",
    "PCA is a linear projection: it just rotates the 32-dim embedding space into the top variance directions. It’s useful for variance explanation (e.g. “PC1 explains 4.5%”). But because `Node2Vec` embeddings are spread across many dimensions, PCA usually looks like a dense cloud with no clear clusters.\n",
    "\n",
    "Right plot (UMAP 2D)\n",
    "\n",
    "UMAP is a nonlinear manifold learner: it tries to preserve local neighborhoods. That’s why distinct **islands** forming.\n",
    "\n",
    "In fraud context: those islands often correspond to structurally similar transaction patterns. The fact that many red (illicit) points concentrate together means UMAP is pulling out behavioral clusters that PCA can’t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xse-vvIkXPYR"
   },
   "source": [
    "## Spectral Partitioning\n",
    "\n",
    "Spectral partitioning divides a graph using eigenvectors of the graph Laplacian. This method minimizes the “cut” between groups while balancing their sizes, providing a mathematically elegant way of revealing network structure, and see Newman, *Spectral methods for community detection and graph partitioning* [21].\n",
    "\n",
    "For fraud detection, spectral methods highlight global partitions in the transaction graph. Since illicit activity often forms dense, tightly connected subgraphs, spectral clustering can help separate suspicious subnetworks from the broader flow of licit transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Y9_HJwNeT7E"
   },
   "outputs": [],
   "source": [
    "# 1. Create node index mapping for reproducibility\n",
    "nodes = list(G_undirected.nodes())\n",
    "node_index = {n: i for i, n in enumerate(nodes)}\n",
    "\n",
    "# 2. Build normalized Laplacian matrix\n",
    "L = nx.normalized_laplacian_matrix(G_undirected, nodelist=nodes)\n",
    "\n",
    "# 3. Compute the first k eigenvectors (skip the trivial all-ones vector at index 0)\n",
    "k = 5  # you can tune this; 5 gives you 5 global features\n",
    "vals, vecs = eigsh(L, k=k, which='SM')  # smallest magnitude eigenvalues\n",
    "\n",
    "# 4. Drop the first eigenvector (trivial), keep others\n",
    "spectral_features = vecs[:, 1:]  # shape: (num_nodes, k-1)\n",
    "\n",
    "# 5. Add to df_scores\n",
    "spec_df = pd.DataFrame(\n",
    "    spectral_features,\n",
    "    index=nodes,\n",
    "    columns=[f'spectral_ev{i}' for i in range(1, spectral_features.shape[1] + 1)]\n",
    ").reset_index().rename(columns={'index': 'txId'})\n",
    "\n",
    "df_scores = df_scores.merge(spec_df, on='txId', how='left')\n",
    "\n",
    "# 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "DbNp94ttDqWs",
    "outputId": "61902064-d50f-4a3f-afd7-3fcd41522faf"
   },
   "outputs": [],
   "source": [
    "# Filter just the columns we need\n",
    "plot_df = df_scores[['spectral_ev1', 'spectral_ev2', 'label']].copy()\n",
    "\n",
    "# Map labels to colors\n",
    "color_map = {'illicit': '#d62728', 'licit': '#2ca02c', 'unknown': '#888888'}\n",
    "colors = plot_df['label'].map(color_map)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(\n",
    "    plot_df['spectral_ev1'],\n",
    "    plot_df['spectral_ev2'],\n",
    "    c=colors,\n",
    "    alpha=0.6,\n",
    "    s=10,\n",
    "    edgecolors='none'\n",
    ")\n",
    "\n",
    "# Legend\n",
    "for lbl, col in color_map.items():\n",
    "    plt.scatter([], [], c=col, label=lbl, s=30)\n",
    "plt.legend(title='Label', loc='upper right')\n",
    "\n",
    "plt.xlabel('Spectral EV1')\n",
    "plt.ylabel('Spectral EV2')\n",
    "plt.title('Spectral Embedding of Elliptic Graph (EV1 vs EV2)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "Poll1SggrMe-",
    "outputId": "8ee737c5-a5dc-437c-ea1b-bda660c2f8c8"
   },
   "outputs": [],
   "source": [
    "# Keep only needed cols and drop NaNs (just in case)\n",
    "plot3 = df_scores[['spectral_ev1', 'spectral_ev2', 'spectral_ev3', 'label']].dropna()\n",
    "\n",
    "# Map labels to colors\n",
    "cmap = {'illicit': '#d62728', 'licit': '#2ca02c', 'unknown': '#888888'}\n",
    "colors = plot3['label'].map(cmap)\n",
    "\n",
    "# subsample for speed if it's too dense\n",
    "max_points = 100_000\n",
    "if len(plot3) > max_points:\n",
    "    plot3 = plot3.sample(max_points, random_state=42)\n",
    "    colors = plot3['label'].map(cmap)\n",
    "\n",
    "fig = plt.figure(figsize=(8,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(\n",
    "    plot3['spectral_ev1'], plot3['spectral_ev2'], plot3['spectral_ev3'],\n",
    "    c=colors, s=6, depthshade=False\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Spectral EV1'); ax.set_ylabel('Spectral EV2'); ax.set_zlabel('Spectral EV3')\n",
    "ax.set_title('Elliptic Spectral Embedding (EV1, EV2, EV3)')\n",
    "\n",
    "# Legends via proxy artists\n",
    "for lbl, col in cmap.items():\n",
    "    ax.scatter([], [], [], c=col, s=20, label=lbl)\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY7b8E6QDrGw"
   },
   "source": [
    "# Act 2 Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TorFC_FJ08HG"
   },
   "source": [
    "## Simplicial Complexes\n",
    "\n",
    "We extend graphs into higher-dimensional objects using simplicial complexes, where nodes, edges, triangles, and higher-order cliques form building blocks for topology. A clique complex (or flag complex) is a simplicial complex constructed from all complete subgraphs (cliques) in the network. See Ghrist, *Barcodes: The persistent topology of data* [22].\n",
    "\n",
    "In fraud detection, higher-order connectivity can reveal patterns missed by pairwise edges. For example, tightly interlinked groups of transactions forming large cliques may correspond to collusive fraud rings. By lifting graphs into clique complexes, we capture not only who connects to whom, but also the multi-way entanglements of suspicious entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Shd1oF_A7wAy",
    "outputId": "aeb87fad-7ce5-4d29-ccfc-3e5b1691df84"
   },
   "outputs": [],
   "source": [
    "# config\n",
    "# K = 6\n",
    "K = 6\n",
    "\n",
    "# bail-out which a guardrail\n",
    "FACE_CAP = 5_000_000\n",
    "OUT_PREFIX = f\"tda_filtr_flagK{K}_epsdim\"\n",
    "\n",
    "# save finite H1 lifetimes table per component\n",
    "SAVE_DIAGS = True\n",
    "\n",
    "# None = all\n",
    "TOP_N_COMPONENTS = None\n",
    "\n",
    "# Filtration source: node time (lower = earlier)\n",
    "node_time = {str(tid): int(ts) for tid, ts in df_features[['txId', 'time_step']].itertuples(index=False)}\n",
    "\n",
    "def vtime(u):\n",
    "    \"\"\"Return node's time_step or a very large value if missing.\"\"\"\n",
    "    return float(node_time.get(str(u), 10**9))\n",
    "\n",
    "def face_filtration(face):\n",
    "    \"\"\"\n",
    "    Upper-star style:\n",
    "      filtration(face) = max vertex time + epsilon * dimension\n",
    "    This ensures vertices < edges < triangles < tets appear strictly later,\n",
    "    avoiding 'birth=death' collapses at a single time.\n",
    "    \"\"\"\n",
    "    t = max(vtime(v) for v in face)\n",
    "    dim = len(face) - 1\n",
    "    return t + dim * 1e-6\n",
    "\n",
    "def insert_flag_with_filtration(H, K, face_cap=FACE_CAP):\n",
    "    \"\"\"Insert vertices and all faces from cliques up to size K with filtration.\"\"\"\n",
    "    st = gudhi.SimplexTree()\n",
    "    inserted = 0\n",
    "\n",
    "    # vertices\n",
    "    for u in H.nodes():\n",
    "        st.insert([int(u)], filtration=face_filtration([u]))\n",
    "        inserted += 1\n",
    "        if inserted > face_cap:\n",
    "            return st, inserted\n",
    "\n",
    "    # cliques up to size K (global cutoff)\n",
    "    for clique in nx.enumerate_all_cliques(H):\n",
    "        if len(clique) > K:\n",
    "            break\n",
    "        if len(clique) >= 2:\n",
    "            for r in range(2, len(clique) + 1):\n",
    "                for face in combinations(clique, r):\n",
    "                    st.insert(list(map(int, face)), filtration=face_filtration(face))\n",
    "                    inserted += 1\n",
    "                    if inserted > face_cap:\n",
    "                        return st, inserted\n",
    "    return st, inserted\n",
    "\n",
    "def summarize_h1(diag):\n",
    "    \"\"\"Return stats for finite H1, plus counts of zero-length and essential (infinite) classes.\"\"\"\n",
    "    finite = []\n",
    "    zero_len = 0\n",
    "    essential = 0\n",
    "    for dim, (b, d) in diag:\n",
    "        if dim != 1:\n",
    "            continue\n",
    "        if np.isinf(d):\n",
    "            essential += 1\n",
    "        else:\n",
    "            if d == b:\n",
    "                zero_len += 1\n",
    "            else:\n",
    "                finite.append(d - b)\n",
    "    if not finite:\n",
    "        return {\n",
    "            \"h1_finite_count\": 0,\n",
    "            \"h1_total_pers\": 0.0,\n",
    "            \"h1_max_pers\": 0.0,\n",
    "            \"h1_mean_pers\": 0.0,\n",
    "            \"h1_median_pers\": 0.0,\n",
    "            \"h1_zero_len\": zero_len,\n",
    "            \"h1_essential\": essential\n",
    "        }, pd.DataFrame(columns=[\"birth\",\"death\",\"lifetime\"])\n",
    "\n",
    "    L = np.array(finite, float)\n",
    "    return {\n",
    "        \"h1_finite_count\": int(len(L)),\n",
    "        \"h1_total_pers\": float(L.sum()),\n",
    "        \"h1_max_pers\": float(L.max()),\n",
    "        \"h1_mean_pers\": float(L.mean()),\n",
    "        \"h1_median_pers\": float(np.median(L)),\n",
    "        \"h1_zero_len\": zero_len,\n",
    "        \"h1_essential\": essential\n",
    "    }, pd.DataFrame({\"birth\": [], \"death\": [], \"lifetime\": []})  # keep small; write detailed diags if you want\n",
    "\n",
    "# resume detection\n",
    "done = {\n",
    "    int(f.split(\"_\")[-1].split(\".\")[0])\n",
    "    for f in os.listdir()\n",
    "    if f.startswith(OUT_PREFIX + \"_summary_\") and f.endswith(\".csv\")\n",
    "}\n",
    "\n",
    "# choose components and largest sorted\n",
    "components = sorted(nx.connected_components(G_undirected), key=len, reverse=True)\n",
    "if TOP_N_COMPONENTS is not None:\n",
    "    components = components[:TOP_N_COMPONENTS]\n",
    "\n",
    "print(f\"[INFO] Nodes={G_undirected.number_of_nodes():,}, Edges={G_undirected.number_of_edges():,}\")\n",
    "print(f\"[INFO] Components to process: {len(components)} (resume skips {len(done)})\")\n",
    "\n",
    "start = time.time()\n",
    "processed = 0\n",
    "\n",
    "for cid, nodes in enumerate(components):\n",
    "    if cid in done:\n",
    "        print(f\"[SKIP] comp {cid} (resume)\")\n",
    "        continue\n",
    "\n",
    "    H = G_undirected.subgraph(nodes).copy()\n",
    "    n, m = H.number_of_nodes(), H.number_of_edges()\n",
    "    print(f\"[PROC] comp {cid}: n={n:,}, m={m:,}\")\n",
    "\n",
    "    st, faces = insert_flag_with_filtration(H, K, FACE_CAP)\n",
    "    print(f\"       inserted faces={faces:,}\")\n",
    "\n",
    "    st.compute_persistence(homology_coeff_field=2)\n",
    "    bettis = st.betti_numbers()\n",
    "    beta0 = bettis[0] if len(bettis) > 0 else 0\n",
    "    beta1 = bettis[1] if len(bettis) > 1 else 0\n",
    "\n",
    "    diag = st.persistence()\n",
    "    h1_stats, h1_df = summarize_h1(diag)\n",
    "    rss = psutil.Process().memory_info().rss / 1e9\n",
    "\n",
    "    summary_path = f\"{OUT_PREFIX}_summary_{cid}.csv\"\n",
    "    pd.DataFrame([{\n",
    "        \"component_id\": cid,\n",
    "        \"n\": n, \"m\": m,\n",
    "        \"betti_0\": beta0, \"betti_1\": beta1,\n",
    "        **h1_stats,\n",
    "        \"faces\": faces, \"rss_gb\": rss\n",
    "    }]).to_csv(summary_path, index=False)\n",
    "    print(f\"       wrote {summary_path} | β0={beta0}, β1={beta1}, \"\n",
    "          f\"finiteH1={h1_stats['h1_finite_count']}, zero={h1_stats['h1_zero_len']}, inf={h1_stats['h1_essential']}, RSS={rss:.2f} GB\")\n",
    "\n",
    "    if SAVE_DIAGS and not h1_df.empty:\n",
    "        h1_df.to_csv(f\"{OUT_PREFIX}_h1_{cid}.csv\", index=False)\n",
    "\n",
    "    processed += 1\n",
    "    del st, H, diag, h1_df\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"[DONE] processed {processed} components in {time.time()-start:.1f}s\")\n",
    "\n",
    "# Merge summaries\n",
    "all_summ = sorted(glob.glob(f\"{OUT_PREFIX}_summary_*.csv\"))\n",
    "if all_summ:\n",
    "    merged = pd.concat((pd.read_csv(p) for p in all_summ), ignore_index=True)\n",
    "    merged.to_csv(f\"{OUT_PREFIX}_summary_all.csv\", index=False)\n",
    "    print(f\"[INFO] merged -> {OUT_PREFIX}_summary_all.csv, shape={merged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-6HMf8n-ubc",
    "outputId": "e714ddc2-f752-457f-e9a6-94e1aedc9e38"
   },
   "outputs": [],
   "source": [
    "# merge component summaries\n",
    "summ_files = sorted(glob.glob(\"tda_filtr_flagK6_epsdim_summary_*.csv\"))\n",
    "tda_comp = pd.concat((pd.read_csv(p) for p in summ_files), ignore_index=True)\n",
    "\n",
    "# map node -> component_id (same order used by your run: largest-first)\n",
    "comp_map = {}\n",
    "for cid, nodes in enumerate(sorted(nx.connected_components(G_undirected), key=len, reverse=True)):\n",
    "    for n in nodes:\n",
    "        comp_map[str(n)] = cid\n",
    "\n",
    "comp_df = pd.DataFrame({\"txId\": list(comp_map.keys()),\n",
    "                        \"component_id\": list(comp_map.values())})\n",
    "\n",
    "# 1) Make sure both sides use strings for txId\n",
    "df_scores[\"txId\"] = df_scores[\"txId\"].astype(str)\n",
    "comp_df[\"txId\"]  = comp_df[\"txId\"].astype(str)\n",
    "\n",
    "# 2) (Optional) sanity check\n",
    "print(\"dtypes:\", df_scores[\"txId\"].dtype, comp_df[\"txId\"].dtype)\n",
    "\n",
    "# 3) Merge (node_comp is one row per txId, so many_to_one is correct)\n",
    "n_before = len(df_scores)\n",
    "df_scores = df_scores.merge(comp_df, on=\"txId\", how=\"left\", validate=\"many_to_one\")\n",
    "assert len(df_scores) == n_before\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQqS8pZ897si"
   },
   "source": [
    "## Persistent Homology\n",
    "\n",
    "Persistent homology tracks topological features (connected components, loops, voids) across multiple scales, forming a “barcode” of network structure Ghrist [22] and Edelsbrunner, Harer [14]. By observing which features persist as we vary thresholds (e.g., transaction amounts or connectivity), we separate noise from meaningful patterns.\n",
    "\n",
    "In fraud detection, persistence can reveal hidden cycles or higher-order structures in transaction flows that remain stable across scales, offering robust indicators of illicit behavior even under noisy or incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPA80MyeEtkk",
    "outputId": "49431f4e-d201-4aff-9164-8fb849e93829"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select role embedding columns\n",
    "role_cols = [col for col in df_scores.columns if col.startswith(\"role_\")]\n",
    "data = df_scores[role_cols].values\n",
    "\n",
    "# Apply PCA to reduce dimensionality before TDA\n",
    "pca = PCA(n_components=10, random_state=42)\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "# Batch settings\n",
    "batch_size = 500  # Adjust based on memory\n",
    "num_batches = int(np.ceil(data_pca.shape[0] / batch_size))\n",
    "results = []\n",
    "\n",
    "# Directory to save results\n",
    "os.makedirs(\"/mnt/data/tda_batches\", exist_ok=True)\n",
    "\n",
    "# Run persistent homology in batches\n",
    "for i in tqdm(range(num_batches)):\n",
    "    batch_data = data_pca[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "    try:\n",
    "        rips = gudhi.RipsComplex(points=batch_data)\n",
    "        simplex_tree = rips.create_simplex_tree(max_dimension=2)\n",
    "        diag = simplex_tree.persistence()\n",
    "\n",
    "        # Save the diagram to file\n",
    "        with open(f\"/mnt/data/tda_batches/persistence_batch_{i}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(diag, f)\n",
    "\n",
    "    except Exception as e:\n",
    "        results.append((i, f\"Failed: {str(e)}\"))\n",
    "    else:\n",
    "        results.append((i, \"Success\"))\n",
    "\n",
    "# 2 hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99jG7JCkWswI"
   },
   "source": [
    "$H_1$ measures loops. In transaction graphs, persistent $H_1$ features correspond to ring-laundering patterns—value that circulates through multiple addresses and returns—whereas $H_0$ only reflects fragmentation and $H_2$ is rarely present. Ranking by persistence suppresses noisy, low-weight loops and highlights coordinated activity. $H_0$ and $H_1$ is all we need to conduct this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ug98nBVyuRjQ"
   },
   "outputs": [],
   "source": [
    "# Define feature extraction function\n",
    "def summarize_diagram(diag):\n",
    "    h0_lifetimes = [death - birth for dim, (birth, death) in diag if dim == 0 and np.isfinite(death)]\n",
    "    h1_lifetimes = [death - birth for dim, (birth, death) in diag if dim == 1 and np.isfinite(death)]\n",
    "\n",
    "    return {\n",
    "        \"tda_h0_count\": len(h0_lifetimes),\n",
    "        \"tda_h0_max\": max(h0_lifetimes) if h0_lifetimes else 0,\n",
    "        \"tda_h0_mean\": np.mean(h0_lifetimes) if h0_lifetimes else 0,\n",
    "        \"tda_h1_count\": len(h1_lifetimes),\n",
    "        \"tda_h1_max\": max(h1_lifetimes) if h1_lifetimes else 0,\n",
    "        \"tda_h1_mean\": np.mean(h1_lifetimes) if h1_lifetimes else 0,\n",
    "    }\n",
    "\n",
    "# Loop through your saved .pkl files and collect features\n",
    "tda_features = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    with open(f\"/mnt/data/tda_batches/persistence_batch_{i}.pkl\", \"rb\") as f:\n",
    "        diag = pickle.load(f)\n",
    "\n",
    "    summary = summarize_diagram(diag)\n",
    "    summary[\"batch_id\"] = i\n",
    "    tda_features.append(summary)\n",
    "\n",
    "df_tda = pd.DataFrame(tda_features)\n",
    "\n",
    "# Expand the df_scores to include those batches\n",
    "tda_rows = []\n",
    "\n",
    "for i, row in df_tda.iterrows():\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(df_scores))\n",
    "\n",
    "    for _ in range(start_idx, end_idx):\n",
    "        tda_rows.append(row.drop(\"batch_id\").to_dict())\n",
    "\n",
    "df_tda_expanded = pd.DataFrame(tda_rows)\n",
    "\n",
    "# Now merge\n",
    "df_scores = pd.concat([df_scores.reset_index(drop=True), df_tda_expanded.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIkjJYSM44st"
   },
   "source": [
    "plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOXrKXUs6dWj"
   },
   "outputs": [],
   "source": [
    "def compute_ph_and_plot(st, title_prefix=\"Persistent Homology\"):\n",
    "    # Full persistence\n",
    "    pers = st.persistence(homology_coeff_field=2, persistence_dim_max=True)\n",
    "\n",
    "    # Split by dimension\n",
    "    H0 = [(b, d) for dim, (b, d) in pers if dim == 0]\n",
    "    H1 = [(b, d) for dim, (b, d) in pers if dim == 1]\n",
    "\n",
    "    def lifetimes(pairs):\n",
    "        L, zero_len, essential = [], 0, 0\n",
    "        for b, d in pairs:\n",
    "            if d == float('inf'):\n",
    "                essential += 1\n",
    "            else:\n",
    "                life = d - b\n",
    "                if life <= 0: zero_len += 1\n",
    "                L.append(max(0.0, life))\n",
    "        return np.asarray(L, float), zero_len, essential\n",
    "\n",
    "    L1, z1, e1 = lifetimes(H1)\n",
    "\n",
    "    # --- Barcode plots (H0/H1)\n",
    "    def barcode(ax, pairs, title):\n",
    "        if not pairs:\n",
    "            ax.set_title(title + \" (empty)\")\n",
    "            ax.axis('off')\n",
    "            return\n",
    "        y = 0\n",
    "        for (b, d) in pairs:\n",
    "            d = (10 if d == float('inf') else d)  # cap infinities for display\n",
    "            ax.hlines(y, b, d, linewidth=2)\n",
    "            y += 1\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"filtration\")\n",
    "        ax.set_ylabel(\"bars\")\n",
    "        ax.grid(alpha=0.2)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax1 = fig.add_subplot(1,2,1); barcode(ax1, H0, f\"{title_prefix}: H0 barcode\")\n",
    "    ax2 = fig.add_subplot(1,2,2); barcode(ax2, H1, f\"{title_prefix}: H1 barcode\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # --- H1 persistence diagram (finite only)\n",
    "    finite = np.array([[b, d] for (b, d) in H1 if d != float('inf')], float)\n",
    "    if finite.size > 0:\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.scatter(finite[:,0], finite[:,1], s=16)\n",
    "        maxv = max(finite.max(), 1.0)\n",
    "        plt.plot([0, maxv], [0, maxv], ls='--', lw=1)\n",
    "        plt.xlabel(\"birth\"); plt.ylabel(\"death\")\n",
    "        plt.title(f\"{title_prefix}: H1 persistence diagram\")\n",
    "        plt.grid(alpha=0.2); plt.show()\n",
    "    else:\n",
    "        print(\"Note: no finite H1 bars (all loops are essential or H1 is empty). Skipping H1 PD.\")\n",
    "\n",
    "    stats = dict(\n",
    "        h1_finite_count=int(len(L1)),\n",
    "        h1_total_pers=float(L1.sum()) if len(L1) else 0.0,\n",
    "        h1_max_pers=float(L1.max()) if len(L1) else 0.0,\n",
    "        h1_mean_pers=float(L1.mean()) if len(L1) else 0.0,\n",
    "        h1_median_pers=float(np.median(L1)) if len(L1) else 0.0,\n",
    "        h1_zero_len=int(z1),\n",
    "        h1_essential=int(e1),\n",
    "    )\n",
    "    return {\"H0_pairs\": H0, \"H1_pairs\": H1, \"H1_stats\": stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "Wbyce4i36xSl",
    "outputId": "88619ef3-57b3-4569-f1c8-239b11c4c5d5"
   },
   "outputs": [],
   "source": [
    "# Build ST from a manageable subgraph H (undirected)\n",
    "st = build_flag_simplextree_up_to_2(H)\n",
    "ph = compute_ph_and_plot(st, title_prefix=\"Community PH\")\n",
    "print(ph[\"H1_stats\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB79ANva7wSi"
   },
   "source": [
    "We applied persistent homology to the clique (flag) complex built from each transaction subgraph. The barcode below summarizes the evolution of topological features across the filtration.\n",
    "\n",
    "$H_0$ (connected components): Each short bar represents a connected component at the beginning; these rapidly merge, leaving a **single dominant component** that persists.\n",
    "\n",
    "$H_1$ (loops): We observe one long bar, corresponding to a loop that **never gets filled** in by higher-order simplices. In the fraud detection setting, such stable loops may correspond to ring-like transaction structures that are robust across scales.\n",
    "\n",
    "Persistence diagrams and barcodes provide a complementary view to Betti numbers: instead of just a snapshot count, they reveal which topological patterns endure and which **vanish quickly** (noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHUSIEOr9mC-"
   },
   "source": [
    "plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwwj0TK99yqp"
   },
   "outputs": [],
   "source": [
    "def plot_persistence_diagram(st, title_prefix=\"Persistence Diagram\"):\n",
    "    pers = st.persistence(homology_coeff_field=2, persistence_dim_max=True)\n",
    "\n",
    "    # Extract H1 finite pairs\n",
    "    H1 = [(b, d) for dim, (b, d) in pers if dim == 1 and d != float(\"inf\")]\n",
    "\n",
    "    if not H1:\n",
    "        print(\"No finite H1 pairs — only essential loops or none at all.\")\n",
    "        return\n",
    "\n",
    "    B = np.array(H1, float)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(B[:,0], B[:,1], s=20, c=\"blue\", alpha=0.7, label=\"H1 features\")\n",
    "\n",
    "    # diagonal reference line\n",
    "    maxv = max(B.max(), 1.0)\n",
    "    plt.plot([0, maxv], [0, maxv], ls=\"--\", lw=1, color=\"black\")\n",
    "\n",
    "    plt.xlabel(\"Birth\")\n",
    "    plt.ylabel(\"Death\")\n",
    "    plt.title(f\"{title_prefix}: H1 Persistence Diagram\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBo9Srlq9yfX",
    "outputId": "fba3ba22-6e65-422f-b37e-59fe69d97191"
   },
   "outputs": [],
   "source": [
    "st = build_flag_simplextree_up_to_2(H)\n",
    "plot_persistence_diagram(st, title_prefix=\"Community PH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cnfVGaY-WMe"
   },
   "source": [
    "This subgraph contains a loop that persists all the way through — it is never filled by higher-order simplices in the flag complex. In fraud detection terms, that’s a stable ring structure, not just a temporary cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWKKqS_V-f3l"
   },
   "outputs": [],
   "source": [
    "def plot_persistence_diagram_with_infinity(st, cap=10, title_prefix=\"Persistence Diagram\"):\n",
    "    pers = st.persistence(homology_coeff_field=2, persistence_dim_max=True)\n",
    "    H1 = [(b, d) for dim, (b, d) in pers if dim == 1]\n",
    "\n",
    "    if not H1:\n",
    "        print(\"No H1 features at all.\")\n",
    "        return\n",
    "\n",
    "    B = []\n",
    "    for (b,d) in H1:\n",
    "        if d == float('inf'):\n",
    "            d = cap  # cap infinite bars for display\n",
    "        B.append((b,d))\n",
    "\n",
    "    B = np.array(B, float)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(B[:,0], B[:,1], s=20, c=\"blue\", alpha=0.7)\n",
    "    plt.plot([0, cap], [0, cap], ls=\"--\", lw=1, color=\"black\")\n",
    "    plt.xlabel(\"Birth\")\n",
    "    plt.ylabel(\"Death (∞ capped)\")\n",
    "    plt.title(f\"{title_prefix}: H1 Persistence Diagram (∞ capped at {cap})\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "-eDsHDd6-jIt",
    "outputId": "e0ead8c7-df3a-4dbe-e3c8-608a8c2b0b4b"
   },
   "outputs": [],
   "source": [
    "plot_persistence_diagram_with_infinity(st, cap=10, title_prefix=\"Community PH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koNPoPM0_sBi"
   },
   "source": [
    "In the persistence diagram, the single blue point corresponds to a 1-dimensional loop that appears early in the filtration and persists indefinitely $(\\text{death} =  \\infty$, here capped at 10 for visualization). The dashed diagonal marks $\\text{birth} = \\text{death}$; features near the diagonal are noise, while features far above are robust. This result indicates that the community contains a stable cycle structure that is never filled in by higher-order simplices, consistent with a ring-like pattern of transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HT5Jmso1QBq"
   },
   "source": [
    "## Clique Complex (flag complex) from graph\n",
    "\n",
    "Simplicial & Clique (Flag) Complexes. We lift the transaction graph into a simplicial complex to capture higher-order structure (edges, triangles, and higher cliques). In particular, we use the clique (flag) complex, which adds a $k$-simplex for every $(k+1)$-clique in the graph; this is the maximal simplicial complex whose 1-skeleton equals the original network. We analyze two complementary signals. First, Betti numbers $(\\beta_0, \\beta_1)$ summarize the topology at a snapshot (connected components and independent loops). Second, we compute persistent homology on $H_1$ under a chosen filtration (thresholding edges/weights over scale), and summarize the barcode by the number of finite bars, total and maximum persistence, and related statistics. Betti highlights instantaneous topology; persistence highlights stability of loops across scales—useful for distinguishing robust ring-like fraud patterns from transient noise.\n",
    "\n",
    "See Edelsbrunner and Harer [14]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJvK9OhTz9c7"
   },
   "outputs": [],
   "source": [
    "# Ensure nodes are integers\n",
    "G_int = nx.convert_node_labels_to_integers(G_undirected)\n",
    "cliques = [list(c) for c in nx.find_cliques(G_int)]\n",
    "\n",
    "# Build the simplex tree from clique complex\n",
    "simplex_tree = gudhi.SimplexTree()\n",
    "\n",
    "for clique in cliques:\n",
    "    for i in range(1, len(clique) + 1):\n",
    "        for subset in combinations(clique, i):\n",
    "            simplex_tree.insert(list(subset))  # insert expects a list of ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuHh5fzk1IPv"
   },
   "outputs": [],
   "source": [
    "# Make sure all IDs are strings for consistency\n",
    "df_edgelist = df_edgelist.astype(str)\n",
    "df_features['txId'] = df_features['txId'].astype(str)\n",
    "df_classes['txId'] = df_classes['txId'].astype(str)\n",
    "\n",
    "# Build the full undirected graph\n",
    "G_full = nx.from_pandas_edgelist(df_edgelist, source='source', target='target', create_using=nx.Graph())\n",
    "\n",
    "# Group by time step\n",
    "dfs_by_time = {t: df for t, df in df_features.groupby(\"time_step\")}\n",
    "time_steps = sorted(dfs_by_time.keys())\n",
    "\n",
    "# Output directory\n",
    "output_dir = \"/mnt/data/clique_tda_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through time steps and build clique complexes\n",
    "for t in time_steps:\n",
    "    df_t = dfs_by_time[t]\n",
    "    tx_ids = df_t['txId'].tolist()\n",
    "    subG = G_full.subgraph(tx_ids)\n",
    "\n",
    "    # Convert node labels to integers for GUDHI\n",
    "    mapping = {node: i for i, node in enumerate(subG.nodes())}\n",
    "    G_int = nx.relabel_nodes(subG, mapping)\n",
    "    reverse_mapping = {v: k for k, v in mapping.items()}\n",
    "\n",
    "    cliques = [list(c) for c in nx.find_cliques(G_int)]\n",
    "    simplex_tree = gudhi.SimplexTree()\n",
    "\n",
    "    for clique in cliques:\n",
    "        for i in range(1, len(clique) + 1):\n",
    "            for subset in combinations(clique, i):\n",
    "                simplex_tree.insert(list(subset))\n",
    "\n",
    "    diag = simplex_tree.persistence()\n",
    "    betti_nums = simplex_tree.betti_numbers()\n",
    "\n",
    "    h1_lifetimes = [d[1][1] - d[1][0] for d in diag if d[0] == 1 and d[1][1] < float('inf')]\n",
    "\n",
    "    summary = {\n",
    "        \"time_step\": t,\n",
    "        \"clique_betti_0\": betti_nums[0] if len(betti_nums) > 0 else 0,\n",
    "        \"clique_betti_1\": betti_nums[1] if len(betti_nums) > 1 else 0,\n",
    "        \"clique_total_persistence_h1\": sum(h1_lifetimes),\n",
    "        \"clique_max_h1_lifetime\": max(h1_lifetimes) if h1_lifetimes else 0,\n",
    "        \"clique_num_long_h1_bars\": sum(1 for l in h1_lifetimes if l > 1.0)\n",
    "    }\n",
    "\n",
    "    # Save this result to file\n",
    "    with open(os.path.join(output_dir, f\"clique_tda_timestep_{t}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(summary, f)\n",
    "\n",
    "    results.append(summary)\n",
    "\n",
    "df_clique_tda = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWL73HAY5Eeq"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Make sure txId and time_step columns are available\n",
    "df_mapping = df_features[['txId', 'time_step']].drop_duplicates()\n",
    "\n",
    "# Merge the TDA results into that mapping\n",
    "df_mapping = df_mapping.merge(df_clique_tda, on=\"time_step\", how=\"left\")\n",
    "\n",
    "def merge_with_tda(df_scores, df_clique_tda):\n",
    "    df_mapping = df_features[['txId', 'time_step']].drop_duplicates()\n",
    "    df_mapping = df_mapping.merge(df_clique_tda, on=\"time_step\", how=\"left\")\n",
    "\n",
    "    df_scores['txId'] = df_scores['txId'].astype(str)\n",
    "    df_mapping['txId'] = df_mapping['txId'].astype(str)\n",
    "\n",
    "    df_scores = df_scores.merge(df_mapping, on=\"txId\", how=\"left\")\n",
    "\n",
    "    # Drop rows where TDA features are missing\n",
    "    tda_cols = [c for c in df_clique_tda.columns if c != 'time_step']\n",
    "    df_scores = df_scores.dropna(subset=tda_cols)\n",
    "\n",
    "    return df_scores.reset_index(drop=True)\n",
    "\n",
    "# Convert txId to string in both DataFrames\n",
    "df_scores['txId'] = df_scores['txId'].astype(str)\n",
    "df_mapping['txId'] = df_mapping['txId'].astype(str)\n",
    "\n",
    "# Step 3: Now merge back into df_scores using txId\n",
    "df_scores = df_scores.merge(df_mapping, on=\"txId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXNA2Iiw0eU3"
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = \"/content/drive/MyDrive/elliptic_tda\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- PCA + kNN ---\n",
    "role_cols = [c for c in df_scores.columns if c.startswith(\"role_\")]\n",
    "X = df_scores[role_cols].values\n",
    "\n",
    "pca = PCA(n_components=10, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=40).fit(X_pca)\n",
    "_, idxs = nbrs.kneighbors(X_pca)\n",
    "\n",
    "def summarize_H1(coords, thresh=1e-4):\n",
    "    out = ripser(coords, maxdim=1)\n",
    "    dgm1 = out[\"dgms\"][1]\n",
    "    if dgm1.size == 0:\n",
    "        return 0.0, 0.0, 0\n",
    "    lifetimes = np.maximum(dgm1[:, 1] - dgm1[:, 0], 0)\n",
    "    return float(lifetimes.sum()), float(lifetimes.max(initial=0)), int((lifetimes > thresh).sum())\n",
    "\n",
    "# --- resume state ---\n",
    "resume_path = os.path.join(SAVE_DIR, \"tda_resume.json\")\n",
    "start = 0\n",
    "if os.path.exists(resume_path):\n",
    "    with open(resume_path, \"r\") as f:\n",
    "        start = json.load(f).get(\"start\", 0)\n",
    "\n",
    "rows = []\n",
    "batch_size = 2000\n",
    "N = X_pca.shape[0]\n",
    "curr_start = start  # moving window start\n",
    "\n",
    "# Ensure df_scores txId is string before any slicing\n",
    "df_scores[\"txId\"] = df_scores[\"txId\"].astype(str)\n",
    "\n",
    "for i in range(start, N):\n",
    "    coords = X_pca[idxs[i]]\n",
    "    rows.append(summarize_H1(coords))\n",
    "\n",
    "    # flush a batch or the tail\n",
    "    if (i + 1) % batch_size == 0 or i == N - 1:\n",
    "        part = pd.DataFrame(\n",
    "            rows,\n",
    "            columns=[\"vr_total_persistence_h1\", \"vr_max_h1_lifetime\", \"vr_num_long_h1_bars\"]\n",
    "        )\n",
    "        # align txIds for ONLY this batch; force str before saving\n",
    "        part[\"txId\"] = df_scores.iloc[curr_start : i + 1][\"txId\"].astype(str).values\n",
    "\n",
    "        out_path = os.path.join(SAVE_DIR, f\"tda_part_{curr_start}_{i}.parquet\")\n",
    "        part.to_parquet(out_path, index=False)\n",
    "\n",
    "        # persist resume point and advance\n",
    "        with open(resume_path, \"w\") as f:\n",
    "            json.dump({\"start\": i + 1}, f)\n",
    "\n",
    "        curr_start = i + 1\n",
    "        rows = []  # reset\n",
    "\n",
    "# --- merge all parts ---\n",
    "parts = []\n",
    "for f in sorted(os.listdir(SAVE_DIR)):\n",
    "    if f.startswith(\"tda_part_\") and f.endswith(\".parquet\"):\n",
    "        p = pd.read_parquet(os.path.join(SAVE_DIR, f))\n",
    "        p[\"txId\"] = p[\"txId\"].astype(str)\n",
    "        parts.append(p)\n",
    "\n",
    "if len(parts) == 0:\n",
    "    raise ValueError(\"No tda_part_*.parquet files found in SAVE_DIR\")\n",
    "\n",
    "tda_all = pd.concat(parts, ignore_index=True).drop_duplicates(subset=[\"txId\"])\n",
    "\n",
    "# left-join back to df_scores (both sides are str)\n",
    "df_scores = df_scores.merge(tda_all, on=\"txId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOgT0enrmb4f",
    "outputId": "22c00ed1-e71e-4df6-e3f2-5a616922441d"
   },
   "outputs": [],
   "source": [
    "print(df_scores[[\"vr_total_persistence_h1\",\"vr_max_h1_lifetime\",\"vr_num_long_h1_bars\"]].describe())\n",
    "print(\"Zero-only rows:\", (df_scores[\"vr_total_persistence_h1\"]==0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76jxFcJZtzCc"
   },
   "source": [
    "# Save\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qUdVlrkt1Pu"
   },
   "outputs": [],
   "source": [
    "# download\n",
    "# df_scores.to_parquet(\"/content/drive/MyDrive/df_scores_v1.parquet\", index=False)\n",
    "\n",
    "# upload\n",
    "df_scores = pd.read_parquet('/content/drive/MyDrive/df_scores_v1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyJxZ3Un10Cy",
    "outputId": "dd496607-aa13-42a9-8a25-1c5f54f67d41"
   },
   "outputs": [],
   "source": [
    "df_scores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv2WExy9-QzU"
   },
   "source": [
    "We engineered a comprehensive feature set for each transaction, consisting of the label, `txId`, and 61 derived features across multiple domains:\n",
    "\n",
    "- Graph theory: degree, centralities (degree, eigenvector, betweenness), clustering coefficient, cycle scores, Louvain clustering, and spectral embeddings.\n",
    "\n",
    "- Embeddings: role embeddings and Node2Vec (32 dimensions).\n",
    "\n",
    "- Spectral graph features: first four eigenvectors of the graph Laplacian.\n",
    "\n",
    "- Topological Data Analysis (TDA): persistence counts, maxima, and means for $H_0$ and $H_1$; clique Betti numbers; and persistence statistics.\n",
    "\n",
    "- Temporal information: transaction time step.\n",
    "\n",
    "With feature engineering complete, the next steps are to save the dataset, run sanity checks, and proceed to modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQ_0YKnPE63F"
   },
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dc69uoJr_PMo",
    "outputId": "835d4089-9caa-4c62-ef33-ebd8511d83c0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Check for missing values\n",
    "missing = df_scores.isnull().sum()\n",
    "print(\"\\n Missing values:\")\n",
    "print(missing[missing > 0].sort_values(ascending=False))\n",
    "\n",
    "# 2. Check for all-zero columns\n",
    "zero_cols = df_scores.drop(columns=['txId', 'label']).columns[(df_scores.drop(columns=['txId', 'label']) == 0).all()]\n",
    "print(f\"\\ All-zero columns: {list(zero_cols)}\")\n",
    "\n",
    "# 3. Check for low-variance columns\n",
    "low_variance = df_scores.drop(columns=['txId', 'label']).std().sort_values()\n",
    "print(\"\\n Low-variance columns (bottom 5):\")\n",
    "print(low_variance.head(10))\n",
    "\n",
    "# 4. Check for duplicate columns\n",
    "def find_duplicate_columns(df):\n",
    "    duplicate_cols = []\n",
    "    cols = df.columns\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            if df[cols[i]].equals(df[cols[j]]):\n",
    "                duplicate_cols.append((cols[i], cols[j]))\n",
    "    return duplicate_cols\n",
    "\n",
    "dups = find_duplicate_columns(df_scores.drop(columns=['txId', 'label']))\n",
    "print(\"\\n Duplicate columns (exact duplicates):\")\n",
    "for pair in dups:\n",
    "    print(pair)\n",
    "\n",
    "# 5. Check correlation to remove highly redundant features\n",
    "corr_matrix = df_scores.drop(columns=['txId', 'label']).corr().abs()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_pairs = [(col, row) for col in upper_triangle.columns for row in upper_triangle.index if upper_triangle.loc[row, col] > 0.95]\n",
    "\n",
    "print(\"\\n Highly correlated feature pairs (correlation > 0.95):\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(pair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQee7rzZ5asm"
   },
   "source": [
    "So because the `clique_betti_0` and `pagerank` are or low variance we're going to drop those because it won't capture the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYk2yMfq5aT0"
   },
   "outputs": [],
   "source": [
    "# drop clique_betti_0 and pagerank\n",
    "df_scores = df_scores.drop(columns=[\"clique_betti_0\", \"pagerank\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty88W7xp6G-I"
   },
   "source": [
    "Here is a correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUSi3_8i6JH4",
    "outputId": "43c057d3-6a67-48ee-bb7b-31309372823d"
   },
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "num = df_scores.drop(columns=[\"txId\"]).select_dtypes(include=[\"number\"])\n",
    "corr = num.corr()\n",
    "\n",
    "# Extract highly correlated pairs\n",
    "threshold = 0.75\n",
    "high_corr = (\n",
    "    corr.where(~np.triu(np.ones(corr.shape, dtype=bool)))  # keep lower triangle only\n",
    "        .stack()\n",
    "        .reset_index()\n",
    ")\n",
    "high_corr.columns = [\"feature1\", \"feature2\", \"correlation\"]\n",
    "\n",
    "# Filter above threshold\n",
    "high_corr = high_corr[high_corr[\"correlation\"].abs() > threshold] \\\n",
    "                     .sort_values(\"correlation\", ascending=False)\n",
    "\n",
    "print(high_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDznANLg6O7W"
   },
   "source": [
    "So `deg_centrality` and `degree` are exactly the same So let's drop `deg_centrality`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4FffH6k6NGJ"
   },
   "outputs": [],
   "source": [
    "# drop deg_centrality\n",
    "df_scores = df_scores.drop(columns=[\"deg_centrality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7yQYIi55391S",
    "outputId": "785331c2-a253-4f22-e857-6012bfa90162"
   },
   "outputs": [],
   "source": [
    "df_scores.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if_sG0jTzUBl"
   },
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NIRqk8MRxuIh",
    "outputId": "d6ca6bc6-22aa-4119-e2fc-99398f02eb6b"
   },
   "outputs": [],
   "source": [
    "for col in df_scores.drop(columns=[\"txId\", \"label\"]).columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(data=df_scores, x=col, hue=\"label\", bins=100, element=\"step\", stat=\"density\") #kde=True ?\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVc9T718JAfk"
   },
   "source": [
    "### Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "M9ySLMPKJJqh",
    "outputId": "7a2c38d3-a5f8-43ed-d641-3c252d78ec3b"
   },
   "outputs": [],
   "source": [
    "# box plots\n",
    "for col in df_scores.drop(columns=[\"txId\", \"label\"]).columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.boxplot(data=df_scores, x=col, hue=\"label\")\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oECCCgMzYDt"
   },
   "source": [
    "### Pair Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vWNM4DDYzhNs",
    "outputId": "ce8a46cf-ce54-4deb-bcd4-e4206343e480"
   },
   "outputs": [],
   "source": [
    "sample = df_scores.sample(2000, random_state=42)  # don’t plot all 200k\n",
    "sns.pairplot(sample[[\"degree\", \"betweenness\", \"neighbors_illicit_ratio\", \"tda_h1_max\", \"label\"]],\n",
    "             hue=\"label\", corner=True, plot_kws={\"alpha\":0.4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0-yEru2-wwe"
   },
   "source": [
    "### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "n2RNx-9A1FpW",
    "outputId": "82598348-d874-48e4-8c14-bf31e2b14677"
   },
   "outputs": [],
   "source": [
    "num = df_scores.drop(columns=[\"txId\"]).select_dtypes(include=[\"number\"])\n",
    "corr = num.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, vmin=-1, vmax=1)\n",
    "plt.title(\"Feature Correlation Heatmap (upper triangle)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g79kWTqW7Qwq"
   },
   "source": [
    "One more time with this sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ol79g6px5VdD",
    "outputId": "0bf80049-f4cb-4c0b-dbf7-e727ba271a67"
   },
   "outputs": [],
   "source": [
    "# 1. Check for missing values\n",
    "missing = df_scores.isnull().sum()\n",
    "print(\"\\n Missing values:\")\n",
    "print(missing[missing > 0].sort_values(ascending=False))\n",
    "\n",
    "# 2. Check for all-zero columns\n",
    "zero_cols = df_scores.drop(columns=['txId', 'label']).columns[(df_scores.drop(columns=['txId', 'label']) == 0).all()]\n",
    "print(f\"\\ All-zero columns: {list(zero_cols)}\")\n",
    "\n",
    "# 3. Check for low-variance columns\n",
    "low_variance = df_scores.drop(columns=['txId', 'label']).std().sort_values()\n",
    "print(\"\\n Low-variance columns (bottom 5):\")\n",
    "print(low_variance.head(10))\n",
    "\n",
    "# 4. Check for duplicate columns\n",
    "def find_duplicate_columns(df):\n",
    "    duplicate_cols = []\n",
    "    cols = df.columns\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            if df[cols[i]].equals(df[cols[j]]):\n",
    "                duplicate_cols.append((cols[i], cols[j]))\n",
    "    return duplicate_cols\n",
    "\n",
    "dups = find_duplicate_columns(df_scores.drop(columns=['txId', 'label']))\n",
    "print(\"\\n Duplicate columns (exact duplicates):\")\n",
    "for pair in dups:\n",
    "    print(pair)\n",
    "\n",
    "# 5. Check correlation to remove highly redundant features\n",
    "corr_matrix = df_scores.drop(columns=['txId', 'label']).corr().abs()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_pairs = [(col, row) for col in upper_triangle.columns for row in upper_triangle.index if upper_triangle.loc[row, col] > 0.95]\n",
    "\n",
    "print(\"\\n Highly correlated feature pairs (correlation > 0.95):\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5DKiXM-S72cY",
    "outputId": "bb891a06-a1fb-4c47-aac4-d4b5bd457cc3"
   },
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "num = df_scores.drop(columns=[\"txId\"]).select_dtypes(include=[\"number\"])\n",
    "corr = num.corr()\n",
    "\n",
    "# Extract highly correlated pairs\n",
    "threshold = 0.75\n",
    "high_corr = (\n",
    "    corr.where(~np.triu(np.ones(corr.shape, dtype=bool)))  # keep lower triangle only\n",
    "        .stack()\n",
    "        .reset_index()\n",
    ")\n",
    "high_corr.columns = [\"feature1\", \"feature2\", \"correlation\"]\n",
    "\n",
    "# Filter above threshold\n",
    "high_corr = high_corr[high_corr[\"correlation\"].abs() > threshold] \\\n",
    "                     .sort_values(\"correlation\", ascending=False)\n",
    "\n",
    "print(high_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqMRRPDc75cm"
   },
   "source": [
    "So `time_step` and `louvain_cluster` might be dropped with different modeling.\n",
    "\n",
    "`tda_h1_count` and `tda_h0_mean`\n",
    "\n",
    "and\n",
    "\n",
    "`clique_betti_1` and `component_id`\n",
    "\n",
    "Or might be drop so I'm going to keep those in mind just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cube0mWA_24s",
    "outputId": "ca824267-60d9-49b0-f5fe-3db281a33cfa"
   },
   "outputs": [],
   "source": [
    "df_scores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo-VmIK_W-9K"
   },
   "source": [
    "`time_step` vs `louvain_cluster`\n",
    "\n",
    "`time_step` is chronological ordering Or a chain block of two weeks and we can see that on the visual from `df_feaures`. On the other hand, `louvain_cluster` is purely graph structural grouping dense communities.\n",
    "\n",
    "So, they aren’t redundant, and measure different phenomena of temporal(`time_step`) and  community(`lounvain_cluster`). It is correlated, but I and Going to keep both, because models might capture interaction between time and structure.\n",
    "\n",
    "`tda_h1_count` vs `tda_h0_mean`\n",
    "\n",
    "`tda_h1_count` how many 1D loops/cycles persisted, while `tda_h0_mean` have a average lifetime of connected components (0D). Even if numerically correlated, they encode different topology (loops vs. connectivity). I am going to keep both, But putting aware though because it could in the long run have redundancy showing up in feature importance.\n",
    "\n",
    "`clique_betti_1` vs `component_id`\n",
    "\n",
    "`clique_betti_1` is Betti number to topological invariant of rank of 1st homology group. On the other hand, `component_id` is just an index for which connected component the node belongs to. These are very different in meaning. If they’re correlated, it’s dataset-specific and maybe most components are trivial.\n",
    "\n",
    "`component_id` is actually categorical of ID label. Using it as a number can be misleading, so I’d either drop it or encode it properly like a one-hot or embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLUhxPo2uKJe"
   },
   "source": [
    "# Act III Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiqvvkvVBNs4"
   },
   "source": [
    "## Feature Engineering Validation\n",
    "\n",
    "Let's test only illict and lict, So that the feature engineerings are more robust. Let's pass other models and make sure it's accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWhROemvwEDO"
   },
   "source": [
    "### Logistic Regression (Only on illicit and licit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1e_FUFIwQ_h"
   },
   "source": [
    "Logistic Regression expects numeric output, and fraud is often framed as:\n",
    "\n",
    "- Illicit (fraud) = 1\n",
    "- Licit (not fraud) = 0\n",
    "- Unknown = drop or handle separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMKRi7FoEK3Q"
   },
   "source": [
    "Let's do a scalar score so it's on the same page with the features but not with the random forset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRL5myMKwLGo",
    "outputId": "5abc3065-8dd3-495f-d7df-bb5a093ccbba"
   },
   "outputs": [],
   "source": [
    "# Map labels\n",
    "label_map = {'licit': 0, 'illicit': 1}\n",
    "\n",
    "# Drop unknowns (optional — depends on use case)\n",
    "df_logreg = df_scores[df_scores['label'].isin(label_map.keys())].copy()\n",
    "\n",
    "# Apply mapping\n",
    "df_logreg['label'] = df_logreg['label'].map(label_map)\n",
    "\n",
    "# features\n",
    "X = df_logreg.drop(columns=['txId', 'label'])\n",
    "\n",
    "# target\n",
    "y = df_logreg['label']\n",
    "\n",
    "# Scalar standard so no bias numbers\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# feature importance\n",
    "feature_importance = pd.Series(logreg.coef_[0], index=X.columns)\n",
    "top_features = feature_importance.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n Top Features:\")\n",
    "print(top_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNbY2Ob7Kalj"
   },
   "source": [
    "Looking good very accurately but of course with `'shortest_path_to_illicit'` is a huge feartue score with one feature you might need to trim that down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "QhReE4ra11MX",
    "outputId": "6cf1b45c-27cf-45b2-f2c4-746b3a7a3bcb"
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "\n",
    "# Plot\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Licit (0)', 'Illicit (1)'])\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYuD3AcJ3nXl"
   },
   "source": [
    "That's good no errors and the feature engineering are doing the job right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbXWuNiFiEuo"
   },
   "source": [
    "## Logistic Regression: illicit, licit and unknowns\n",
    "\n",
    "Raw: The shortest path to elicit is a high feature so I'm going to keep that in mind but I just want to run it and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWnftaIfwZe6"
   },
   "outputs": [],
   "source": [
    "def run_logistic(df_scores, mode=\"raw\", top_n=50):\n",
    "    # --- Features / target ---\n",
    "    X = df_scores.drop(columns=['txId', 'label']).copy()\n",
    "    y = df_scores['label'].map({'licit': 0, 'illicit': 1})\n",
    "\n",
    "    # Transform shortest-path per mode\n",
    "    if \"shortest_path_to_illicit\" in X.columns:\n",
    "        if mode == \"cap\":\n",
    "            X[\"shortest_path_to_illicit\"] = X[\"shortest_path_to_illicit\"].clip(upper=5)\n",
    "        elif mode == \"sigmoid\":\n",
    "            X[\"shortest_path_to_illicit\"] = 1 / (1 + np.exp(X[\"shortest_path_to_illicit\"]))\n",
    "        elif mode == \"drop\":\n",
    "            X = X.drop(columns=[\"shortest_path_to_illicit\"])\n",
    "\n",
    "    # Known vs unknown\n",
    "    X_known, y_known = X[y.notnull()], y[y.notnull()]\n",
    "    X_unknown = X[y.isnull()]\n",
    "\n",
    "    # Train/test split on knowns\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_known, y_known, test_size=0.3, random_state=42, stratify=y_known\n",
    "    )\n",
    "\n",
    "    # Scale (fit on train, apply to test + unknown)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s  = scaler.transform(X_test)\n",
    "    X_unknown_s = scaler.transform(X_unknown)\n",
    "\n",
    "    # Fit logistic regression\n",
    "    log_reg = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "    log_reg.fit(X_train_s, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred  = log_reg.predict(X_test_s)\n",
    "    y_proba = log_reg.predict_proba(X_test_s)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    print(f\"\\n=== Logistic Mode: {mode.upper()} ===\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Confusion matrix\n",
    "    plt.figure()\n",
    "    ConfusionMatrixDisplay.from_estimator(log_reg, X_test_s, y_test, cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix ({mode})\")\n",
    "    plt.show()\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"{mode} (AUC={auc:.3f})\")\n",
    "    plt.plot([0,1],[0,1],\"--\",color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # \"Importance\" for logistic = absolute standardized coefficients\n",
    "    coef = pd.Series(log_reg.coef_[0], index=X_train.columns)\n",
    "    coef_abs = coef.abs().sort_values(ascending=False).head(15)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(x=coef_abs.values, y=coef_abs.index)\n",
    "    plt.title(f\"Top |Coefficients| ({mode})\")\n",
    "    plt.xlabel(\"|standardized coefficient|\")\n",
    "    plt.ylabel(\"feature\")\n",
    "    plt.show()\n",
    "\n",
    "    # Predict unknowns + save\n",
    "    unknown_proba = log_reg.predict_proba(X_unknown_s)[:, 1]\n",
    "    df_unknown = df_scores.loc[y.isnull(), ['txId']].copy()\n",
    "    df_unknown['prob_illicit'] = unknown_proba\n",
    "    df_unknown = df_unknown.sort_values('prob_illicit', ascending=False)\n",
    "\n",
    "    out_path = f\"suspicious_unknowns_log_{mode}.csv\"\n",
    "    df_unknown.to_csv(out_path, index=False)\n",
    "    print(f\"Saved top suspicious unknowns to {out_path}\")\n",
    "\n",
    "    # Show top-N preview\n",
    "    display(df_unknown.head(top_n))\n",
    "\n",
    "    return df_unknown, auc, log_reg, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UFb3iOUewb1R",
    "outputId": "c4f04fbe-68d1-4687-da0b-3cb1568865f4"
   },
   "outputs": [],
   "source": [
    "df_log_raw, auc_raw, _, _  = run_logistic(df_scores, mode=\"raw\", top_n=50)\n",
    "df_log_cap, auc_cap, _, _  = run_logistic(df_scores, mode=\"cap\", top_n=50)\n",
    "df_log_drop, auc_drop, _, _ = run_logistic(df_scores, mode=\"drop\", top_n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pabmqTwuOQL"
   },
   "source": [
    "### Random Forest (Only on illicit and licit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "n0n7aofREi8U",
    "outputId": "025bd628-31f4-4dcd-8251-69d4cc5cced6"
   },
   "outputs": [],
   "source": [
    "# Known labeled data\n",
    "df_known = df_scores[df_scores['label'].isin(['licit', 'illicit'])].copy()\n",
    "df_known['label'] = df_known['label'].map({'licit': 0, 'illicit': 1})\n",
    "\n",
    "# Drop leakage\n",
    "X_rf = df_known.drop(columns=['txId', 'label'])\n",
    "y_rf = df_known['label']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_rf, y_rf, test_size=0.2, stratify=y_rf, random_state=42\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',  # helpful for imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Licit', 'Illicit'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Random Forest on Known)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbM_abQmK3mR"
   },
   "source": [
    "Looking good no errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "id": "0sVVypAuE_0X",
    "outputId": "470539a5-88a2-443b-a276-94c8c0ba2ba1"
   },
   "outputs": [],
   "source": [
    "importances = pd.Series(rf.feature_importances_, index=X_rf.columns)\n",
    "top_features = importances.sort_values(ascending=False).head(15)\n",
    "\n",
    "print(\"\\n Top Features:\")\n",
    "print(top_features)\n",
    "\n",
    "# Optional: plot\n",
    "top_features.plot(kind='barh', figsize=(8, 6), title=\"Top Random Forest Features\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z09vish2K7Aq"
   },
   "source": [
    "Good again except `'shortest_path_to_illicit'` Is a huge feature block the other ones and that was the whole point to have a lot of robust features so it's more we definitely need to do something about that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px_zi-rnq2IN"
   },
   "source": [
    "## Random Forset on unknows, licit and illicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJM42qROqXn5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_random_forest(df_scores, mode=\"cap\", top_n=50):\n",
    "    \"\"\"\n",
    "    Train Random Forest on licit vs illicit and apply to unknowns.\n",
    "    mode: \"raw\", \"cap\", \"drop\"\n",
    "    \"\"\"\n",
    "    # --- Prepare X, y ---\n",
    "    X = df_scores.drop(columns=['txId', 'label']).copy()\n",
    "    y = df_scores['label'].map({'licit':0, 'illicit':1})\n",
    "\n",
    "    # Transform shortest path depending on mode\n",
    "    if \"shortest_path_to_illicit\" in X.columns:\n",
    "        if mode == \"cap\":\n",
    "            X[\"shortest_path_to_illicit\"] = X[\"shortest_path_to_illicit\"].clip(upper=5)\n",
    "        elif mode == \"drop\":\n",
    "            X = X.drop(columns=[\"shortest_path_to_illicit\"])\n",
    "        # mode == \"raw\" → leave unchanged\n",
    "\n",
    "    # Split into known and unknown\n",
    "    X_known, y_known = X[y.notnull()], y[y.notnull()]\n",
    "    X_unknown = X[y.isnull()]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_known, y_known, test_size=0.3, random_state=42, stratify=y_known\n",
    "    )\n",
    "\n",
    "    # --- Fit Random Forest ---\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        class_weight=\"balanced\",  # helps with class imbalance\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # --- Evaluate on test ---\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_proba = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "    print(f\"\\n=== Random Forest Mode: {mode.upper()} ===\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    print(\"AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "    # Confusion matrix\n",
    "    ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test, cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix ({mode})\")\n",
    "    plt.show()\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f\"{mode} (AUC={roc_auc_score(y_test,y_proba):.3f})\")\n",
    "    plt.plot([0,1],[0,1],\"--\",color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Feature importance ---\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    top_feats = importances.sort_values(ascending=False).head(15)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(x=top_feats.values, y=top_feats.index)\n",
    "    plt.title(f\"Top Feature Importances ({mode})\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Predict on unknowns ---\n",
    "    unknown_proba = rf.predict_proba(X_unknown)[:,1]\n",
    "    df_unknown = df_scores.loc[y.isnull(), ['txId']].copy()\n",
    "    df_unknown['prob_illicit'] = unknown_proba\n",
    "    df_unknown = df_unknown.sort_values(\"prob_illicit\", ascending=False)\n",
    "\n",
    "    # Save suspicious unknowns\n",
    "    filename = f\"suspicious_unknowns_rf_{mode}.csv\"\n",
    "    df_unknown.to_csv(filename, index=False)\n",
    "    print(f\"Saved top suspicious unknowns to {filename}\")\n",
    "\n",
    "    # Show top results\n",
    "    print(\"\\nTop suspicious unknowns:\")\n",
    "    display(df_unknown.head(top_n))\n",
    "\n",
    "    return df_unknown, rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e2-NFPd9qkKK",
    "outputId": "f1e7040b-5fcb-421b-f3c2-e7348b59c37c"
   },
   "outputs": [],
   "source": [
    "df_rf_raw, model_raw = run_random_forest(df_scores, mode=\"raw\")\n",
    "df_rf_cap, model_cap = run_random_forest(df_scores, mode=\"cap\")\n",
    "df_rf_drop, model_drop = run_random_forest(df_scores, mode=\"drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h58Aj37rIFxF"
   },
   "source": [
    "### Neural Network Classifier (Only on illicit and licit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Qp5XTIYOIfMH",
    "outputId": "45720fb0-1317-49d7-d79e-d5b13150d477"
   },
   "outputs": [],
   "source": [
    "# 1. Prepare data (licit + illicit only)\n",
    "df_known = df_scores[df_scores['label'].isin(['licit', 'illicit'])].copy()\n",
    "df_known['label'] = df_known['label'].map({'licit': 0, 'illicit': 1})\n",
    "\n",
    "X_known = df_known.drop(columns=['txId', 'label'])\n",
    "y_known = df_known['label']\n",
    "\n",
    "# 2. Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_known)\n",
    "\n",
    "# 3. Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_known, test_size=0.2, stratify=y_known, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Build model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 5. Train\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 6. Evaluate\n",
    "y_pred_probs = model.predict(X_test).flatten()\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Licit\", \"Illicit\"])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Neural Network)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nF7QqIRMySe"
   },
   "source": [
    "Do have errors here but overall still pretty good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se3LEFlX022P"
   },
   "source": [
    "## Neural Network unknows, licit and illict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unL13Uev03hc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_neural_net(df_scores, mode=\"raw\", top_n=20):\n",
    "    # --- Features/labels ---\n",
    "    X = df_scores.drop(columns=['txId', 'label']).copy()\n",
    "    y = df_scores['label'].map({'licit':0, 'illicit':1})\n",
    "\n",
    "    # --- Handle shortest_path_to_illicit ---\n",
    "    if \"shortest_path_to_illicit\" in X.columns:\n",
    "        if mode == \"cap\":\n",
    "            X[\"shortest_path_to_illicit\"] = X[\"shortest_path_to_illicit\"].clip(upper=5)\n",
    "        elif mode == \"sigmoid\":\n",
    "            X[\"shortest_path_to_illicit\"] = 1 / (1 + np.exp(X[\"shortest_path_to_illicit\"]))\n",
    "        elif mode == \"drop\":\n",
    "            X = X.drop(columns=[\"shortest_path_to_illicit\"])\n",
    "\n",
    "    # --- Split known/unknown ---\n",
    "    X_known, y_known = X[y.notnull()], y[y.notnull()]\n",
    "    X_unknown = X[y.isnull()]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_known, y_known, test_size=0.3, random_state=42, stratify=y_known\n",
    "    )\n",
    "\n",
    "    # --- Scale ---\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_unknown_scaled = scaler.transform(X_unknown)\n",
    "\n",
    "    # --- Fit Neural Net ---\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),   # two hidden layers\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=1e-4,\n",
    "        max_iter=200,\n",
    "        random_state=42\n",
    "    )\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # --- Evaluate ---\n",
    "    y_pred = mlp.predict(X_test_scaled)\n",
    "    y_proba = mlp.predict_proba(X_test_scaled)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    print(f\"\\n=== Neural Net, Mode: {mode.upper()} ===\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Confusion matrix\n",
    "    ConfusionMatrixDisplay.from_estimator(mlp, X_test_scaled, y_test, cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix (NN, {mode})\")\n",
    "    plt.show()\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f\"NN-{mode} (AUC={auc:.3f})\")\n",
    "    plt.plot([0,1],[0,1],\"--\",color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Neural Net)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Predict unknowns ---\n",
    "    unknown_proba = mlp.predict_proba(X_unknown_scaled)[:,1]\n",
    "    df_unknown = df_scores.loc[y.isnull(), ['txId']].copy()\n",
    "    df_unknown['prob_illicit'] = unknown_proba\n",
    "\n",
    "    filename = f\"suspicious_unknowns_nn_{mode}.csv\"\n",
    "    df_unknown.to_csv(filename, index=False)\n",
    "    print(f\"Saved top suspicious unknowns to {filename}\")\n",
    "\n",
    "    print(\"\\nTop suspicious unknowns:\")\n",
    "    display(df_unknown.sort_values(by=\"prob_illicit\", ascending=False).head(top_n))\n",
    "\n",
    "    return df_unknown, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Mxx0ISsC1DZT",
    "outputId": "a760ae5d-d8dd-4eae-9a9d-d48a2db3b392"
   },
   "outputs": [],
   "source": [
    "for mode in [\"raw\", \"cap\", \"drop\"]:\n",
    "    run_neural_net(df_scores, mode=mode, top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6Kn8yUH6wU1"
   },
   "source": [
    "## Simple Ensemble (Equal Weighting)\n",
    "\n",
    "To build a baseline ensemble, we took the average of three model predictions: Logistic Regression (LR), Random Forest (RF), and Neural Network (NN). Formally:\n",
    "\n",
    "$$ \\text{Ensemble_Prob} = \\frac{1}{3} (LR + RF + NN) $$\n",
    "\n",
    "This approach treats each model equally, smoothing out individual weaknesses and producing a more stable probability score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrWuIpW19tUV",
    "outputId": "ce975bad-da8a-44a6-aa46-21483584093f"
   },
   "outputs": [],
   "source": [
    "# change to ['raw', 'cap', 'drop']\n",
    "MODES = [\"cap\", \"drop\"]\n",
    "MODELS = {\n",
    "    \"log\": \"suspicious_unknowns_log_{}.csv\",\n",
    "    \"rf\":  \"suspicious_unknowns_rf_{}.csv\",\n",
    "    \"nn\":  \"suspicious_unknowns_nn_{}.csv\",\n",
    "}\n",
    "\n",
    "def load_model_df(model_key, pattern, modes=MODES):\n",
    "    \"\"\"\n",
    "    Loads cap/drop for one model and returns:\n",
    "      txId, prob_cap, prob_drop  (only the modes that exist)\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for m in modes:\n",
    "        f = Path(pattern.format(m))\n",
    "        if f.exists():\n",
    "            df = pd.read_csv(f)\n",
    "            if \"txId\" not in df.columns or \"prob_illicit\" not in df.columns:\n",
    "                raise ValueError(f\"{f} must have columns: txId, prob_illicit\")\n",
    "            df = df[[\"txId\", \"prob_illicit\"]].rename(columns={\"prob_illicit\": f\"prob_{m}\"})\n",
    "            parts.append(df)\n",
    "        else:\n",
    "            print(f\"[warn] Missing file for {model_key}-{m}: {f}\")\n",
    "    if not parts:\n",
    "        return None\n",
    "    # Outer-merge in case sets of unknown txIds differ by mode\n",
    "    out = reduce(lambda l, r: pd.merge(l, r, on=\"txId\", how=\"outer\"), parts)\n",
    "    return out\n",
    "\n",
    "# Load each model's cap/drop files and compute per-model averages\n",
    "model_avgs = []\n",
    "for key, patt in MODELS.items():\n",
    "    df_m = load_model_df(key, patt, modes=MODES)\n",
    "    if df_m is None:\n",
    "        continue\n",
    "    # mean across available mode columns\n",
    "    mode_cols = [c for c in df_m.columns if c.startswith(\"prob_\")]\n",
    "    df_m[f\"{key}_avg\"] = df_m[mode_cols].mean(axis=1, skipna=True)\n",
    "    model_avgs.append(df_m[[\"txId\", f\"{key}_avg\"]])\n",
    "\n",
    "# Safety check\n",
    "if not model_avgs:\n",
    "    raise RuntimeError(\"No model files found. Make sure you saved your CSVs with the expected names.\")\n",
    "\n",
    "# Merge per-model averages\n",
    "ensemble = reduce(lambda l, r: pd.merge(l, r, on=\"txId\", how=\"outer\"), model_avgs)\n",
    "\n",
    "# Final ensemble across models (mean of available per-model avgs)\n",
    "avg_cols = [c for c in ensemble.columns if c.endswith(\"_avg\")]\n",
    "ensemble[\"ensemble_prob\"] = ensemble[avg_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "# Rank & save\n",
    "ensemble = ensemble.sort_values(\"ensemble_prob\", ascending=False).reset_index(drop=True)\n",
    "ensemble.to_csv(\"suspicious_ensemble.csv\", index=False)\n",
    "print(\"Saved ensemble to suspicious_ensemble.csv\")\n",
    "print(ensemble.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDQKfNIz0Z3g"
   },
   "source": [
    "This “flat-line” ensemble ensures no single model dominates. By averaging across LR, RF, and NN, we create a balanced risk score (ensemble_prob) that reduces noise and increases reliability in identifying suspicious transactions.\n",
    "\n",
    "At the top of 50 risk scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hv8CR_UWVnWP"
   },
   "source": [
    "## Ensemble of Three Models Using Probabilities\n",
    "\n",
    "We combined three different model types—Logistic Regression, Random Forest, and Neural Network—into a single ensemble. A simple average of their probabilities provides a balanced score, since each model contributes equally.\n",
    "\n",
    "However, in practice some models may perform better than others. In that case, it can be even more effective to create an **optimized ensemble**, where models are weighted differently (or stacked) to maximize overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzBn5mj5Vy-4"
   },
   "outputs": [],
   "source": [
    "# You’ll need small wrappers that, given train_idx/test_idx and a mode (\"cap\"/\"drop\"),\n",
    "# fit your model (LR/RF/NN) on X_train_known and output probabilities on X_valid_known and X_unknown.\n",
    "# You already have run_logistic/run_random_forest/run_neural_net — adapt them to accept indices.\n",
    "\n",
    "def get_base_oof_and_unknown(df_scores, build_and_predict_fn_list, modes=(\"cap\",\"drop\"), n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      oof_df: DataFrame with columns ['txId','y_true', features...] for known examples\n",
    "      unk_df: DataFrame with columns ['txId', features...] for unknown examples\n",
    "    features include one column per (model,mode), e.g., 'log_cap', 'rf_drop', ...\n",
    "    \"\"\"\n",
    "    X = df_scores.drop(columns=['txId','label'])\n",
    "    y = df_scores['label'].map({'licit':0,'illicit':1})\n",
    "    tx = df_scores['txId']\n",
    "\n",
    "    known_mask = y.notnull()\n",
    "    unk_mask = y.isnull()\n",
    "\n",
    "    Xk, yk, txk = X[known_mask].reset_index(drop=True), y[known_mask].reset_index(drop=True), tx[known_mask].reset_index(drop=True)\n",
    "    Xu, txu = X[unk_mask].reset_index(drop=True), tx[unk_mask].reset_index(drop=True)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Initialize storage\n",
    "    oof = pd.DataFrame({\"txId\": txk, \"y_true\": yk})\n",
    "    unk = pd.DataFrame({\"txId\": txu})\n",
    "\n",
    "    # For each base model fn, and each mode, collect OOF predictions and fold-avg unknown preds\n",
    "    for name, fn in build_and_predict_fn_list:  # e.g., [(\"log\", log_fn), (\"rf\", rf_fn), (\"nn\", nn_fn)]\n",
    "        for mode in modes:\n",
    "            oof_col = f\"{name}_{mode}\"\n",
    "            oof[oof_col] = np.nan\n",
    "            unk_preds_folds = []\n",
    "\n",
    "            for train_idx, valid_idx in skf.split(Xk, yk):\n",
    "                # fn should: fit on (Xk.iloc[train_idx], yk.iloc[train_idx]) with mode,\n",
    "                # return (valid_pred_proba_on_Xk_valid, unknown_pred_proba_on_Xu)\n",
    "                valid_pred, unk_pred = fn(Xk.iloc[train_idx], yk.iloc[train_idx],\n",
    "                                          Xk.iloc[valid_idx], Xu, mode)\n",
    "\n",
    "                oof.loc[valid_idx, oof_col] = valid_pred\n",
    "                unk_preds_folds.append(unk_pred)\n",
    "\n",
    "            # unknown preds: average over folds\n",
    "            unk[oof_col] = np.vstack(unk_preds_folds).mean(axis=0)\n",
    "\n",
    "    return oof, unk\n",
    "\n",
    "\n",
    "def logistic_wrapper(Xtr, ytr, Xval, Xunk, mode):\n",
    "    # Apply your mode transform to shortest_path_to_illicit on Xtr/Xval/Xunk consistently.\n",
    "    def transform_shortest_path(df):\n",
    "        if \"shortest_path_to_illicit\" in df.columns:\n",
    "            df = df.copy()\n",
    "            if mode == \"cap\":\n",
    "                df[\"shortest_path_to_illicit\"] = df[\"shortest_path_to_illicit\"].clip(upper=5)\n",
    "            elif mode == \"drop\":\n",
    "                df = df.drop(columns=[\"shortest_path_to_illicit\"])\n",
    "        return df\n",
    "\n",
    "    Xtr_t = transform_shortest_path(Xtr)\n",
    "    Xval_t = transform_shortest_path(Xval)\n",
    "    Xunk_t = transform_shortest_path(Xunk)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = scaler.fit_transform(Xtr_t)\n",
    "    Xval_s = scaler.transform(Xval_t)\n",
    "    Xunk_s = scaler.transform(Xunk_t)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "    clf.fit(Xtr_s, ytr)\n",
    "\n",
    "    val_pred = clf.predict_proba(Xval_s)[:,1]\n",
    "    unk_pred = clf.predict_proba(Xunk_s)[:,1]\n",
    "    return val_pred, unk_pred\n",
    "\n",
    "# Build the list of base wrappers\n",
    "build_and_predict_fn_list = [\n",
    "    (\"log\", logistic_wrapper),\n",
    "    # (\"rf\",   rf_wrapper),     # implement like logistic_wrapper but without scaling\n",
    "    # (\"nn\",   nn_wrapper),     # implement with StandardScaler + MLPClassifier\n",
    "]\n",
    "\n",
    "# 1) Get OOF predictions (known) + averaged unknown predictions per base feature\n",
    "oof_df, unk_df = get_base_oof_and_unknown(df_scores, build_and_predict_fn_list, modes=(\"cap\",\"drop\"), n_splits=5)\n",
    "\n",
    "# 2) Train meta-model on OOF (known)\n",
    "#    Choose features (all base-mode columns) and target y_true\n",
    "meta_feats = [c for c in oof_df.columns if c not in (\"txId\",\"y_true\")]\n",
    "meta_clf = LogisticRegression(max_iter=1000)\n",
    "meta_clf.fit(oof_df[meta_feats], oof_df[\"y_true\"])\n",
    "\n",
    "# 3) Apply meta-model to unknowns using the averaged base predictions\n",
    "unk_df[\"ensemble_prob_stack\"] = meta_clf.predict_proba(unk_df[meta_feats])[:,1]\n",
    "ranked_unknowns = unk_df.sort_values(\"ensemble_prob_stack\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gii2Wwe-WIdp",
    "outputId": "59d1a979-aef2-4077-82d0-c1bc47793505"
   },
   "outputs": [],
   "source": [
    "print(ranked_unknowns.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBCBKHYo1i2X"
   },
   "source": [
    "Here is an optimize for the three models of assemble and here is top 50 risk score.\n",
    "\n",
    "Next we'll do the percentage to see what the linear regression learning from the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGVglhiA6J6m",
    "outputId": "e38b0df3-6104-4464-8e58-675e963a2693"
   },
   "outputs": [],
   "source": [
    "# Use the exact matrix you trained on to guarantee alignment\n",
    "X_fit = oof_df[meta_feats].copy()\n",
    "feat_names = list(X_fit.columns)\n",
    "\n",
    "# Coefficients (binary logistic => shape (1, n_features))\n",
    "coefs = np.ravel(meta_clf.coef_)\n",
    "intercept = float(np.ravel(meta_clf.intercept_)[0])\n",
    "\n",
    "# Sanity check: lengths must match\n",
    "if len(feat_names) != len(coefs):\n",
    "    print(f\"[WARN] Feature/coef length mismatch: feats={len(feat_names)}, coefs={len(coefs)}\")\n",
    "    # Fallback: align to the smaller length so you can still inspect\n",
    "    L = min(len(feat_names), len(coefs))\n",
    "    feat_names = feat_names[:L]\n",
    "    coefs = coefs[:L]\n",
    "\n",
    "coef_df = pd.DataFrame({\"feature\": feat_names, \"coef\": coefs})\n",
    "# Split \"log_cap\" -> model=\"log\", mode=\"cap\"\n",
    "parts = coef_df[\"feature\"].str.split(\"_\", n=1, expand=True)\n",
    "coef_df[\"model\"] = parts[0]\n",
    "coef_df[\"mode\"]  = parts[1]\n",
    "\n",
    "print(\"=== Meta-model (stacker) coefficients ===\")\n",
    "print(coef_df.sort_values(\"coef\", ascending=False).to_string(index=False))\n",
    "print(f\"\\nIntercept: {intercept:+.6f}\\n\")\n",
    "\n",
    "# Signed influence by base model\n",
    "signed_sum = coef_df.groupby(\"model\")[\"coef\"].sum().sort_values(ascending=False)\n",
    "\n",
    "# Positive-contribution percentages (stakeholder-friendly)\n",
    "pos_df = coef_df.assign(pos_coef=np.clip(coef_df[\"coef\"], 0, None))\n",
    "pos_sum = pos_df.groupby(\"model\")[\"pos_coef\"].sum()\n",
    "\n",
    "# If everything is <= 0, fall back to absolute values\n",
    "if pos_sum.sum() == 0:\n",
    "    print(\"Note: all learned coefficients are <= 0; using absolute values for percentages as a fallback.\\n\")\n",
    "    pos_sum = coef_df.groupby(\"model\")[\"coef\"].apply(np.abs).sum()\n",
    "\n",
    "percent = (pos_sum / pos_sum.sum() * 100).sort_values(ascending=False)\n",
    "\n",
    "print(\"=== Model-level influence (signed sum of coefs) ===\")\n",
    "for m, v in signed_sum.items():\n",
    "    print(f\"{m:>3}: {v:+.6f}\")\n",
    "\n",
    "print(\"\\n=== Model-level contribution (percent of positive weight) ===\")\n",
    "for m, v in percent.items():\n",
    "    print(f\"{m:>3}: {v:5.1f}%\")\n",
    "\n",
    "# Optional: see whether 'cap' vs 'drop' matters per model\n",
    "per_mode = pos_df.groupby([\"model\",\"mode\"])[\"pos_coef\"].sum()\n",
    "print(\"\\n=== Positive contribution by model × mode ===\")\n",
    "print(per_mode.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI0g64-p65Ln"
   },
   "source": [
    "Interpreting the Stacking Weights (cap vs drop)\n",
    "\n",
    "In our stacking setup, we trained two variants of the logistic model:\n",
    "\n",
    "cap: uses a capped version of shortest_path_to_illicit (e.g., clip at 5).\n",
    "\n",
    "drop: excludes shortest_path_to_illicit.\n",
    "\n",
    "The stacker learned a much larger weight for the cap variant than for drop, which means the capped shortest-path signal is highly predictive. This matches our earlier finding that shortest path to an illicit node is one of the strongest features across models.\n",
    "\n",
    "Important: The “percentages” we printed right now reflect only the logistic model’s two variants (cap vs drop), because RF and NN aren’t included yet. So it’s correct to say “cap contributes more than drop,” but it’s not yet a three-model split (LR vs RF vs NN).\n",
    "\n",
    "Next step to get true ensemble percentages: add the RF and NN wrappers to the stack, re-fit, and then read the model-level percentages (LR / RF / NN). Those learned weights are what you’d use to describe or implement an optimized three-model ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJHKc84BN97I"
   },
   "source": [
    "### OOF Stacked Meta-Model\n",
    "\n",
    "We extend beyond a single model by stacking the outputs of Logistic Regression, Random Forest, and Neural Network. Each base model produces out-of-fold (OOF) predictions, and then a meta-model (logistic regression) learns how to combine them into a single probability score.\n",
    "\n",
    "This approach allows us to use the strengths of all three models together, rather than relying on one alone. The meta-model effectively decides how much weight each base model should receive, creating a more optimized ensemble probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEuyyDyTenEF"
   },
   "outputs": [],
   "source": [
    "# --- helper: apply the mode transform consistently ---\n",
    "def transform_shortest_path(df, mode):\n",
    "    if \"shortest_path_to_illicit\" in df.columns:\n",
    "        df = df.copy()\n",
    "        if mode == \"cap\":\n",
    "            df[\"shortest_path_to_illicit\"] = df[\"shortest_path_to_illicit\"].clip(upper=5)\n",
    "        elif mode == \"drop\":\n",
    "            df = df.drop(columns=[\"shortest_path_to_illicit\"])\n",
    "    return df\n",
    "\n",
    "# --- base model wrappers: each returns (val_pred, unk_pred) ---\n",
    "\n",
    "def logistic_wrapper(Xtr, ytr, Xval, Xunk, mode):\n",
    "    Xtr_t = transform_shortest_path(Xtr, mode)\n",
    "    Xval_t = transform_shortest_path(Xval, mode)\n",
    "    Xunk_t = transform_shortest_path(Xunk, mode)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = scaler.fit_transform(Xtr_t)\n",
    "    Xval_s = scaler.transform(Xval_t)\n",
    "    Xunk_s = scaler.transform(Xunk_t)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "    clf.fit(Xtr_s, ytr)\n",
    "    return clf.predict_proba(Xval_s)[:,1], clf.predict_proba(Xunk_s)[:,1]\n",
    "\n",
    "def rf_wrapper(Xtr, ytr, Xval, Xunk, mode):\n",
    "    Xtr_t = transform_shortest_path(Xtr, mode)\n",
    "    Xval_t = transform_shortest_path(Xval, mode)\n",
    "    Xunk_t = transform_shortest_path(Xunk, mode)\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=None,\n",
    "        class_weight=\"balanced\", n_jobs=-1, random_state=42\n",
    "    )\n",
    "    rf.fit(Xtr_t, ytr)\n",
    "    return rf.predict_proba(Xval_t)[:,1], rf.predict_proba(Xunk_t)[:,1]\n",
    "\n",
    "def nn_wrapper(Xtr, ytr, Xval, Xunk, mode):\n",
    "    Xtr_t = transform_shortest_path(Xtr, mode)\n",
    "    Xval_t = transform_shortest_path(Xval, mode)\n",
    "    Xunk_t = transform_shortest_path(Xunk, mode)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = scaler.fit_transform(Xtr_t)\n",
    "    Xval_s = scaler.transform(Xval_t)\n",
    "    Xunk_s = scaler.transform(Xunk_t)\n",
    "\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(64,32), activation='relu',\n",
    "                        solver='adam', alpha=1e-4, max_iter=200, random_state=42)\n",
    "    mlp.fit(Xtr_s, ytr)\n",
    "    return mlp.predict_proba(Xval_s)[:,1], mlp.predict_proba(Xunk_s)[:,1]\n",
    "\n",
    "# --- stacking utilities you already have (kept for completeness) ---\n",
    "\n",
    "def get_base_oof_and_unknown(df_scores, build_and_predict_fn_list, modes=(\"cap\",\"drop\"), n_splits=5, random_state=42):\n",
    "    X = df_scores.drop(columns=['txId','label'])\n",
    "    y = df_scores['label'].map({'licit':0,'illicit':1})\n",
    "    tx = df_scores['txId']\n",
    "\n",
    "    known_mask = y.notnull()\n",
    "    unk_mask = y.isnull()\n",
    "\n",
    "    Xk, yk, txk = X[known_mask].reset_index(drop=True), y[known_mask].reset_index(drop=True), tx[known_mask].reset_index(drop=True)\n",
    "    Xu, txu = X[unk_mask].reset_index(drop=True), tx[unk_mask].reset_index(drop=True)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    oof = pd.DataFrame({\"txId\": txk, \"y_true\": yk})\n",
    "    unk = pd.DataFrame({\"txId\": txu})\n",
    "\n",
    "    for name, fn in build_and_predict_fn_list:\n",
    "        for mode in modes:\n",
    "            col = f\"{name}_{mode}\"\n",
    "            oof[col] = np.nan\n",
    "            unk_fold_preds = []\n",
    "            for tr_idx, va_idx in skf.split(Xk, yk):\n",
    "                val_pred, unk_pred = fn(Xk.iloc[tr_idx], yk.iloc[tr_idx],\n",
    "                                        Xk.iloc[va_idx], Xu, mode)\n",
    "                oof.loc[va_idx, col] = val_pred\n",
    "                unk_fold_preds.append(unk_pred)\n",
    "            unk[col] = np.vstack(unk_fold_preds).mean(axis=0)\n",
    "    return oof, unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQJUQuWAerzX",
    "outputId": "57db321a-5692-4f9d-cb9e-59eeb0c24c59"
   },
   "outputs": [],
   "source": [
    "# 1) Collect OOF preds for knowns + averaged preds for unknowns\n",
    "build_and_predict_fn_list = [\n",
    "    (\"log\", logistic_wrapper),\n",
    "    (\"rf\",  rf_wrapper),\n",
    "    (\"nn\",  nn_wrapper),\n",
    "]\n",
    "oof_df, unk_df = get_base_oof_and_unknown(df_scores, build_and_predict_fn_list, modes=(\"cap\",\"drop\"), n_splits=5)\n",
    "\n",
    "# 2) Train a simple meta-model on OOF predictions (to avoid leakage)\n",
    "meta_feats = [c for c in oof_df.columns if c not in (\"txId\",\"y_true\")]\n",
    "meta = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "meta.fit(oof_df[meta_feats], oof_df[\"y_true\"])\n",
    "\n",
    "# 3) Apply meta-model to unknowns (using fold-averaged base preds)\n",
    "unk_df[\"ensemble_prob_stack\"] = meta.predict_proba(unk_df[meta_feats])[:,1]\n",
    "ranked_unknowns = unk_df.sort_values(\"ensemble_prob_stack\", ascending=False)\n",
    "\n",
    "# Save & preview\n",
    "ranked_unknowns.to_csv(\"suspicious_unknowns_stacked.csv\", index=False)\n",
    "print(ranked_unknowns.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1n-Iwpl8Q5V"
   },
   "source": [
    "Here's the top of all three models and let's go see how much probability from those models are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFH9XieL8q5X",
    "outputId": "046e6f4b-bafb-4d3f-c0f8-ba964e5c2f78"
   },
   "outputs": [],
   "source": [
    "# Use exactly what the meta-model saw to avoid any mismatch\n",
    "X_fit = oof_df[meta_feats].copy()\n",
    "feat_names = list(X_fit.columns)\n",
    "\n",
    "# Coefs & intercept (binary logistic: shape (1, n_features))\n",
    "coefs = np.ravel(meta.coef_)\n",
    "intercept = float(np.ravel(meta.intercept_)[0])\n",
    "\n",
    "# Safety check\n",
    "if len(feat_names) != len(coefs):\n",
    "    print(f\"[WARN] Feature/coef length mismatch: feats={len(feat_names)}, coefs={len(coefs)}\")\n",
    "    L = min(len(feat_names), len(coefs))\n",
    "    feat_names, coefs = feat_names[:L], coefs[:L]\n",
    "\n",
    "# Build coef table\n",
    "coef_df = pd.DataFrame({\"feature\": feat_names, \"coef\": coefs})\n",
    "parts = coef_df[\"feature\"].str.split(\"_\", n=1, expand=True)\n",
    "coef_df[\"model\"] = parts[0]   # 'log' | 'rf' | 'nn'\n",
    "coef_df[\"mode\"]  = parts[1]   # 'cap' | 'drop'\n",
    "\n",
    "print(\"\\n=== Stacker coefficients per feature ===\")\n",
    "print(coef_df.sort_values(\"coef\", ascending=False).to_string(index=False))\n",
    "print(f\"\\nIntercept: {intercept:+.6f}\\n\")\n",
    "\n",
    "# 1) Signed influence by model (positive pushes risk up)\n",
    "signed_sum = coef_df.groupby(\"model\")[\"coef\"].sum().sort_values(ascending=False)\n",
    "\n",
    "# 2) Stakeholder-friendly % split:\n",
    "#    Use only positive mass (negatives clipped to 0) so we don’t give % credit to down-weights.\n",
    "pos_df = coef_df.assign(pos_coef=np.clip(coef_df[\"coef\"], 0, None))\n",
    "pos_sum = pos_df.groupby(\"model\")[\"pos_coef\"].sum()\n",
    "\n",
    "# Fallback: if all coefs <= 0, use absolute values to show relative magnitude\n",
    "if pos_sum.sum() == 0:\n",
    "    print(\"Note: all learned coefficients are <= 0; using absolute magnitudes for percentages.\\n\")\n",
    "    pos_sum = coef_df.groupby(\"model\")[\"coef\"].apply(np.abs).sum()\n",
    "\n",
    "percent = (pos_sum / pos_sum.sum() * 100).sort_values(ascending=False)\n",
    "\n",
    "print(\"=== Model-level influence (signed sum of coefs) ===\")\n",
    "for m, v in signed_sum.items():\n",
    "    print(f\"{m:>3}: {v:+.6f}\")\n",
    "\n",
    "print(\"\\n=== Model-level contribution (percent of positive weight) ===\")\n",
    "for m, v in percent.items():\n",
    "    print(f\"{m:>3}: {v:5.1f}%\")\n",
    "\n",
    "# Optional: see cap vs drop within each model\n",
    "per_mode = pos_df.groupby([\"model\",\"mode\"])[\"pos_coef\"].sum()\n",
    "print(\"\\n=== Positive contribution by model × mode ===\")\n",
    "print(per_mode.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrsDFj2D8zY2"
   },
   "source": [
    "So here's the contribution of the percentage of the weights: RF: 43.2%, LR: 32.4%, NN: 24.4%. Now we're using a average and the weight and we'll merge those in. And now we'll try to sort all of it using the best both worlds for the average and the waited to find unknown nodes of high probability of illicit.\n",
    "\n",
    "Next using average and a weighted score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lC9avGzQF98r",
    "outputId": "e5ba916d-8665-4d62-831a-f3b2a4956618"
   },
   "outputs": [],
   "source": [
    "# --- 1) Build per-model averages from your cap/drop columns ---\n",
    "need = [\"log_cap\",\"log_drop\",\"rf_cap\",\"rf_drop\",\"nn_cap\",\"nn_drop\"]\n",
    "missing = [c for c in need if c not in unk_df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Expected base columns missing from unk_df: {missing}\")\n",
    "\n",
    "unk_df = unk_df.copy()\n",
    "unk_df[\"log_avg\"] = unk_df[[\"log_cap\",\"log_drop\"]].mean(axis=1)\n",
    "unk_df[\"rf_avg\"]  = unk_df[[\"rf_cap\",\"rf_drop\"]].mean(axis=1)\n",
    "unk_df[\"nn_avg\"]  = unk_df[[\"nn_cap\",\"nn_drop\"]].mean(axis=1)\n",
    "\n",
    "# Simple (non-stacked) ensemble as mean of available model avgs\n",
    "unk_df[\"ensemble_prob\"] = unk_df[[\"log_avg\",\"rf_avg\",\"nn_avg\"]].mean(axis=1)\n",
    "\n",
    "# --- 2) Build merge inputs ---\n",
    "ens   = unk_df[[\"txId\",\"log_avg\",\"rf_avg\",\"nn_avg\",\"ensemble_prob\"]].copy()\n",
    "stack = ranked_unknowns[[\"txId\",\"ensemble_prob_stack\"]].copy()\n",
    "\n",
    "ens[\"txId\"]   = ens[\"txId\"].astype(str)\n",
    "stack[\"txId\"] = stack[\"txId\"].astype(str)\n",
    "\n",
    "# --- 3) Merge + rank + percentiles + save ---\n",
    "final = ens.merge(stack, on=\"txId\", how=\"outer\")\n",
    "\n",
    "final[\"rank\"] = final[\"ensemble_prob\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "final[\"percentile\"] = (1 - final[\"rank\"] / len(final)) * 100\n",
    "\n",
    "final = final.sort_values(\"ensemble_prob\", ascending=False)[\n",
    "    [\"txId\",\"log_avg\",\"rf_avg\",\"nn_avg\",\"ensemble_prob\",\"ensemble_prob_stack\",\"rank\",\"percentile\"]\n",
    "]\n",
    "\n",
    "final.to_csv(\"suspicious_unknowns_final.csv\", index=False)\n",
    "print(final.head(20))\n",
    "\n",
    "# --- 4) Top-3 for your Stakeholder Summary table ---\n",
    "top3 = final.head(3).copy()\n",
    "top3[\"Percentile Rank\"] = top3[\"percentile\"].round(2).astype(str) + \"%\"\n",
    "top3_table = top3[[\"txId\",\"ensemble_prob\",\"Percentile Rank\"]].rename(\n",
    "    columns={\"ensemble_prob\": \"Ensemble Probability (Illicit)\"}\n",
    ")\n",
    "print(\"\\nTop 3 suspicious unknowns:\")\n",
    "print(top3_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Tbs3GGuCkeC",
    "outputId": "3e5da81d-6db6-45e2-d46e-94f49f44e734"
   },
   "outputs": [],
   "source": [
    "# Expect: unk_df has txId, log_cap, log_drop, rf_cap, rf_drop, nn_cap, nn_drop\n",
    "#         ranked_unknowns has txId, ensemble_prob_stack\n",
    "\n",
    "# 0) Validate columns\n",
    "need = [\"txId\",\"log_cap\",\"log_drop\",\"rf_cap\",\"rf_drop\",\"nn_cap\",\"nn_drop\"]\n",
    "missing = [c for c in need if c not in unk_df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Expected base columns missing from unk_df: {missing}\")\n",
    "\n",
    "unk_df = unk_df.copy()\n",
    "unk_df[\"txId\"] = unk_df[\"txId\"].astype(str)\n",
    "\n",
    "# 1) Per-model averages (cap/drop → avg)\n",
    "unk_df[\"log_avg\"] = unk_df[[\"log_cap\",\"log_drop\"]].mean(axis=1)\n",
    "unk_df[\"rf_avg\"]  = unk_df[[\"rf_cap\",\"rf_drop\"]].mean(axis=1)\n",
    "unk_df[\"nn_avg\"]  = unk_df[[\"nn_cap\",\"nn_drop\"]].mean(axis=1)\n",
    "\n",
    "# 2) Equal-weight ensemble across the 3 model avgs\n",
    "unk_df[\"ensemble_prob_equal\"] = unk_df[[\"log_avg\",\"rf_avg\",\"nn_avg\"]].mean(axis=1)\n",
    "\n",
    "# 3) Optimized weighted ensemble using learned percentages (from stacker)\n",
    "w_log, w_rf, w_nn = 0.324, 0.432, 0.244  # LR 32.4%, RF 43.2%, NN 24.4%\n",
    "unk_df[\"ensemble_prob_weighted\"] = (\n",
    "    w_log*unk_df[\"log_avg\"] + w_rf*unk_df[\"rf_avg\"] + w_nn*unk_df[\"nn_avg\"]\n",
    ")\n",
    "\n",
    "# 4) Bring in stacked score\n",
    "stack = ranked_unknowns[[\"txId\",\"ensemble_prob_stack\"]].copy()\n",
    "stack[\"txId\"] = stack[\"txId\"].astype(str)\n",
    "\n",
    "final = (\n",
    "    unk_df[[\"txId\",\"log_avg\",\"rf_avg\",\"nn_avg\",\n",
    "            \"ensemble_prob_equal\",\"ensemble_prob_weighted\"]]\n",
    "    .merge(stack, on=\"txId\", how=\"inner\")\n",
    ")\n",
    "\n",
    "# 5) Rank each score separately + percentiles\n",
    "for col in [\"ensemble_prob_equal\", \"ensemble_prob_weighted\", \"ensemble_prob_stack\"]:\n",
    "    final[f\"rank_{col}\"] = final[col].rank(ascending=False, method=\"first\").astype(int)\n",
    "n = len(final)\n",
    "for col in [\"ensemble_prob_equal\", \"ensemble_prob_weighted\", \"ensemble_prob_stack\"]:\n",
    "    final[f\"pct_{col}\"] = (1 - final[f\"rank_{col}\"] / n) * 100\n",
    "\n",
    "# 6) Consensus rank (average of ranks; lower = riskier) + consensus percentile\n",
    "final[\"rank_consensus\"] = final[\n",
    "    [\"rank_ensemble_prob_equal\",\"rank_ensemble_prob_weighted\",\"rank_ensemble_prob_stack\"]\n",
    "].mean(axis=1)\n",
    "final = final.sort_values(\"rank_consensus\", ascending=True).reset_index(drop=True)\n",
    "final[\"percentile_consensus\"] = (1 - (final[\"rank_consensus\"] - 1) / len(final)) * 100\n",
    "\n",
    "# 7) High-agreement flag (top 5% in both equal + stacked)\n",
    "final[\"high_agreement\"] = (\n",
    "    (final[\"pct_ensemble_prob_equal\"] >= 95.0) &\n",
    "    (final[\"pct_ensemble_prob_stack\"] >= 95.0)\n",
    ")\n",
    "\n",
    "# 8) Save + preview\n",
    "cols_out = [\n",
    "    \"txId\",\"log_avg\",\"rf_avg\",\"nn_avg\",\n",
    "    \"ensemble_prob_equal\",\"ensemble_prob_weighted\",\"ensemble_prob_stack\",\n",
    "    \"rank_ensemble_prob_equal\",\"rank_ensemble_prob_weighted\",\"rank_ensemble_prob_stack\",\n",
    "    \"percentile_consensus\",\"high_agreement\"\n",
    "]\n",
    "final[cols_out].to_csv(\"suspicious_unknowns_final.csv\", index=False)\n",
    "print(\"Saved -> suspicious_unknowns_final.csv\")\n",
    "print(final[cols_out].head(20))\n",
    "\n",
    "# 9) Tiny stakeholder table\n",
    "top3 = final.head(3).copy()\n",
    "top3_tbl = top3[[\"txId\",\"ensemble_prob_weighted\",\"ensemble_prob_stack\",\n",
    "                 \"percentile_consensus\",\"high_agreement\"]].rename(columns={\n",
    "    \"ensemble_prob_weighted\": \"Weighted Risk\",\n",
    "    \"ensemble_prob_stack\": \"Stacked Risk\",\n",
    "    \"percentile_consensus\": \"Consensus Percentile\"\n",
    "})\n",
    "print(\"\\nTop 3 suspicious unknowns (consensus):\")\n",
    "print(top3_tbl.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I-2Rv7wBD61"
   },
   "source": [
    "**Consensus percentile** is your way of saying: “This node is ranked highly by both the equal-weight ensemble and the weighted/stacked probabilities.” It averages the rankings from equal-weight, weighted (learned percentages), and stacked meta-model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbrwlgW7R894"
   },
   "source": [
    "Sanity check with models score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Joa4_yz7SBsW",
    "outputId": "93e21c2c-1738-46d0-fbc4-77494a575ba0"
   },
   "outputs": [],
   "source": [
    "# OOF has y_true and the same meta_feats (log_cap/drop, rf_cap/drop, nn_cap/drop)\n",
    "oof_preds_avg   = oof_df[[\"log_cap\",\"log_drop\",\"rf_cap\",\"rf_drop\",\"nn_cap\",\"nn_drop\"]].mean(axis=1)\n",
    "oof_preds_stack = meta.predict_proba(oof_df[meta_feats])[:,1]  # same meta as you trained\n",
    "\n",
    "def summarize(name, y, p):\n",
    "    auc = roc_auc_score(y, p)\n",
    "    brier = brier_score_loss(y, p)\n",
    "    print(f\"{name:22s}  AUC={auc:.3f}  Brier={brier:.4f}\")\n",
    "\n",
    "y = oof_df[\"y_true\"].values\n",
    "summarize(\"avg ensemble (OOF)\", y, oof_preds_avg.values)\n",
    "summarize(\"stacked (OOF)\",      y, oof_preds_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZoFdvzlVdPA"
   },
   "source": [
    "**Key points:**\n",
    "- Both the average and stacked ensembles achieved **perfect separation (AUC=1.0)** on labeled validation data.  \n",
    "- The stacked meta-model gave **sharper calibration** (lower Brier score).  \n",
    "- For stakeholders, we report the **average ensemble (`ensemble_prob`)** because its values are easier to interpret (e.g., 0.64 ≈ 64% chance illicit).  \n",
    "- The stacked scores (`ensemble_prob_stack`) are **retained** in the dataset for analysts who want to experiment with alternative ranking strategies.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fh35YlM7h7zC"
   },
   "source": [
    "## Results\n",
    "\n",
    "### Models Tested\n",
    "\n",
    "We built and evaluated three different models:\n",
    "\n",
    "Logistic regression for interpretability\n",
    "\n",
    "Random forest for tree-based pattern recognition\n",
    "\n",
    "Neural network for capturing non-linear structure\n",
    "\n",
    "### Ensemble Approach\n",
    "\n",
    "Rather than relying on a single model, we combined them into an ensemble. This approach balances recall (catching more suspicious cases) with precision (minimizing false alarms).\n",
    "\n",
    "The ensemble improves over individual models by capturing complementary signals. Transactions most likely to be illicit show consistent agreement across methods.\n",
    "\n",
    "### Stakeholder Illustration\n",
    "\n",
    "The ensemble flagged several unknown transactions as high-probability illicit. For illustration, here are three of the top cases identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iWDcVjQ0d0Mk",
    "outputId": "13117a96-2ad5-45be-f3a8-3ebd6d36d719"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _bfs_hops(G, center, radius=4):\n",
    "    \"\"\"Return dict: node -> hop distance (0..radius) from center in G.\"\"\"\n",
    "    center = str(center)\n",
    "    if center not in G:\n",
    "        return {center: 0}\n",
    "    dist = {center: 0}\n",
    "    q = deque([center])\n",
    "    while q:\n",
    "        v = q.popleft()\n",
    "        if dist[v] == radius:\n",
    "            continue\n",
    "        for w in G[v]:\n",
    "            if w not in dist:\n",
    "                dist[w] = dist[v] + 1\n",
    "                q.append(w)\n",
    "    return dist\n",
    "\n",
    "def _radial_positions_by_hop(hops, jitter=0.03):\n",
    "    \"\"\"Place nodes on concentric rings by hop distance around (0.5,0.5).\"\"\"\n",
    "    by_hop = {}\n",
    "    for n, d in hops.items():\n",
    "        by_hop.setdefault(d, []).append(n)\n",
    "    pos = {}\n",
    "    R = max(by_hop) if by_hop else 0\n",
    "    for d, nodes in by_hop.items():\n",
    "        r = 0.18 + 0.74 * (d / max(1, R))  # normalized radius\n",
    "        k = len(nodes)\n",
    "        for i, n in enumerate(nodes):\n",
    "            angle = 2 * math.pi * (i / max(1, k))\n",
    "            x = 0.5 + r * math.cos(angle) + np.random.uniform(-jitter, jitter)\n",
    "            y = 0.5 + r * math.sin(angle) + np.random.uniform(-jitter, jitter)\n",
    "            pos[n] = (x, y)\n",
    "    return pos\n",
    "\n",
    "def _node_color(n, class_map):\n",
    "    c = class_map.get(str(n), \"unknown\")\n",
    "    if c == \"illicit\": return \"#d62728\"   # red\n",
    "    if c == \"licit\":   return \"#2ca02c\"   # green\n",
    "    return \"#7f7f7f\"                      # gray\n",
    "\n",
    "def _focused_subgraph(G, tx, class_map, illicit_set, radius=4, connect_all_illicit=True):\n",
    "    \"\"\"\n",
    "    Keep a readable subset around tx:\n",
    "      - always center + 1-hop neighbors\n",
    "      - all illicit nodes within 'radius'\n",
    "      - shortest path(s) from center to illicit:\n",
    "          * if connect_all_illicit=True => to every illicit within radius\n",
    "          * else only to the nearest illicit\n",
    "    \"\"\"\n",
    "    tx = str(tx)\n",
    "    if tx not in G:\n",
    "        H = nx.Graph(); H.add_node(tx); return H\n",
    "\n",
    "    keep = set(nx.ego_graph(G, tx, radius=1).nodes())\n",
    "    ego_big = nx.ego_graph(G, tx, radius=radius)\n",
    "    illicit_in_ego = {n for n in ego_big if n in illicit_set}\n",
    "\n",
    "    lengths = nx.single_source_shortest_path_length(G, tx, cutoff=radius)\n",
    "\n",
    "    if illicit_in_ego:\n",
    "        if connect_all_illicit:\n",
    "            for target in illicit_in_ego:\n",
    "                if target in lengths:\n",
    "                    keep |= set(nx.shortest_path(G, tx, target))\n",
    "        else:\n",
    "            target = min(illicit_in_ego, key=lambda n: lengths.get(n, float(\"inf\")))\n",
    "            if target in lengths:\n",
    "                keep |= set(nx.shortest_path(G, tx, target))\n",
    "\n",
    "    keep |= illicit_in_ego  # keep the illicit nodes themselves\n",
    "    H = G.subgraph(keep).copy()\n",
    "\n",
    "    # Ensure string labels\n",
    "    if len(H) and not isinstance(next(iter(H.nodes())), str):\n",
    "        H = nx.relabel_nodes(H, lambda n: str(n), copy=True)\n",
    "    return H\n",
    "\n",
    "# =========================\n",
    "# Plotter\n",
    "# =========================\n",
    "def plot_candidate_brief(\n",
    "    tx,\n",
    "    final,\n",
    "    G_undirected,\n",
    "    class_map,\n",
    "    SHOW_COL=\"ensemble_prob_weighted\",      # score to DISPLAY (prob or percentile)\n",
    "    PRED_PROB_COL=\"ensemble_prob_weighted\", # prob (0..1) used for neighbor flags\n",
    "    p_thresh=0.50,\n",
    "    radius=4,\n",
    "    focus=True,\n",
    "    connect_all_illicit=True,               # if focus=True, connect to ALL illicit nodes\n",
    "    figsize=(12, 5.5),\n",
    "    save_path=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    SHOW_COL can be: 'ensemble_prob_equal', 'ensemble_prob_weighted',\n",
    "                     'ensemble_prob_stack', 'percentile_consensus'\n",
    "    PRED_PROB_COL must be a probability column in [0,1] (recommended: weighted).\n",
    "    \"\"\"\n",
    "\n",
    "    tx = str(tx)\n",
    "    assert SHOW_COL in final.columns, f\"{SHOW_COL} not found in final.columns\"\n",
    "    assert PRED_PROB_COL in final.columns, f\"{PRED_PROB_COL} not found in final.columns\"\n",
    "\n",
    "    # Maps\n",
    "    final = final.copy()\n",
    "    final[\"txId\"] = final[\"txId\"].astype(str)\n",
    "    prob_map_show = dict(zip(final[\"txId\"], final[SHOW_COL].values))\n",
    "    prob_map_pred = dict(zip(final[\"txId\"], final[PRED_PROB_COL].values))\n",
    "\n",
    "    show_val = float(prob_map_show.get(tx, float(\"nan\")))\n",
    "    title_val = f\"{show_val:.2f}\" if \"percentile\" in SHOW_COL.lower() else f\"{show_val:.3f}\"\n",
    "\n",
    "    illicit_set = {str(k) for k, v in class_map.items() if v == \"illicit\"}\n",
    "\n",
    "    # Subgraph we will actually draw + hop distances on THIS subgraph\n",
    "    if focus:\n",
    "        H = _focused_subgraph(G_undirected, tx, class_map, illicit_set,\n",
    "                              radius=max(radius, 2), connect_all_illicit=connect_all_illicit)\n",
    "    else:\n",
    "        H = nx.ego_graph(G_undirected, tx, radius=radius)\n",
    "        if len(H) and not isinstance(next(iter(H.nodes())), str):\n",
    "            H = nx.relabel_nodes(H, lambda n: str(n), copy=True)\n",
    "\n",
    "    hops = nx.single_source_shortest_path_length(H, tx, cutoff=radius)\n",
    "\n",
    "    # Layout\n",
    "    pos = _radial_positions_by_hop(hops)\n",
    "    # Backfill any missing positions\n",
    "    if len(pos) < H.number_of_nodes():\n",
    "        for n in H.nodes():\n",
    "            if n not in pos:\n",
    "                pos[n] = (0.5 + np.random.uniform(-0.02, 0.02),\n",
    "                          0.5 + np.random.uniform(-0.02, 0.02))\n",
    "\n",
    "    # Neighbor metrics (1-hop in the full graph)\n",
    "    nbrs = list(G_undirected.neighbors(tx)) if tx in G_undirected else []\n",
    "    labels = [class_map.get(str(n), \"unknown\") for n in nbrs]\n",
    "    deg = len(nbrs)\n",
    "    illicit_n = sum(z == \"illicit\" for z in labels)\n",
    "    licit_n   = sum(z == \"licit\"   for z in labels)\n",
    "    unknown_n = sum(z == \"unknown\" for z in labels)\n",
    "    pred_illicit_unknown = sum(\n",
    "        1 for n in nbrs\n",
    "        if class_map.get(str(n), \"unknown\") == \"unknown\"\n",
    "        and prob_map_pred.get(str(n), 0.0) >= p_thresh\n",
    "    )\n",
    "\n",
    "    # Shortest path to any illicit (bounded in full graph)\n",
    "    sp = math.inf\n",
    "    if tx in G_undirected and illicit_set:\n",
    "        dist_full = _bfs_hops(G_undirected, tx, radius=radius)\n",
    "        candidates = [n for n in dist_full if n in illicit_set and n != tx]\n",
    "        if candidates:\n",
    "            sp = min(dist_full[n] for n in candidates)\n",
    "    sp_txt = \"∞\" if math.isinf(sp) else str(sp)\n",
    "\n",
    "    # ----- Draw -----\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = fig.add_gridspec(1, 2, width_ratios=[2.1, 1.0])\n",
    "    axG = fig.add_subplot(gs[0, 0]); axB = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    nx.draw_networkx_edges(H, pos, ax=axG, alpha=0.28, width=0.9)\n",
    "    nx.draw_networkx_nodes(\n",
    "        H, pos,\n",
    "        nodelist=list(H.nodes()),\n",
    "        node_color=[_node_color(n, class_map) for n in H.nodes()],\n",
    "        node_size=90, ax=axG\n",
    "    )\n",
    "    if tx in H:\n",
    "        # center node (bigger + black outline)\n",
    "        nx.draw_networkx_nodes(\n",
    "            H, pos, nodelist=[tx],\n",
    "            node_color=[_node_color(tx, class_map)],\n",
    "            edgecolors=\"black\", linewidths=1.6, node_size=280, ax=axG\n",
    "        )\n",
    "        # halo\n",
    "        circ = plt.Circle(pos[tx], radius=0.06, color=\"black\", fill=False, lw=1.6, alpha=0.6)\n",
    "        axG.add_patch(circ)\n",
    "\n",
    "    # hop rings (0..R)\n",
    "    if hops:\n",
    "        R = max(hops.values())\n",
    "        for d in range(0, R + 1):\n",
    "            r = 0.18 + 0.74 * (d / max(1, R))\n",
    "            ring = plt.Circle((0.5, 0.5), r, color=\"lightgray\", fill=False, lw=0.6, alpha=0.35)\n",
    "            axG.add_patch(ring)\n",
    "            axG.text(0.5, 0.5 + r, f\"hop {d}\", ha=\"center\", va=\"bottom\", fontsize=8, alpha=0.6)\n",
    "\n",
    "    axG.legend(handles=[\n",
    "        Patch(facecolor=\"#d62728\", edgecolor=\"none\", label=\"illicit (known)\"),\n",
    "        Patch(facecolor=\"#2ca02c\", edgecolor=\"none\", label=\"licit (known)\"),\n",
    "        Patch(facecolor=\"#7f7f7f\", edgecolor=\"none\", label=\"unknown (unlabeled)\"),\n",
    "    ], loc=\"upper right\", fontsize=8, frameon=False)\n",
    "\n",
    "    axG.set_axis_off(); axG.margins(0.08)\n",
    "\n",
    "    # Right metrics panel\n",
    "    axB.axis(\"off\")\n",
    "    ring_counts = Counter(hops.values())\n",
    "    ring_line = \" | \".join(f\"{k}-hop:{v}\" for k, v in sorted(ring_counts.items()))\n",
    "    lines = [\n",
    "        f\"{SHOW_COL}:                {title_val}\",\n",
    "        f\"Degree (1-hop):           {deg}\",\n",
    "        f\"Illicit neighbors:        {illicit_n}/{deg} ({(illicit_n/deg if deg else 0):.2f})\",\n",
    "        f\"Pred illicit (unknown):   {pred_illicit_unknown}/{unknown_n} (p ≥ {p_thresh:.2f})\",\n",
    "        f\"Shortest path to illicit: {sp_txt}\",\n",
    "        f\"Hop counts:               {ring_line}\",\n",
    "    ]\n",
    "    y = 0.95\n",
    "    for s in lines:\n",
    "        axB.text(0.02, y, s, transform=axB.transAxes, ha=\"left\", va=\"top\", fontsize=10)\n",
    "        y -= 0.12\n",
    "\n",
    "    fig.suptitle(f\"txId: {tx}  |  {SHOW_COL}={title_val}  |  deg={deg}  |  sp_illicit={sp_txt}\",\n",
    "                 fontsize=11, y=1.02)\n",
    "    fig.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"txId\": tx, \"show_col\": SHOW_COL, \"show_val\": show_val,\n",
    "        \"pred_prob_col\": PRED_PROB_COL, \"p_thresh\": p_thresh,\n",
    "        \"degree\": deg, \"illicit_neighbors\": illicit_n, \"licit_neighbors\": licit_n,\n",
    "        \"unknown_neighbors\": unknown_n, \"pred_illicit_unknown\": pred_illicit_unknown,\n",
    "        \"shortest_path_to_illicit\": None if math.isinf(sp) else int(sp),\n",
    "        \"radius\": int(radius), \"saved\": bool(save_path), \"save_path\": save_path,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Usage examples\n",
    "# =========================\n",
    "# 1) Harmonize IDs (run once in your notebook before plotting)\n",
    "final[\"txId\"] = final[\"txId\"].astype(str)\n",
    "class_map = {str(k): v for k, v in class_map.items()}\n",
    "if len(G_undirected) > 0 and not isinstance(next(iter(G_undirected.nodes())), str):\n",
    "    G_undirected = nx.relabel_nodes(G_undirected, lambda n: str(n), copy=True)\n",
    "\n",
    "# 2) Choose prob column for neighbor flags (0..1)\n",
    "PRED_PROB_COL = \"ensemble_prob_weighted\"\n",
    "THRESH = 0.50\n",
    "\n",
    "# 3) Top-3 by equal-weight (full ego, with rings)\n",
    "for tx in final.sort_values(\"ensemble_prob_equal\", ascending=False).head(3)[\"txId\"]:\n",
    "    info = plot_candidate_brief(\n",
    "        tx=tx,\n",
    "        final=final,\n",
    "        G_undirected=G_undirected,\n",
    "        class_map=class_map,\n",
    "        SHOW_COL=\"ensemble_prob_equal\",\n",
    "        PRED_PROB_COL=PRED_PROB_COL,\n",
    "        p_thresh=THRESH,\n",
    "        radius=4,\n",
    "        focus=False,                # full ego neighborhood\n",
    "        connect_all_illicit=True,   # (used only if focus=True)\n",
    "        save_path=None,             # show inline\n",
    "    )\n",
    "    print(info)\n",
    "\n",
    "# 4) Top-3 by optimized weighted (full ego, with rings)\n",
    "for tx in final.sort_values(\"ensemble_prob_weighted\", ascending=False).head(3)[\"txId\"]:\n",
    "    info = plot_candidate_brief(\n",
    "        tx=tx,\n",
    "        final=final,\n",
    "        G_undirected=G_undirected,\n",
    "        class_map=class_map,\n",
    "        SHOW_COL=\"ensemble_prob_weighted\",\n",
    "        PRED_PROB_COL=PRED_PROB_COL,\n",
    "        p_thresh=THRESH,\n",
    "        radius=4,\n",
    "        focus=False,                # full ego neighborhood\n",
    "        connect_all_illicit=True,   # (used only if focus=True)\n",
    "        save_path=None,\n",
    "    )\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjSqp-IksKUv"
   },
   "source": [
    "Node `208930302` Has the **risk score = 0.641** Which we use the features and models and doing ensemble of `PROB_COL = \"ensemble_prob\"`. The **degree is 1** And the **shortest path is also 1** to a illicit. The Pred illicit for unknowns are zero, There are no unlabeled neighbors. The **illicit neighbors are all 1** meaning $\\frac{\\text{illicit}}{\\text{illict + licit + unknown}} = [0,1]$ probability range.\n",
    "\n",
    "In other words, this transaction sits in a **bad neighborhood**: its only direct neighbor is already labeled illicit, and there are no unlabeled or licit neighbors to offset this signal. The ensemble probability places this node in a high-risk range, consistent with proximity-based indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX1tJW3ogT00"
   },
   "source": [
    "### Wrap-Up\n",
    "\n",
    "Well, that’s a wrap folks!\n",
    "\n",
    "Here are the stakeholder visuals and results — the top suspicious unknowns, highlighted by multiple ensemble approaches. These nodes consistently show patterns that put them very close to being classified as illicit.\n",
    "\n",
    "If you’re still reading, thank you for following along with my project and for your attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-QtFBIQlJCV"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] M. P. Deisenroth, A. A. Faisal, and C. S. Ong, Mathematics for Machine Learning. Cambridge, U.K.: Cambridge Univ. Press, 2020.\n",
    "\n",
    "[2] V. H. Masías, M. Valle, C. Morselli, F. Crespo, A. Vargas, and S. Laengle, “Modeling verdict outcomes using social network measures: The Watergate and Caviar Network cases,” PLOS ONE, vol. 11, no. 1, p. e0147248, Jan. 2016.\n",
    "\n",
    "[3] S. Masihullah, M. Negi, M. Joe, and J. Sathyanarayana, “Identifying fraud rings using domain-aware weighted community detection,” Medium, 2022. [Online]. Available: https://medium.com/@example/article-link\n",
    "\n",
    "[4] N. Mukhopadhyay, Probability and Statistical Inference. New York, NY, USA: Marcel Dekker, 2000.\n",
    "\n",
    "[5] M. E. J. Newman, Networks: An Introduction. Oxford, U.K.: Oxford Univ. Press, 2010.\n",
    "\n",
    "[6] H. Schenck, Algebraic Foundations for Applied Topology and Data Analysis. Cham, Switzerland: Springer, 2022.\n",
    "\n",
    "[7] L. N. Trefethen and D. Bau III, Numerical Linear Algebra. Philadelphia, PA, USA: SIAM, 1997.\n",
    "\n",
    "[8] Elliptic. [Online]. Available: https://www.elliptic.co\n",
    "\n",
    "[9] M. Weber, G. Domeniconi, J. Chen, D. K. I. Weidele, C. Bellei, T. Robinson, and C. E. Leiserson, “Anti-money laundering in Bitcoin: Experimenting with graph convolutional networks for financial forensics,” in Proc. KDD Workshop Anomaly Detection Finance, New York, NY, USA, Aug. 2019.\n",
    "\n",
    "[10] D. B. West, Introduction to Graph Theory. Boston, MA, USA: Pearson Educ., 2001.\n",
    "\n",
    "[11] G. Nikolentzos, G. Dasoulas, and M. Vazirgiannis, “k-hop graph neural networks,” in Proc. 38th Int. Conf. Machine Learning (ICML), 2021. [Online]. Available: https://arxiv.org/abs/2011.05303\n",
    "\n",
    "[12] D. B. Johnson, “Finding all the elementary circuits of a directed graph,” SIAM J. Comput., vol. 4, no. 1, pp. 77–84, 1975.\n",
    "\n",
    "[13] U. Brandes, “A Faster Algorithm for Betweenness Centrality,” Journal of Mathematical Sociology, vol. 25, no. 2, pp. 163–177, 2001. doi:10.1080/0022250X.2001.9990249.\n",
    "\n",
    "[14] H. Edelsbrunner and J. L. Harer, Computational Topology: An Introduction. Providence, RI, USA: American Mathematical Society, 2010.\n",
    "\n",
    "[15] L. Page, S. Brin, R. Motwani, and T. Winograd, \"The PageRank Citation Ranking: Bringing Order to the Web. Technical Report,\" Stanford InfoLab, 1999.\n",
    "\n",
    "[16] A. A. Hagberg, D. A. Schult, and P. J. Swart, “Exploring network structure, dynamics, and function using NetworkX,” in Proceedings of the 7th Python in Science Conference (SciPy2008), G. Varoquaux, T. Vaught, and J. Millman, Eds., Pasadena, CA, USA, Aug. 2008, pp. 11–15.\n",
    "\n",
    "[17] K. Paton, \"An algorithm for finding a fundamental set of cycles of a graph,\" Communications of the ACM, vol. 12, no. 9, pp. 514–518, 1969.\n",
    "\n",
    "[18] V. D. Blondel, J. L. Guillaume, R. Lambiotte, and E. Lefebvre, \"Fast unfolding of communities in large networks,\" Journal of Statistical Mechanics: Theory and Experiment, vol. 2008, no. 10, p. P10008, 2008.\n",
    "\n",
    "[19] D. J. Watts and S. H. Strogatz, \"Collective dynamics of 'small-world' networks,\" Nature, vol. 393, no. 6684, pp. 440–442, 1998.\n",
    "\n",
    "[20] A. Grover and J. Leskovec, node2vec: Scalable Feature Learning for Networks, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 855–864, Aug. 2016. doi: 10.1145/2939672.2939754\n",
    ".\n",
    "\n",
    "[21] M. E. J. Newman, \"Spectral methods for community detection and graph partitioning,\" Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 373, no. 2056, p. 20150120, 2015.\n",
    "\n",
    "[22] R. Ghrist, \"Barcodes: The persistent topology of data,\" Bulletin of the American Mathematical Society, vol. 45, no. 1, pp. 61–75, 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qj_CV8uzKxiI",
    "outputId": "98a0728f-c8ab-4d98-a18f-185692966112"
   },
   "outputs": [],
   "source": [
    "# This strips all outputs (plots, printouts, widget junk). Leaves only code + markdown.\n",
    "# GitHub can render it without errors.\n",
    "!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace Elliptic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eF2uAJ2woKng",
    "outputId": "0418e28d-51cb-46e4-fe58-ece38b3f4544"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --ClearOutputPreprocessor.enabled=True \\\n",
    "  --to notebook --output Elliptic_clean.ipynb Elliptic.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
